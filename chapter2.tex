\def\mychapter{II}
\input{chapterhead}
\chapter{Mersenne questions in ethics}\label{ch:ethics}
\section{Motivating examples}
\subsection{The rule of preferential treatment}
Let us begin with a more detailed discussion of an example from Thomas Aquinas's discussion of the order of charity. Aquinas thinks,
along with common sense, that those who are more closely related\footnote{In some relevant way, that is.
A torturer is closely related to the victim, but that doesn't produce a special duty on the part of the victim!} to 
us have a greater moral call on us.
Thus, if it is a question of bestowing the same good on one of two people, if one is more closely related,
we should benefit the closer one. But Aquinas writes: ``The case may occur, however, that one 
ought rather to invite strangers [to eat with us], on 
account of their greater want.'' But then he immediately raises the question of what one should do ``if of two, one be 
more closely connected, and the other in greater want''~\parencite[II-II Q31 A3 Rp.~1]{AquinasST}.

We might hope that here Aquinas would give us some clever rule for weighing connection against need. But 
instead he writes something that is very sensible and yet disappointing: ``it is not possible to decide, by any general rule, which of them we ought 
to help rather than the other, since there are various degrees of want as well as of connection''~\parencite[II-II Q31 A3 Rp.~1]{AquinasST}.

It is
tempting at this point to throw up one's hands and simply say that in these in-between cases there is no
fact of the matter as to what should be done, or both options are permissible, or else relativism applies
to the case. But that would not do justice to the way we agonize when we find ourselves in such a difficult 
situation, trying to discover the truth of the matter. (It is interesting to note that the most common real-life moral dilemmas
tend to be like these kinds of cases, rather than highly controversial questions about trolleys, strategic bombing or
bioethics much discussed by philosophers.)
And indeed Aquinas maintains a realist attitude to
the question while simply offering this advice for how to figure out the answer in a particular case: ``the matter 
requires the judgment of a prudent [person]'' \parencite[II-II Q31 A3 Rp.~1]{AquinasST}.

We can formalize this as the problem of specifying a function $f(r_1,b_1,r_2,b_2)$ of four variables, two of them, $r_1$ and $r_2$, being
degrees of relation and the other two, $b_1$ and $b_2$, being degrees of benefit, where the function yields one of three values
corresponding to whether it is (i)~obligatory, (ii)~permissible but not obligatory or (iii)~impermissible to bestow a benefit of degree $b_1$ on a person with
relation of degree $r_1$ to the agent in place of bestowing a benefit of degree $b_2$ on someone related to degree $r_2$. 

In fact, the problem of a rule of preferential treatment is much more complicated than the above indicates. First, the \textit{kinds} of benefit and relation also matter: ``we ought in preference 
to bestow on each one such benefits as pertain to the matter in which, speaking simply, he is most closely connected with us'' \parencite[II-II Q31 A3]{AquinasST}.
So the function will depend not merely on quantitative features but qualitative ones. Second, although Aquinas does not mention it here,
the evaluation will no doubt depend on various features of the circumstances. And, third, in practice instead of choosing between
two certain benefits, we are choosing between two probability distributions over the space of possible benefits.

Now, as Aquinas admits, we do not know what the moral evaluation function for choices between benefits to different people is.
But given moral realism, once enough relevant features are satisfied, there has to be some such function, even if we do not know what it is, just as there is a function that assigns to each person
alive now the number of hairs they now have, even though we cannot specify any of the values of the function.
And we have good reason to expect the moral evaluation function to be very complicated. Indeed, probably the only serious proposal for a
relatively simple function $f$ here is the utilitarian suggestion that $f(r_1,b_1,r_2,b_2)$ yields obligation when $b_1>b_2$,
mere permission when $b_1=b_2$ and prohibition when $b_1<b_2$. But this simple utilitarian solution would betray the intuition that
the degrees of relation $r_1$ and $r_2$, much less the kinds of benefit and relation, are relevant to the moral evaluation.???refs (For further discussion of utilitarianism, see ??forwardref.)

Indeed, whatever the function is, it is likely to look arbitrary. Fix the degrees of relationship to be one's parent and a total stranger,
and fix a specific and certain financial benefit of \$1000 to one's parent, and fix the circumstances. Then as we increase the 
financial benefit to the stranger from zero to infinity, we will presumably initially have a requirement of benefiting the parent
(it would be wrong to give \$1 to a stranger instead of \$1000 to a parent in ordinary circumstances), 
then a permission either way, and then a requirement to benefit the second party. There will be boundaries between these regions
of logical space, and these boundaries will look as arbitrary and contingent as the boundaries between different tax brackets.
Like the tax brackets, some proposals for boundaries will be \textit{clearly} unreasonable, but there will be many proposals
that appear reasonable. And whatever the actual boundaries will look arbitrary.

Of course, seemingly arbitrary numbers can come out of an elegant and simple rule: it seems arbitrary that the fifth and sixth 
digits of $\pi$ are $5$ and $9$ respectively, but there is an elegant mathematical explanation. However apart from the
utilitarian proposal, we do not have any at all plausible simple proposal for $f$.

These seemingly arbitrary boundaries in the order of charity call out for an explanation at least as much as 
the exact distance between the earth and the moon does. Just as it seems implausible that the distance between the earth
and the moon \textit{must} be exactly what it is, it seems implausible to think that the boundaries must be exactly where
they are---unless the utilitarian is right about $f$ being very simple. 

In fact, the ethics case calls out for an explanation more than Mersenne's scientific examples did. For we might 
be able to swallow the earth-moon distance being a contingent and brute unexplained fact. But a brute fact is 
intuitively unfitting for a moral rule. A claim that it just so happens, with no explanation at all, that you should 
$\phi$ undercuts the moral force of the alleged moral obligation. We expect anything seemingly arbitrary in our moral norms to have an explanatory ground.

To further argue for this point, consider a version of Divine Command Theory on which obligations are divine commands, and
God rolled indeterministic
dice to decide which actions to command, and by chance God's commands coincided with our common-sense morality, though they
could just as well as well have commanded cruelty and dishonesty. A Divine Command Theory on which it is mere chance
that cruelty is forbidden rather than commanded provides an unacceptable answer to the Euthyphro problem.??
Intuitively, a set of injunctions that is as arbitrary as that cannot constitute morality. But this point generalizes beyond
divine command theory. Suppose that that we have some preferential treatment rules that are brute and contingent, rules
that could have been so different from what they are that they could have enjoined on us the anti-utilitarian rule that we should always prefer the lesser benefit. Then whatever
these brute rules are, they do not constitute morality, but at best happen to agree with morality in content. 

Thus, even if there is some bruteness in the rules of preferential treatment, the rules in our world must be generated in a way
that makes rules such as the anti-utilitarian rules not be among the possible outcomes. But this makes it very unlikely that
the rules would be brute. For what force would limit the brute rules to avoid unacceptable options? Such a view of limited
bruteness would be akin to a metaphysical optimism on which banana peels can come into existence \textit{ex nihilo}, but not where we might trip
over them.

It is important to remember that the Mersenne question here is a metaphysical question: What explanatory grounds are there for why this
rule, rather than some competitor, holds? The \textit{epistemic} question about preferential treatment may well have a virtue-theoretic answer like Aquinas's: if
we acquire the requisite virtues, we will be able to judge particular cases fairly reliably, and until then our best bet is
to ask the advice of virtuous others.

But before I continue the discussion of the possible explanation for the above ethical Mersenne question, let me follow
Mersenne's lead and multiply the examples, in order to defend against potential answers that only work in some cases, and
to make clear how widespread the problem is. In doing so, I take the risk of boring the reader. But it is also my
hope that, apart from the benefit of identifying a broad pattern of questions in furtherance of my projects, I will 
also bring to the reader's attention a number of questions that have some intrinsic interest.

\subsection{Risk and uncertainty}
Some people---perhaps you---would accept a 92\% chance of winning a thousand dollars at the cost of an 8\% chance
of losing ten thousand. I wouldn't. I say that both I and they are reasonable. On the other hand, someone who 
(in ordinary circumstances) rejects a 99.9999\% chance of winning a thousand dollars at the cost of a 0.0001\% chance
of losing ten thousand and someone someone who accepts a 10\% chance of winning a thousand dollars at the cost of a 90\% chance
of losing ten thousand are unreasonable. 
It is well known that attitudes to risk vary between people, and while there are clearly unreasonable attitudes, it is very plausible
that there is a broad range of reasonable attitudes.??refs
So, as we vary the probabilities of wins and losses, we move between cases
where accepting the risk is unreasonable, to cases where both accepting and rejecting are reasonable, to cases where
rejecting is unreasonable.

This, once again, raises the Mersenne problem of why the transitions between the various evaluative categories fall right where they
do.  And of course things are more complicated than described above. The rational evaluation function will depend not just
on the probabilities involved but also on the values of the potential gains and losses. 

While in the case of preferential treatment, utilitarianism provided a neat but implausible solution, so too in this case, expected utility
maximization provides a neat but implausible solution. On expected utility maximization, you are rationally required to
accept a chance $p$ of a good of degree $\alpha$ despite a chance $q$ of a bad of degree $\beta$ against a status quo of
value zero just in case the
expected utility $p\alpha + q\beta$ is strictly positive; when it is zero, you are permitted but not required; 
and when it is negative, you are not permitted. The solution is neat and simple. 

One problem with this solution is it requires all goods to be neatly
quantifiable (cf.\ the next example for difficulties related to that). But the more serious problem is that it requires an
implausibly negatively judgmental attitude towards ordinary people's attitudes to risk.
Indeed, here is a plausible trio of theses about risk that are incompatible with expected utility maximization:
\ditem{2-nobound}{There is no upper bound on possible finite utilities.}
\ditem{2-finite}{A decade of the worst tortures the KGB could think of has a finite negative utility.}
\ditem{2-notworthit}{There is no possible good $G$ of finite utility such that one would be rationally required in
accepting a certainty of a decade of the worst tortures the KGB could think in exchange for a one in billion chance of $G$.}
For as long as $10^{-9}\alpha + \beta>0$, where $\alpha$ is the value of $G$ and $\beta$ is the (highly negative)
value of the tortures, one would rationally required to accept the deal on expected utility maximization, and by \dref{2-nobound}
and \dref{2-finite} there exists a possible $G$ that makes $10^{-9}\alpha + \beta$ strictly positive.
Hence, we should reject expected utility maximization, and absent expected utility maximization, it is unlikely that the rationality 
evaluation function for risk is anything other than messy and arbitrary-looking.

The most plausible objection to my argument against expected utility maximization is to reject the no-upper-bound thesis \dref{2-nobound}.
Here is one way an argument for such a rejection might go. First, there is a maximum intensity of goods that our brain can handle.
Second, goods become significantly less valuable as they are repeated, decreasing in such a way that the sum of the values of any 
goods you could have over an arbitrarily long life has an upper bound.??refs

But the repetition thesis is only plausible when boredom and other memory-based phenomena are in play. Suppose you have 
lived for a very long time. Then you suffer from partial amnesia: you have
lost all episodic memory of your past meals and of your past pinpricks. You are offered what you are reliably informed is 
the most delicious and wholesome dessert every prepared by the best chef on earth, a dessert  which you are told you've eaten some large 
number $n$ times in the past, and you may eat the dessert at the cost of a one in ten chance of a small pinprick. It's clearly worth it,
regardless of what $n$ is. So now suppose this happens to you every day of a very long life. The marginal value of each such 
dessert (i.e., the amount it contributes to total lifelong utility), absent memories of past desserts, must  be at least one 
tenth of the marginal disvalue of the pinprick, at least given expected  utility maximization. But the disvalue of the pinpricks 
clearly does not tend to zero with forgotten repetition. Hence, the value of the desserts does not tend to zero. And hence for any
finite utility bound, enough such desserts will exceed the bound.

For a different example, not involving radically large utilities, imagine this Star Trek plot. Captain Kirk visits a planet inhabited
by two intelligent alien species, all on par with respect to moral value: there are $1,000,000$ oligons and 
$2,000,000,000$ pollakons. Unfortunately, an asteroid is heading for the planet. If nothing is done, it will certainly hit the planet in such
a way as to wipe out all the pollakons but leave the oligons untouched. The only thing Kirk can do is to fire phasers at the asteroid. Spock has calculated that if this
is done, the asteroid's track will be redirected in such a way that it will certainly wipe out all the oligons. Kirk asks whether that will help 
the pollakons? Spock's answer is that probably not: there is a $999/1000$ chance that all the pollakons will still die, but there is
a $1/1000$ chance that they will all survive. 

It seems plausible that Kirk ought not fire phasers. And it is even more plausible that Kirk is not required to fire phasers.
He should not sacrifice the million oligons for a small chance of saving the two billion pollakons. 
But the expected utility of firing phasers is $-1,000,000+(1/1000)(2,000,000,000)=+1,000,000$ lives.
On the other hand, if the chance of saving the pollakons were high enough, say 99\%, the story would turn into 
a plausible trolley case, and Kirk would be permitted and likely required to fire phasers. Where the switchover
occurs is a Mersenne question.

Perhaps the most intuitive justification of expected utility maximization is that, under some technical assumptions,
the Strong Law of Large Numbers guarantees that if in a sufficiently long sequence of independent wagers, you keep 
on choosing a wager whose expected value is better by (at least) some fixed increment $\delta>0$ than the expected
value of an alternate, with probability one your total revenue will eventually be higher than if you were always to take the
lower expected-value wager.\footnote{$^*$Let $X_1,X_2,...$ be a sequence of independent random variables. The Strong 
Law of Large Numbers says that, under some technical assumptions, if $X_1,X_2,...$ are independent random variables, 
then with probability one we have $\lim_{n\to\infty} n^{-1} (X_1+\dots+X_n) = \lim_{n\to\infty} n^{-1} (E[X_1]+\dots+E[X_n])$, with both limits finite, where $E[X]$ is the expected value of $X$. Thus, if $X_1,X_2,...$ are independent random variabls satisfying the
technical assumptions, and so are $Y_1,Y_2,...$, and the $X_i$ represent one set of wagers while the $Y_i$ represent
the other set of wagers, so that $E[X_i] \ge \delta + E[Y_i]$, then the average revenue from selecting the $X_i$ will
converge to at least $\delta$ plus the average revenue from selecting the $Y_i$. One set of
technical assumptions that suffices is that $n^{-1}(E[X_1]+\dots+E[X_n])$ converges to a finite value and 
$\sum_{n=1}^\infty n^{-2} \Var[X_n]$, where $\Var[X]=E[(X-E[X])^2]$ is the variance of $X$.cf.??ref:Kolmogorov}

But the devil is in the details, and the technical assumptions are not always satisfied. One can construct an infinite
sequence of wagers where going for the lower expected-value wager of each pair will, with probability one, eventually
result in a higher total result. For suppose that on every day $n$ of
eternity, with $n\ge 1$, you are offered the opportunity to pay half a unit of utility in exchange for playing a game with a 
$1/2^n$ chance of winning $2^n$ units of utility. By expected utility maximization, you would value the $n$th game at
$(1/2^n)\cdot (2^n)=1$ units of utility, and at a price of $1/2$ units, it would be worth playing. 

But consider what will almost surely happen if you adopt the policy of following expected utility maximization and playing the game. The sum of the probabilities of winning on the different days is finite: 
$1/2+1/4+1/8+\dots=1<\infty$. The Borel-Cantelli Lemma??ref then says that almost surely you will win only a finite number of times
(``almost sure'' is the technical term probabilists use to describe an event that happens with probability one, such as 
getting heads at least once if you toss a fair coin infinitely many times).\footnote{We 
can give an elementary proof of this fact in the case at hand (the proof generalizes to the general case). Let $W_n$ be the event that you will win at least once after day $n$. 
Then $P(W_n)\le 2^{-(n+1)}+2^{-(n+2)}+\dots = 2^{-n}$. Let $W$ be the event that you will win infinitely many times. Then $P(W)\le P(W_n)$ for
every $n$, since if you win infinitely many times, you must win on infinitely many days after day $n$, and so you must win on at least one
day after day $n$. Since $P(W_n) \le 2^{-n}$, we have $P(W) \le 2^{-n}$ for every $n$. But probabilities cannot be negative, and the only 
non-negative real number $x$ such that $x \le 2^{-n}$ for every $n$ is zero. So $P(W)=0$, and hence almost surely $W$ does not happen,
so that almost surely you win only finitely many times.} In other words, almost surely, there will come a day after which you will win no more. At that point, you may well be ahead,
having won more than you paid. But the sum of what you won up to that day is finite, and from then on you will just lose half a unit of utility every 
day. Eventually, there will come a day when your losses will overtake your winnings, and from then on, you will just fall further and further
behind every day.\footnote{The example above uses exponential growth. More moderate growth will work, as long as the sum of the probabilities
is finite. Thus, we could say that on day $n$ the prize has value $n (\log (n+2))^2$ and the probability of winning the prize is the reciprocal
of that, since $\sum_{n=1}^\infty 1/(n (\log (n+2))^2)<\infty$.}

The very unhappy situation of playing infinitely many times and eventually starting to lose every time is the almost 
sure result of following expected utility maximization on each day. We can compare this to the neutral situation of 
refusing ever to play, and getting zero each day, or the situation of accepting the expected utility maximizing gamble
for a number of days, until the probability of winning becomes really small, and refusing from then on.

It is worth noting that this is not just a paradox involving the aggregation of infinitely many utilities.
For, almost surely, after a finite number of days, the expected utility maximizer
falls behind the consistent refuser, and every day after that, the expected utility maximizer is further and further behind,
like someone who got a subscription to a streaming service and forgot to either use or cancel it. And all these amounts are 
finite, and occur a finite, albeit unknown, distance into the future.

We can also consider an interpersonal version of the story. Suppose we have (countably) infinitely many people, numbered $1,2,...$, and person $n$ 
is offered the chance to pay half a unit of utility in exchange for a chance $1/2^n$ of winning $2^n$ units. As before, by expected utility
considerations it's worth it. So, if everyone is an expected utility maximizer, everyone will pay. But by the Borel-Cantelli Lemma, 
almost surely, only finitely many people will win. Thus, almost surely, we will have infinitely many people
pay a cost of half a unit each, and finitely many people win some finite amount. This is a disastrous situation, with a negative infinite
overall utility. Almost surely, it would be much better if everyone refused to play, or only those who had a ``non-negligible'' chance 
at winning played.\footnote{It is worth noting that exponential growth is not necessary for the examples to work. All we need is that
there is a chance $p_n$ of winning a prize of $1/p_n$, and that $\sum_{n=1}^\infty p_n < \infty$. While for ease of calculation above
I let $p_n=1/2^n$, one can have much more moderate shrinkage, such as $p_n=1/n^2$ or even $p_n=1/(n(\log (n+2))^2$.}

??refs:PrussBlog2011,Zhao, 
Wilkinson??ref:https://philpapers.org/rec/WILRAA-16

It appears that expected utility maximization is not rationally required. But it is the only clearly non-arbitrary solution
to the problem of deciding under uncertainty. While some of the examples involved prudential rationality, the Kirk example
was essentially moral. And the prudential rationality examples can all be switched to moral rationality by supposing
that the victim or beneficiary of the action is someone that one has responsibility for, such as one's child.
Or just consider the difficult questions involving balancing the risk that the innocent are punished with 
the risk that the guilty go free.
There will also be Mersenne questions about involving morality and balancing risks and benefits between persons.
For instance, what risks we may morally impose on others in exchange for a good to ourselves depends in a complex way 
on one's relationship to these others, the probability of the risk, the degree to which these others accept the risk, the 
benefit to self, and so on. 

Whenever I drive, I risk killing other drivers, their passengers, pedestrians by the side road, and so on.
But the probability of these awful outcomes is very small, and typically other people on or by the road have accepted reasonable
risks (or have had them accepted by proxies, in the case of children), so these dire but unlikely outcomes typically do not render it impermissible for
me to go to the grocery store to pick up ice cream.\footnote{I leave open the question whether concerns about global warming 
render it impermissible.} But when the risk is higher, say because I am tired and sleepy after a long day and hence less likely to be
a safe driver, the matter becomes less clear. At some point, as the risk increases, it becomes impermissible to go to the grocery
store for ice cream. 

In all these cases, we have the Mersenne question of grounding the fact that the switchovers happen where they do.
Expected utility utilitarians\footnote{As opposed to actual-outcome utilitarians who evaluate actions morally based on the
actual utilities that would result from an action.??refs} will have a nice answer to this problem. But utilitarianism, as already
noted??ref, has many highly counterintuitive implications. 

\subsection{Orderings between goods}
Under ordinary circumstances, it would not be reasonable to choose to be a mediocre mathematician rather than a superb musician. 
But suppose one's choice is whether to be a superb
musician or a superb mathematician? Here we are dealing with incommensurable goods and either choice is reasonable.

So now let's ask this general question: Is it is reasonable to choose to be a mathematician of quality $\alpha$ rather than
a musician of quality $\beta$? Again, we have a function that takes a number of variables, including $\alpha$ and $\beta$
and the circumstances, and tells us whether (a)~it reasonable to opt to become a mathematician but not reasonable to opt for
music, or (b)~both are reasonable, or (c)~opting for music is reasonable but opting for mathematics is not. And, just as before,
it is very plausible that the function is quite complex.

The problem obviously generalizes to all the many kinds of pairings of incommensurable goods there are.  In each case, there 
will be some function of many variables encoding the correct rational evaluation of the situation, and we will have the Mersenne
question of what grounds the fact that this function, rather than one of the infinitely many others, encodes the correct
rational evaluation.

We also have Mersenne questions here that involve qualitative rather than quantitative comparisons. Other things being equal,
social pleasures are better than solitary ones. This seems rather arbitrary. What makes it be so?
 
In the preferential treatment and moral risk examples, utilitarianism offered a nice solution. But incommensurable goods are a problem for any plausible utilitarianism. Utilitarianism comes in two varieties, depending on whether
the good is pleasure or the good is satisfaction of desire. As Mill famously noted, it is essential to the plausibility
of utilitarianism that one be able to make a distinction between lower and higher pleasures, so as to get the common-sense
conclusion that it is better to be Socrates unsatisfied than to be a satisfied pig.??ref

But once one makes the distinction between lower and higher pleasures, or lower and higher desires, incommensurability
quickly shows up, since different kinds of pleasures and desires do not simply come in a linear ranking. Let's suppose that you get more 
enjoyment and satisfaction of the desire for truth from of mathematics and more enjoyment and satisfaction of aesthetic desire from music, and let us suppose (contrary to typical situations)
that your choice of life will not affect anyone else. Then it seems right to say that the mathematical and musical lives are
incommensurable even on utilitarianism. But even if they are not incommensurable, but equal or one is better than the other, 
we still have a Mersenne problem as to what level of quality of mathematical life exceeds, equals or falls below what level of 
quality of musical life. And in fact it will be more complex than that, in that the quality of a mathematical or musical life
is clearly multidimensional.

One might try to get out of this by hoping for some precise definition of the degree of pleasure or the strength of a desire.
Perhaps there is a perfect neural correlate of the degrees of pleasure or the strengths of desire that can be quantified in a single
number. But such an approach is likely to lead to the swinish utilitarianism that Mill wisely rejects. For presumably the
neural correlate can be manipulated directly, and the pig could be given pleasures which, in terms of neural intensity,
exceed the highest of Socrates' refined joys, and could be made to have a degree of intensity of desire for its swill far
exceeding Socrates' desire for virtue.  

Moreover, any neural approach is likely to fall prey to questions of cross-species comparison. While pig and human brains are
similar, they are not the same, and neural states of pleasure and desire are likely to be merely analogical between them (and if
that's not right, then replace the pig by octopus with its decentralized brain). Clearly at least some
comparisons between human and porcine goods are possible: a tiny human pleasure is worth less than a great porcine one. As one
increases the human pleasure and/or decreases the porcine one, there will come cases where neither of the two is to be
preferred, and then eventually cases where the human pleasure is to be preferred over the porcine one. But where exactly
the cross-over points are is not something we can just read off the neural correlates. And things get even messier when we
compare humans to possible beings that have no brains, such as intelligent robots (if these are possible) or aliens with very 
different biochemistry.

And even if one could give some such precise neural formulation, we would still have
the Mersenne problem of why \textit{this} formulation corresponds with true value rather than some other. 

\subsection{Intersubject aggregation of value and population ethics}
Whether or not consequentialism is true, there are some questions which need to be settled in a 
consequentialist way, for instance questions where the stakeholders are strangers one has no special obligations 
towards and where there are no relevant deontic considerations beyond the consequences. If we aim for the consequentialistically best outcome for 
more than one person, we will need a way of aggregating value between these subjects. A particularly difficult set
of cases comes up when the number of subjects in existence varies between the options.

Perhaps the most straightforward option is to \textit{add up} utilities across the affected population. This faces
several problems. The most famous is Parfit's repugnant conclusion??ref. Any finite scenario full of highly
fulfilled people can be beat by a scenario with a much larger number of people whose level of fulfillment is
minimal. Yet it is implausible to think that we should aim at vastly multiplying human population at a 
minimal level of fulfillment. 

A technical problem here is the following. In decision theory, utilities are normally considered to be defined 
``up to positive affine invariance''.??ref They can be rescaled and shifted without changing anything. This means that for any positive 
number $\alpha$ and any number $\beta$, if we consistently replace every utility $x$ with $\alpha x + \beta$, then 
we have not changed anything. But if we add utilities across a variable number of individuals, although multiplying all
utilities by a positive factor $\alpha$ makes no difference to our aggregate decisions, the addition of 
a constant $\beta$ to every utility can make a difference. For instance, suppose we have a choice between these
options:
\begin{enumerate}
\item[(i)] there are ten individuals
each enjoying a utility of $15$ each
\item[(ii)] there are twelve individuals enjoying a utility of $10$ each. 
\end{enumerate}
As it stands, on
the additive model of aggregation, (i) has a total utility of $10\cdot 15 = 150$ and (ii) has a total utility of $12\cdot 10 = 120$, thereby yielding a preference for the ten individual
option. But if we apply the affine transformation with $\beta = 25$ and $\alpha=1$, then the utilities enjoyed 
by the individuals in the two scenarios become $15+25=40$ and $10+25=35$, yielding respective totals of $10\cdot 40=400$ and 
$12\cdot 35=420$, flipping the preference in favor of the larger population option. 

Essentially, the technical problem is that we need a ``zero point'' for utilities. If we have such a point, then increasing
the number of people with utility above zero will always improve the outcome, while increasing the number of people below it
will make things worse. Now, while there may be clear cases---someone might say that Einstein's life was above the zero 
point, but a life of constant torture would be below---precisely defining a zero point in terms of the vast multitude of various good- and bad-making features of 
a human life involves a large number of Mersenne questions.

The most natural alternative to adding utilities is averaging them. As Parfit has noted, this leads to the another
unpleasant conclusion: if the average of some nation's utility is even slightly below the average utility for all
human beings, then things would be better if the people in that nation didn't exist.\footnote{The problem becomes
perhaps even more vivid if we include non-human animals as subjects. For it may well be that no non-human animal
enjoys a utility greater than that of an average human. One way to this conclusion is the intuition that no 
non-human animal is such that we should save its life instead of an average human's. Another way is to reflect on
the fact that an average human enjoys goods---say, moral or cultural ones---that are qualitatively higher than those
of any non-human animal. On any averaging view, then, it seems it would be good to allow all non-human animals on 
earth die out. (Some antinatalists will disagree about the intuitions regarding average human life.??refs)} 

Furthermore, averaging only escapes the repugnant conclusion for humans given assumptions about what the rest of reality 
is like. Suppose that it turns out that humans are outnumbered by non-human persons by a factor of ten, and the overall average
utility in reality is terrible, as most persons live lives of horrific misery, so that the average utility of a non-human 
person is $-100$ while that of a human is $10$. This makes the average utility overall be $(-100\cdot 10 + 10)/11 = -90$.
Now suppose we have a choice between two options:
\begin{enumerate}
\item[(i)] make every human enjoy a utility of $100$ (a life of deep fulfillment) without changing the number of humans
\item[(ii)]  multiply human population by a factor of $10$, but make each person's utility be a miserable $-50$. 
\end{enumerate}
On (i), overall average utility
becomes $(-100\cdot 10 + 100)/11 = -81.8$. On (ii), it becomes $(-100+(-50))/2 = -75.0$. In other words, multiplication 
of human misery, with sufficient increase of population, may be better than making all humans happy. This is even more repugnant than the original repugnant conclusion, since we are 
``improving'' things by making people not merely slightly well-off but by making them actually miserable, just not as miserable
as the average.

The last point vividly illustrates a well-known problem with averaging: what decision is right depends on epistemically
inaccessible facts about intelligent life outside earth.??ref One can, of course, solve this by restricting the averaging
to our own species and hoping that we won't meet aliens. But we can still get
some version of this repugnant conclusion simply within the human species. We can imagine a situation where three quarters of humans 
are found in a repressive nuclear state, on average have an utterly miserable flourishing level of $-100$, and there is no way for those of us outside
that state to remedy the situation. Suppose instead we have a choice between two policies for the rest of the world: keep the 
population of that part of the world constant and bring everyone to enjoying approximately a utility of $20$ or triple the population 
and bring  everyone down to a pretty miserable $-35$. The resulting averages will be $(-100\cdot 3/4)+20\cdot 1/4 = -70.0$ and $(-100+(-35))/2 = -67.5$.
As in the alien case, we have a nasty version of the repugnant conclusion: lots of people at a low level of happiness---indeed, a level 
of significant misery---outweigh a moderate amount of people at a high level of happiness. 

It is worth noting that in the case of averaging, there is a decision point about aggregating across time. Averaging the utility 
across all subjects at all times is mathematically straightforward if the the number of subjects across time is finite. If it is infinite,
there are many additional complications.\footnote{$^*$If the infinity is countable, then any averaging will have to 
involving a limiting procedure. For instance, if $u_i$ is the utility of the $i$th subject, then we can aggregate by computing
$\lim_{n\to\infty} \sum_{i=1}^n u_i$. However, the value of that limit is likely depend on the order of the subjects if
infinitely many of the $u_i$ are positive and infinitely many are negative (unless $\sum_{i=1}^n |u_i|$ is finite).
If all the subjects are in the same spacetime, we might try to order the subjects temporally. That itself carries some decision
points. Do we order subjects temporally by the beginnings of their existence, the middles, or the ends?} However, averaging across all
subjects at all times could seriously exacerbate some of the repugnant conclusion worries. For instance, if non-rational 
conscious animals count, then the vast majority of conscious animals on earth across all time are relatively primitive,
say on the level of lizards and squirrels, and hence not capable of much flourishing. As a result, the average level of happiness
may easily end up being even lower than if we just average at the present time, thereby making it more practical to increase 
average flourishing by vastly multiplying subjects (human or not) with low levels of flourishing.

The other intuitively natural option is to average simultaneous utilities at each time, and then integrate them across time. This runs into
at least three special problems. First, we have a difficulty of how to account for a multiverse whose universes do not share a 
common time-line. Second, given Relativity Theory, we need a privileged sequence of reference frames---a privileged foliation---to define the 
simultaneity of utilities. Third, and perhaps most problematically, many aspects of the flourishing or languishing of 
humans are difficult to localize in time. Examples include having a \textit{diversity} of cultural experiences over a lifetime, having 
one's goals be posthumously fulfilled or frustrated, or failing to ever find a good friend.


It is possible that some simple function without free parameters can be used to aggregate utility across people in a way that
avoids paradoxes. But it seems unlikely. Intuitively, we have some sort of diminishing returns as we increase the number of
people with a minimal level of happiness, but the diminishment is not the simple ``one over the population count'' multiplier of
the averaging solution. It seems very plausible that an aggregation function that does not generate some repugnant conclusion will
be rather complex and will have multiple free parameters. And then we can ask what grounds these parameters' having the values they
do. ??refs


The above assumed, for ease of modeling, that we could reduce the utility enjoyed by each individual to a number. For reasons 
discussed in Section??ref, this is itself unlikely. Our flourishing seems to consist of a number of incommensurable goods such 
as friendship, understanding, play, etc. Moreover, even if we reduce flourishing to pleasure or desire satisfaction, it is 
likely that we have incommensurable pleasures and incommensurable desires. All this greatly complicates any aggregation procedure,
multiplying the number of its free parameters. 

Finally, even simple aggregation of pleasure or desire-satisfaction over a lifetime raises analogues of the 
repugnant conclusion.??ref Replacing eighty years of a deeply fulfilled life by a million of barely positive
utility does not seem appropriate. Averaging, on the other hand, is also wrong: my life would not improve by
being reduced to its best day. 

\subsection{Double Effect}
Consider these two cases, where all the people other than the dictator are assumed to be innocent.

\nitem{Trolley}{A runaway trolley is heading for a fork in the tracks. If nothing is done, it will
    turn left and kill five people who are on the track. You can redirect the trolley onto the right track, 
    where there is one person on the track, who will be killed by the trolley.}
\nitem{Dictator}{A dictator tells you to kill one person. If you fail to do so, the dictator will kill
    five others.}
    
Assume there are no further relevant consequences.  Deontologists tend to have the intuition that redirecting 
the trolley is permissible but obeying the dictator's orders is murder??refs, while consequentialists assume 
that both are permissible, and indeed obligatory. Let us assume that deontologists are right.

The usual explanation of the difference is that in \textsc{Trolley} one merely foresees, without intending, the 
harm to the person on the right track if one redirects, while in \textsc{Dictator} one intends the death of the person
if one obeys the order, even if one does so merely as a means to saving the five. 

Often, this difference is formalized into a Principle of Double Effect, which says that an action with a foreseen
evil effect and an intended good effect is not ruled out simply on account of the evil effect precisely when 
the evil effect is intended neither as an end nor as a means, and the bad consequences are not disproportionate to the good ones.

But now consider the following variant:

\nitem{Gunshot}{If you do not cause the death of one person, the dictator will kill five others.
    The one person is tied up, and a loaded gun is permanently affixed pointing at that person. The dictator does not
    care about your intention. Never heard a gunshot before, you are curious what it sounds like, so you consider pulling the trigger.}
In \textsc{Gunshot}, if you pull the trigger out of curiosity, you don't intend to kill the one. Moreover, the consequences
on balance are slightly better than in \textsc{Trolley}: five are saved and one dies, but also your curiosity about gunshots is
satisfied. But if we allow \textsc{Gunshot}, then with a bit of cleverness one can come up with a way of non-intentionally
killing the one person in \textsc{Dictator}. Perhaps one is curious what size of hole a bullet makes in a shirt pocket. 

How can one rule out the non-intentional killing in \textsc{Gunshot} without allowing one to accede to the dictator's demand? 

Here are four lines of response. The first is that proportionality needs to hold not just between the good consequences and
the bad ones, but between the \textit{intended} good consequences and the foreseen bad consequences. In \textsc{Gunshot},
while the foreseen goods are proportionate to the death of the person at the other end of the gun, the intended good
of the satisfaction of curiosity is not proportionate to that evil. Effectively, this approach forbids one from counting
goods that are caused by the foreseen evil towards proportionality, since if one were to intend these goods, one would be
intending the evil as a means to them.

But there are other cases where counting goods causally downstream of evils towards proportionality seems exactly right. 

\nitem{Bear}{You have a leg caught in a trap. If you open the trap now, you can save the ;eg. If you wait
    to open the trap later, your leg will become infected and will need to be amputated. However, nearby two other people 
    are trapped in a way that they cannot escape (even with your help) until rescue comes tomorrow. A hungry bear is about 
    to eat one of them, Alice. You foresee that if you open your leg-trap now, the noise of the opening will distract the 
    bear from Alice so that it will eat Bob instead. Once the bear has eaten one person, it won't eat again until after rescue comes.}
The problem here is that a foreseeable consequence of your opening your trap is Bob's getting eaten, and saving a leg is not
proportionate to Bob's death. Now, it is true that Bob's getting eaten keeps Alice from getting eaten. But if we do not count
goods causally downstream of evils towards proportionality, then we cannot count Alice's being saved among the goods in 
our proportionality calculation, and all we get to compare is your leg and Bob's life, and hence there is no proportionality.
But, intuitively, it is permissible to escape the trap even if this leads to a different person getting eaten.

A second line of response is that intentionally firing a gun pointed in someone's direction is very ``close'' to intentionally
killing them, and we should forbid not only intentionally killing innocent people, but also actions that are very close to such
killing.??refs But it is difficult to see why intentionally firing a gun that happens to be pointed in someone's direction should 
count as too close to killing, but redirecting a trolley at someone does not. After all, nothing of moral significance should
hang on the means by which one redirects the trolley. Now imagine that the trolley is heading by inertia towards the left 
track, but a carefully placed barrel of gunpowder near the junction can push the trolley in the direciton of the right track.
And why should it matter whether it is a trolley or a bullet that is being powered by the gunpowder? (We can imagine the
trolley is shaped like a bullet train!) 

That said, there is something intuitive about closeness theories. However, on such theories we have a seemingly arbitrary 
parameter defining the forbidden degrees of closeness to intentional killing, and we have a Mersenne question about what
grounds the parameter's value.

??ref(Koons) have, however, offered a view of Double Effect on which it \textit{does} matter whether we are dealing with 
a trolley or a bullet, and where Mersenne questions are less apparent. The reason is that trolleys and bullets are artifacts with different socially-assigned ends, and when 
one employs an artifact according to its usual operating instructions, the usual ends count morally. The end of a bullet is
killing, and when one propels a bullet in that direction, we might say that one's action counts as close enough to intentional 
killing.\footnote{??refs taken literally claim that one \textit{intends} killing in such a case. This seems wrong. After all,
after the bullet lands, one can rationally, and with no change of one's ends, try to save the person hit by it. 
But it is not rational to act against one's former ends if one has not repudiated them. However one does not need the implausible
claim about intention: all one needs is that the action is close enough to intentional killing for moral purposes.} One
difficulty with this view is that it claims a perhaps implausible moral difference between firing a \textit{gun} and detonating explosives that happen to be
packed in a steel \textit{pipe} sealed at one end with a ball-bearing inserted on the other side of the explosive,
when both the gun and the pipe are pointed at the same innocent person.  

Additionally, the ??ref account does not entirely escape Mersenne questions. An artifact has a particular
end and particular set of operating instructions in virtue of patterns of social behavior. However, there are infinitely many functions 
from social behavior to ends and operating instructions that roughly match our intuitions but disagree on edge cases, such as exactly
how often must forks be used to scratch one's back for them to acquire the end of scratching the back. Again, we need an explanation
of why this function rather than another defines the artifact for moral purposes.

A third line of response depends on contingencies of psychology. It seems unlikely that one could fire the gun pointed at Bob's heart
without intending Bob's death, unless one has done something to protect Bob (say, putting a shield between the gun and Bob). 
But note that a really callous person who does not care about human life probably \textit{could} fire a gun that is 
pointed at Bob simply in order to hear the gunshot. Our actions have
plenty of consequences we know about but that we are completely indifferent to, and hence do not intend. I know that moving
my fingers to type this sentence disturbs air molecules. But I have no intention to disturb air molecules. For if it turned out
that due to some random quantum oddity the air molecules were unmoved by my fingers, my prediction that I will disturb the
air would be falsified, but my action would not be unsuccessful in any way. But an action that fails to achieve one of its
ends is at least partly unsuccessful. 

We might, however, make an Aristotelian move here. Perhaps it is abnormal to fire a gun pointed at Bob's heart without intending
death, and we are morally responsible for the \textit{normal} ends of an action in addition to its actual ends. And perhaps
our human nature defines what ends an action \textit{should} have, given the agent's knowledge of the circumstances.
This is a version of the closeness view, perhaps similar to the ??ref:Koons version. 

A fourth response notes that there is something callous about aiming at a minor end when there are evident great evils at stake. Consider:

\nitem{Candy}{A runaway trolley is heading for a fork in the tracks. If nothing is done, it will
    turn left and kill five people who are on the track. You can redirect the trolley onto the right track, 
    where there are five persons on the track, who will be killed by the trolley, but turning the knob to
	redirect the trolley will cause a candy to pop out for you to eat.}
It is callous to redirect the trolley for the sake of the candy. Similarly,
it is callous to fire a shot in the direction of a person in order to see the hole the bullet makes in their clothing. The intended 
end of the action is absurdly small in comparison to the foreseen harm. 

To expand on this response, imagine that Carl is the agent in \textsc{Trolley}, but Carl is someone who does not have much
in the way of moral feelings, though he does intellectually desire to the right thing. Carl also enjoys redirecting trolleys, 
though like most people he rarely has a chance to do so. So when he finds himself in \textsc{Trolley}, he rejoices, and he
redirects the trolley solely for the fun of it, while reasoning that by Double Effect he is acting permissibly, because the
death of the one person on the left track is not a means to the pleasure of redirection, and proportionality holds, because
on balance the effects are good. 

What is wrong with Carl's action? It seems that he is being callous: the one person on the left track dies for Carl's pleasure.
If Carl were intending to save the five on the right track, in the way that we normally expect someone in a trolley case to do, 
there would be no callousness. This suggests that in checking proportionality, we need to compare the \textit{intended} good 
against the \textit{foreseen} evils. 

We could simply specify that the intended goods are proportionate to the foreseen evils. However, \textsc{Bear} points away 
from that. In \textsc{Bear}, your intended good, the saving of the leg, is not proportionate to the foreseen evil of Bob's 
death. Moreover, let's modify the case of Carl and the trolley. 

\nitem{Leg}{Dave is strapped down near
		the trolley track in such a way that if the trolley turns left, it will destroy his leg. 
		Dave terrified of his leg being crushed, and he is about to redirect the trolley, but he
		notices that there is a person on the right track. He regretfully decides that he must sacrifice his
		leg and do nothing. But then in turn he notices that on the left track there will be five people
		who will be saved by redirecting the trolley, and concludes that redirecting is permissible
		because now the consequences are positive.}
The overall consequences function in Dave's reasoning as a defeater to the 
observation that redirecting the trolley will result in the death of the person on the right track, rather than as 
an end. 

While Carl is callously  and acting wrongly in having a person die as a side-effect of a trivial end, Dave is acting for a quite
serious end. It would be better if Dave adopted the lives of the five people on the track as an end as well, but given his
terror at the amputation, it is not reasonable to \textit{require} that (though it would be wrong for him to redirect the
trolley if the only other relevant effect he saw was the death of the person on the right track).

Our fourth response thus suggests a conjunctive proportionality condition in the Principle of Double Effect:
 \ditem{prop2}{First, the intended good is not trivial in comparison to the foreseen evil, and, second, the foreseen goods
    are proportionate to the foreseen evils.}

Now, the triviality condition clearly involves an apparently arbitrary parameter measuring relative triviality, and raises serious 
Mersenne questions. 

We have seen that of the available solutions to the problem presented by \textsc{Gunshot}, the two most tenable ones involve 
parameters calibrating closeness or triviality, and hence raise Mersenne questions.

Additionally, we should think a little about the condition that the goods---whether specifies as foreseen or as intended---are 
proportionate to the foreseen evils. To a first approximation, one might opt for a utilitarian analysis: the overall
utility is positive. We have already seen serious problems with this in ??backref in regard to uncertainty, expected value
and risk, as well as incommensurability. Further, there is probably a significant overlap between deontology and a friendliness
to partiality in ethics. Imagine a trolley problem where on the right track there is a stranger and a cat and on the left track
there is one's child. The utilitarian consequences of redirection are negative, but the fact that the person on the left track
is one's child seems to make it permissible (some will even say required) to redirect. 

But we have also seen in ??backref that taking relationships into account has many apparently arbitrary free parameters. So 
the complexity only multiplies.


\subsection{A miscellany of other Mersenne questions}
\subsubsection{Introduction}
There are many other cases which involve thresholds or transitions that appear to be arbitrary. We will discuss them briefly.
Some readers will doubtless disagree with a number of my examples. But a sufficient number of the remaining examples 
should still compellingly raise Mersenne problems. And the list is not
exhaustive: the reader should be able to generate more items, as the following are largely given as examples.

\subsubsection{Ethics: Gratitude}
If $x$ receives a benefit from $y$s $\phi$ing, the degree of gratitude that it is
appropriate for $x$ to show $y$ covaries with:
\begin{enumerate}
\item[(i)] benefit to $x$,
\item[(ii)] cost to $y$, and
\item[(iii)] deontic status of $\phi$ing (from wrong, to merely permissible, to obligatory, to supererogatory).
\end{enumerate}

If the benefit is $b$, cost is $c$ and deontic status is $d$ (e.g., negative for wrong, zero for merely
permissible, between zero and one for obligatory, and one or more for supererogatory), we will then have 
some function $g(b,c,d)$ that specifies how much gratitude is needed. This function has to include at least
one parameter we can raise a Mersenne question about, because the function has to convert from the units
of cost and benefit that $b$ and $c$ are in to some sort of units of gratitude. As a first
attempt, we might try the product rule:
$$
	g_1(b,c,d)=\alpha b^+ c^+ d^+,
$$	
where $\alpha$ is a constant of proportionality, and for any real
number $x$, we use $x^+$ to denote the positive part of $x$, i.e., $x^+=x$ if $x>0$ and $x^+=0$ if $x\le 0$.
This captures the idea that gratitude is only apt when there is a benefit from $y$'s $\phi$ing to $x$ (if $b<0$, 
then $b^+=0$). It also captures the idea that wrong and merely permissible actions, not being praiseworthy,
deserve no gratitude, since for wrong and merely permissible actions we have $d^+=0$.

But the $g_1$ gratitude function is not quite right, because it implies that when the cost is zero, the appropriate gratitude 
is zero, despite the broadly held intuition that if there is a God, we owe him gratitude for our existence, 
even though creation has no cost to an omnipotent being. 

We might try to combine $b^+$ with $c^+$ by addition rather than multiplication. This captures the idea that
gratitude can be appropriate even when there is no cost to benefactor, but it misses the fact that where there
is no benefit, no gratitude is appropriate. Perhaps a formula such as:
$$
	g_2(b,c,d)=\alpha b^+ (\beta+c^+) d^+,
$$	
is a better match to our intuitions, where $\beta$ is some positive value (in the same units as the cost). 
This does imply that where benefit or deontic status is zero or negative, no gratitude is appropriate, but
gratitude for costless things is appropriate. But now we have Mersenne questions about \textit{two} parameters.
Moreover, we have a Mersenne question about what grounds the gratitude formula having the exact form it does, with three
multiplications and two additions. 

But in fact things are even more complicated. We need to distinguish actual benefits received by $x$ from
the benefits that $y$ intended $x$ to receive. Both are relevant in subtly different ways to the degree of 
gratitude. Gratitude for trying to benefit need not be as large as for actually benefiting. And in the case
of costs, while expected costs are probably more relevant than actual costs, the actual costs incurred by
the benefactor likely enter into the rule of gratitude. We can thus expect a complex formula, with Mersenne
questions about the form of the formula as well as about a number of constants in the formula.

\subsubsection{Ethics: Private property}
There is something quite compelling about Locke's idea that if, in the state of nature, you mix your labor with 
an object (including an area of land), leaving as much and as good for others, you gain exclusive rights over 
the object. 

A number of Mersenne questions come up. What exactly counts as labor? Suppose I come across a stick in 
the grass, absentmindedly pick it up, and lay it back down. As a result, the stick is now a little easier to find, as 
some grass that was over it is now under it.  It doesn't seem right to say that I now own
the stick: my input was too minimal. 

What is the exact extent of the exclusive rights? Suppose that I have erected a large 
building, and a science teacher uses it to demonstrate how to measure the speed of sound by recording the sound of a loud 
clap and the ensuing echo. In doing so, the teacher has deliberately bounced sound waves off my building, thereby vibrating 
it. This is surely too minimal a ``use'' to fall under the scope of exclusive rights. The example suggests that perhaps use that 
does not impede the owner's use is outside the scope of the owner's exclusive rights. But that principle is generally 
denied: I don't get to use books in my absent colleague's office unless past practice makes it clear that my 
colleague consents. We have some idea of a minimal use that falls outside the scope of exclusivity, but where the 
line lies is unclear.

And how do we abandon property rights? I pick up a rock, and hit it against another rock a couple of times in the way
I would to knap it into the head of an axe. I then decide that perhaps this rock isn't the best candidate for an axe, and I
put it down. I do have in the back of my mind the thought that if one day I want to make an axe and I can't find a better 
rock, I will come back to this one. My memory of the rock fades gradually. Early in the process, I own this 
rock with which I mixed my labor. Eventually, I have abandoned the rock. When exactly? Just how weak a thought of ``I might 
come back for this'' is enough for me to continue owning it? 

These are nitpicky questions. But by the law of excluded middle, at any point either it is or is not permissible 
for another party to make a 
particular use of a perhaps owned object. What grounds exactly where, in the foggy area between the clear and unclear cases, the
use is permissible and where it is impermissible? 

\subsubsection{Ethics: Torture}
On strict deontological views, one shouldn't torture one innocent person to save any number of lives. But of course
it would be permissible to gently prick someone with a pin to save a single life. Somewhere between the pinprick
and the torture is a transition. What makes the transition be where it is?

On threshold deontological views, it is wrong to torture one innocent to save a small number (say, one or two) of lives,
but it is permissible to do so to save a very large number (say, a billion). Again, we have a transition to be 
explained.\footnote{I am grateful to Philip Swenson for this example.} And note that even if one is a strict deontologist
about torturing the innocent, likely one is a threshold deontologist about some other things. Thus, one may think it's
permissible to save an innocent life but not permissible to lie to get a deserved salary raise,
and hence there needs to be an explanation of the grounds of the transition from permissibility to impermissibility
as the gravity of the case changes. Or
one may think it is permissible to trespass on a neighbor's property to save a cat's life but not to save a grasshopper's.
I expect that every moral realist who isn't a full-blown consequentialist is a threshold deontologist about some things.
And each threshold raises a Mersenne question.

\subsubsection{Ethics: Self-defense}
It is widely held that lethal self-defense an aggressor is permissible. There are at least two questions that come up here.
The first is akin to proportionality in double effect: what degree of aggression licenses lethal self-defense. For instance, if an assaulting party clearly threatens only to shave your 
hair, lethal self-defense is unjustified, even though such an assault constitutes battery.??ref On the other hand, 
if the assaulting party is likely to kill you, then lethal self-defense appears permissible. 
Lethality of an attack is a natural line to draw, but it is not the right line. If an evil surgeon is going to 
cut off your healthy legs without your consent, lethal self-defense appears permissible even if 
you are confident of survival. Furthermore, the probability of the aggressor's succeeding in inflicting the 
threatened damage matters. Lethal force is not permitted against a malicious child with a foam sword, even though 
there is a tiny chance that the sword will trip you up, resulting in your hitting your head on a rock and dying. 
At the same time, certainty of the threatened damage is more than is required. Finally, what I called 
``lethal self-defense'' itself comes in probabilistic degrees: shooting an attacker in the head is nearly
certain to kill; hitting them with a rock is less so. So there is now a Mersenne question of 
what explains what combinations of probability and damage justify self-defense with what 
probabili lethality.\footnote{Some will say that one's intentions are also
relevant.  Aquinas, for instance, held that a private individual is not permitted to intend the death of 
an assailant.??ref}

The second, and more conceptually complex, question is what constitutes aggression. Again, there are clear cases of what does not 
and what does. If a terrorist hands Alice a weapon and informs her that she will be killed if Bob is still alive in fifteen 
minutes, and it is not physically possible for Alice to use the weapon against the terrorist, she cannot engage in lethal 
self-defense against \textit{Bob} on the grounds that by being alive he too is an aggressor. On the other hand, the terrorist's credible threat 
makes the terrorist an aggressor, and it is very plausible that Alice could legitimately engage in lethal self-defense against terrorist if it were 
to become possible.

In between the extremes of threatening by simply being alive and threatening by culpable voluntary attack is a range of cases, 
such as threatening by breathing (e.g., there is only enough air in spaceship for one of two people), threatening
by involuntary activity similar to voluntary activity (e.g., a muscle spasm is about to happen resulting in a detonator 
being activated), threatening in non-culpable non-moral ignorance (e.g., thinking one is controlling a video game when one is controlling an armed real-life robot), threatening by voluntary and right activity (e.g., a police officer using justified lethal force), wrongly threatening in non-culpable moral ignorance (e.g., innocently not knowing that the complex medical situation at hand is one where in fact disconnecting the ventilator is impermissible), and threatening in culpable moral ignorance (e.g., engaging in genocide because
it is more convenient to come to agree with the leader than to investigate the moral questions for oneself). 

Moreover, the lines between these options are themselves more continuous than may initially strike the eye. It may seem, for instance,
that it is very natural to draw the line between cases where the threatening party is acting rightly and where they are
acting wrongly. But now consider cases where the threatening party is acting only \textit{slightly} wrongly, for instance 
because they are a police officer engaging in a use of lethal force that is only slightly disproportionate.\footnote{There may
also be cases where the wrongfulness is not of the relevant sort to justify lethal self-defense. For instance, suppose that 
for the sake of personal witness to the importance of solving problems non-violently, Alice had vowed non-violence for life. Bob now threatens Alice's 
life in a paradigmatically murderous way, and Alice pulls out a gun and is about to shoot Bob in self-defense. Assuming that a vow of non-violence is morally 
binding, it is wrong for Alice to shoot Bob. But the wrongness of Alice's shooting Bob does not justify Bob's switching 
to ``self-defense mode'' and pulling the trigger before Alice can pull hers.} Likewise, one might think the difference between
inculpable and culpable ignorance is sharp, but there are cases where the ignorance is only slightly culpable. For instance, 
consider the case of the doctor disconnecting someone from a respirator incorrectly thinking that the disconnection is permissible,
where the reason the doctor was mistaken about the morality of the situation was because she was five minutes late for an 
ethics lecture many years ago in medical school, and their lateness was only very slightly culpably wrong (they should have 
put slightly more effort in running to catch the bus). 

It is thus unlikely that there is a natural place where we draw the line as to the nature of the aggression. But given realism there are
truths of the matter, and a Mersenne question as to why they are what they are.

\subsubsection{Ethics: Respect}
We need to show respect for rational beings. This respect includes such things as not killing them when they are
innocent and non-aggressive, not eating them (except perhaps in extreme circumstances), not acting as if they were fungible, treating them as ends rather than as
mere means, and so on. But what is a rational being? First, we have a distinction between an individual- and a 
kind-based
concept of rationality: on the former, a being is rational to the extent that it currently has certain rational powers;
on the latter, a being is rational to the extent that it is of a kind that should have certain rational powers. But whichever we choose, and
plausibly there are principled reasons to choose one rather than the other\footnote{Though they will be highly controversial, since 
a significant part of the fraught debate about the moral status of human zygotes, embryos and fetuses turns on this.}, 
we still have a Mersenne question as to the degree of rational power---whether actual or proper to the kind---that is 
needed for us to have duties of respect. Rational powers, after all, clearly come in degrees, and if at some point respect
is called for, we need an explanation of why that point shows up where it does.

The question of what degree of rational powers is needed for respect is one that we actually face with regard to our treatment
of higher mammals on earth, and merely hypothetically face with regard to extraterrestrial life. 
It is an important question. But, as usual, the Mersenne puzzle isn't that of determining what the fact is, but of what makes 
an answer be an answer, especially in light of the appearance that any threshold will be arbitrary.

\subsubsection{Ethics: Punishment}
Punishment should not be disproportionate to a crime. \textit{Lex talionis} would provide a neat and elegant account
of this: we do to criminals what they did to the innocent. But it is morally abhorrent to torture torturers or rape rapists. And even if we accept such
abhorrent extremes, there are cases where punishment simply has to switch types of harm. If a thief does not have enough
honest property of their own to make possible a deprivation equal to what they stole, imprisonment is a plausible alternate currency,
but there is no simple and elegant formula providing an exchange rate between the value of stolen property (perhaps
corrected for the economic state of the victim) and length of imprisonment, unless it turns out---as is quite unlikely??back/forwardref---that 
that there is a precise way of quantifying personal utilities. Maybe there is no simple and elegant formula, but at least
a  complex one. If so, we have our Mersenne question about what grounds it. And even if there is no specific formula, there are bounds of moral acceptability: a day's imprisonment for stealing and destroying a car is insufficient; 
a lifetime's imprisonment for stealing a book is excessive. And what grounds the transitions between acceptability
and unacceptability?

\subsubsection{Ethics: Consent}
Standards of consent necessary to permit one's being treated a certain way vary widely depending on the treatment.
There are multiple dimensions in which we can measure the ``strength'' of a consent requirement: how well informed the 
consenting party needs to be, what age or level of intellectual development the party needs to have, what proxies if
any can legitimately offer consent on the party's behalf, how unpressured the consent needs to be, how clearly formulated the consent
needs to be, whether the consent must be specific to the case or whether prior blanket consent suffices, etc.
Under ordinary circumstances, only a clear lack of refusal is needed for a pat on the shoulder. The permissibility
of major surgery, however, has a consent requirement of significant ``strength'' along many of the above axes. 
And the permissibility of sex has a consent requirement of even greater ``strength'' along some of the above axes,
since while proxy consent and prior blanket consent can suffice for major surgery, they do not suffice for sex.\footnote{It is tempting
to explain this in terms of the fact that surgery---or at least the sort of surgery for which proxy consent suffices---benefits 
the patient regardless of the patient's consent, while sex is only beneficial when consented to. But this is arguably false.
Parents can validly consent to an organ transplant between their children, even if the donor is not expected to benefit
on balance (though generally there is a benefit from having one's sibling alive!).} The mapping between
the form of treatment and the multidimensional ``strength'' of consent is of great complexity, and has an appearance of significant
arbitrariness. What grounds it?\footnote{For more on this argument, see Durham??ref.}

Now, at least some of the norms regarding consent are constituted by social expectations. For instance, it is likely
that many of the expectations concerning consent to companies' use of our personal information are such\footnote{This
is example is due to Anne Jeffrey.}, and a society
where these expectations are more stringent or less stringent need not be corrupt. However, first, not all norms
regarding consent are like that. A society that does not require sexual consent just \textit{is} corrupt, no matter
the context, and much if not all of our increased focus on patient autonomy in medical ethics, together with consequent
emphasis on consent, is genuine moral progress. And even in the case of privacy, it is plausible that some
expectations are not merely socially constituted---it would plausibly be wrong, no matter what society
said, to follow non-consenting people 24-hours a day with wall-penetrating sensor technology and broadcast all 
their activity to all willing to pay a small fee. Thus we need a ground for the distinction between the cases where the 
need for consent comes from social expectations and where it is independent of social expectations. 

And even in the case where consent is constituted by social expectations, the mapping between social expectations 
and moral norms is likely a quite complex function involving apparently arbitrary parameters that raise Mersenne
questions. After all, it's not always wrong to violate social expectations. Presumably, there is some kind of a
proportionality between the strength of a social expectation and the strength of the resulting moral norm (how
easily it can be overridden by other considerations, how much of a risk of violation it is worth taking, etc.),
while the ``strength'' of a social expectation is a complex thing constituted by a distribution of individual
expectations. Both the method of aggregating individual expectations into a social expectation and the proportionality
constant between social expectation and normative force raise Mersenne questions.

\subsubsection{Ethics: Normative powers}
Consent is one special case of a normative power.
We have a number of normative powers that we exercise through communicative acts with specific illocutionary force.
With promises, we create obligations for ourselves, and with commands, we create obligations
for others. With requests, we create reasons for others, and with permissions, we remove reasons for others. 

The scope of our normative powers has limitations, though where exactly the limitations lie can be a matter of 
controversy. Perhaps the clearest case is requests, where as a society we have developed a broad spectrum of
levels of insistence, signaled by linguistic and extra-linguistic cues. Normally, 
more insistent requests create stronger reasons and less insistent ones create weaker ones. We do not appear
to have any lower limit on the strength of reasons we can create solely by requests. We can always add yet another
``But, please, don't let me impose on you'' to weaken the request. However, there is a contextually-variable
upper limit on the strength of reasons created by a request. One can roughly measure the stregth of these
reasons, say, by the cost to the requestee at which fulfillment becomes pretty unreasonable. The strength of
the reasons to fulfill a request is then a function of variables including the intrinsic reason provided by the requester's needs
(if someone is starving, one has reason to offer them food even if they don't ask for it), the degree of
insistence, and the relationship between the two parties.\footnote{Note that the relationship may itself have been modified
by the fact of the request. A friendship might be damaged by an unreasonable or rude request, and strengthened by 
the vulnerability revealed in a disclosure of need.} Normally, then, the more insistent the request, the more it skews
the requestee's reasons in favor of the requested action, but there is a limit to how far one can skew the strength of
reasons away from the no-request \textit{status quo}. If I am not actually in poverty, requesting money from strangers
for my personal pleasure cannot create a very strong request-based reason, no matter how much I ask for it.\footnote{Though
of course I might create a prudential reason to give me the money to shut me up, or out of fear that I am a mugger, but
that's not a request-based reason in the sense I am talking about. There are also unusual cases where a request on
balance creates a reason against doing something, such as when it is appropriate to punish a child by withholding
from them something that they specifically want, but are not request-based reasons.}

We have similar limits on the strength of reasons coming from the exercise of other normative powers. The example of
promises is particularly interesting here in that we might think that our normative powers are strongest here since 
we use them to bind our own wills. But notice that while I can probably make a promise to defend my friend's life 
where the promise creates a reason whose strength is such that I am obligated to seriously risk my own life, no
promise to come to my friend's party can create a reason strong enough to stand against serious risk to my life, unless
there is something very special about that party.\footnote{Perhaps the party is the best hope for reconciling with 
someone who is dying of cancer, and my friend cares deeply about the relationship.} Even in promises, thus, there are 
serious limits to how far we can affect the reasonableness of our decisions. 

Social convention sets certain aspects of the mapping from communicative acts exercising normative powers to the normative
effects. It determines which normative powers are exercised in which words or gestures, and how various communicative and
extra-communicative features affect the strength of reasons. But social convention works within the limits discussed
above. Social convention can \textit{say} that if I spit on my hand and shake hands after promising to come to your
party, then I am obligated to come to the party even if it costs me my life, but in fact this action would not create
any such reason, since skewing the intrinsic reasons thus far is just as beyond our normative power as running a one minute
mile is beyond our locomotive power. But while we can find a physical explanation for the locomotive limits on an 
individual human, the normative limits raise Mersenne questions about all of their free parameters---and they have many,
since there are multiple normative powers and in each one the limit will depend on multiple contextual factors.

\subsubsection{Political: Constitution problem}
Political philosophy also provides a number of examples of seemingly arbitrary parameters. Consider the constitution problem. A state has a written or unwritten
constitution specifying what must happen for legislation to be valid and hence authoritatively binding on the citizens. But 
how is a constitution instituted? One theory is that it happens by the consent of the people.??Aquinas But obviously for any state
of sizeable size it will be false that all the people have consented: some have not made their opinions heard and some have been
overruled. 

Requiring ``consensus'' or a supermajority raises the question of exactly how many dissenters can be tolerated, and
once that question is answered we have a Mersenne question as to what grounds that cutoff being where it is. Requiring a simple
majority or plurality involves one less free parameter: the cutoffs in having more than half of the voters or having more voters
than any alternatives seem non-arbitrary. However, even a majority or plurality based system leaves questions about other parameters.
Does one need a quorum of the governed? It would not seem right, for instance, to have a vote on a constitution on a day 
where the bulk of the population is unable to get the polls due to a hurricane or a war, and as a result only a small number of
unrepresentative citizens can express their opinion. But if a quorum is required, then of course we have a Mersenne question
as to exactly what grounds the facts about what counts as quorum.

Voting cutoffs and quorums are relatively easy to quantify compared to the question of who the people giving their consent are?
Presumably, small children should not be eligible. But where do we draw the line between small children and paradigmatic adult
deciders? Any age-based line raises several Mersenne question---one about the numerical age cutoff and multiple questions about 
how age is measured (from fertilization, implantation, brain development, beginning of the birth process, completion of the
birth process, etc.)? A cutoff based on mental capacity, on the other hand, involves many parameters that need to be set, because
there is no single measure of mental capacity, and so one needs to have multiple measures with their respective weights. Moreover,
we have a decision point on whether those who do not get a vote have proxies voting for them (e.g., parents) and, if so, who 
counts as whose proxy.

Or consider such details as how well-publicized the constitutional consultation needs to be, how clearly spelled-out the 
constitution needs to be for people's vote on it to be valid, how voting with more than two options is done (for, surely,
there are more than two options for constitutional questions), and who gets to decide which options are presented for
voting?

Many of the above questions only make sense in the case of a formal consultation process of a sort that has occurred rather
rarely over the course of human history. If we are not to think the vast majority of polities to be illegitimate, the account
needs to allow for implicit consent, maybe of the sort involved in social customs. But there things become even less clear.
There will almost always be some citizens who regard the state as illegitimate---indeed, some regard any state 
as illegitimate. For many people, acceptance of a political system's legitimacy is not an simple binary question, but 
something that comes in degrees and has contexts. Imagine that two thirds of the population has a credence of two thirds
that the political system is legitimate, and the remaining third of the population has a credence of ten percent in 
the system's legitimacy, and they express these credences in their actions. Should we then look at the average credence
of the relevant citizens (e.g., those of age)---$0.48$ in my example above---and see if it meets some cutoff? If so, the
cutoff will raise Mersenne questions. Moreover, people often do not just have a single credence it the all-or-nothing 
legitimacy of the political system, but rather have different credences regarding different aspects of legitimacy: is this
a state that has the right to use violence against its citizens to enforce laws, is it a state that has a right to levy taxes,
to draft citizens to defend it or do other work for it, etc.

\subsubsection{Political: Dissolution problem}
A set of Mersenne questions similar to those raised by the constitution problem is raised by the dissolution problem: the question
of when it is that a political regime becomes illegitimate, and the respects in which it may be illegitimate (perhaps the
street traffic regulations of the Third Reich were legitimate, but little else was). One might think of dissolution as resulting from the state's failure to
keep its side of the social bargain. But there has probably never been a state that kept its side in every respect. It seems to be only
gross failure that implies illegitimacy, but that raises a Mersenne question about what grounds the relevant degree of grossness.

\subsubsection{Political: Scope of authority}
We have a set of Mersenne questions regarding who lies within the scope of the state's authority. Typical human states
have authority largely but not entirely defined geographically---for an exception, consider ships under a country's flag that
move on the high seas. Geography itself raises Mersenne questions. Suppose we say that a state in the future has authority
over the same territory as it now occupies. But what grounds something counting as ``the same territory''? We live on a planet that is constantly
changing its shape, whether at a large scale due to movements of tectonic plates or a small scale due to children digging holes in beaches or a medium scale due to states building new off-shore islands. When
a tectonic plate shifts, how does territory shift? There are precise ways one might try to answer these questions. We define
latitude and longitude in terms of coordinates on a mathematically idealized oblate spheroid approximating the earth. We might
then define ``same territory'' in terms of these coordinates. But there are infinitely many ways of mathematically modeling the
earth at any given time and infinitely many ways of matching that model to the physical soil of the earth over time.
And it seems unlikely that the geography of a planet is in the end what defines a state. It may well be the case that in the future 
a significant proportion of the earth's population will live on space stations or in the asteroid belt, and we will have ship of Theseus kinds of
questions about identifying the sameness of a space station over time, and difficult questions about defining segments of the asteroid
belt. 

One might say that a state, or its people in their constitution, gets to define 
its own understanding of who counts as among the governed, and so a state can opt to define the governed as those occupying a 
certain portion of a certain specific diachronic oblate spheroid model or to define the governed as those within a certain specific
distance of a particular landmark. But there must be limits to a state's normative power to define these boundaries, since otherwise
a state could simply swallow up territory by mere fiat. Imagine, for instance, if France defined its territory in terms of all land
within five hundred kilometers of the Eiffel tower, and then French agents conquered the world by secretly taking small bits
of the tower and distributing them worldwide so that no place was more than 500~km away from one or more of them. It is very plausible
that a state has some freedom in how it defines its boundaries, but that freedom is limited. And the range of ways of defining the
scope of authority seems like the sort of thing that would have many different parameters without privileged values, and hence 
raises many Mersenne questions.

One may wonder why questions about the legitimacy and scope of a political system are being raised as part of a discussion of ethical
questions. There are two reasons. First, we have a moral duty to obey the commands of a legitimate state. Second, only those
acting on behalf of a legitimate state are morally permitted to make certain onerous demands on the population, especially ones backed up 
by threats of violence.  

\subsubsection{Just war}
Traditionally, three conditions are required to enter into a war justly: 
\begin{enumerate}
\item[(i)] the cause must be just,
\item[(ii)] the value of the cause must be proportionate to the expected harms, and 
\item[(iii)] the chance of success must be 
sufficiently high. 
\end{enumerate}
Let's focus on (ii) and (iii). Note first that the conditions need to be combined rather than having independent cutoffs. If the threat posed by the enemy is sufficiently large---say, a threat of large-scale genocide---a small chance of success is sufficient to justify fighting back.

As in many questions like this, 
an expected-value utilitarian has a simple account of (ii) and (iii): you just compute the expected utility 
of waging war and not waging war. I will discuss utilitarian solutions in general in more detail in ??forwardref. 
But here is an intuition that suggests a problem with such solutions here. Suppose Country~A intends
to invade solely in order to murder the inhabitants of a small village with 1500 inhabitants in Country~B, 
and the best military analysis shows that the attack can be easily repelled by using drones which will 
kill 2000 invaders with no friendly causalties. Clearly fighting back is right, even though on a short-term
utilitarian analysis 2000 lives are lost by fighting back, and 1500 by not fighting back. Of course, there 
are standard things utilitarians will say: not fighting back will encourage more slaughter in the future,
and so on. But it is easy to imagine cases where there will be no such encouragement. (E.g., this war might occur
during a short malfunction in force-fields between countries, which force-fields when functional are perfectly 
effective at keeping peace, and a new engineering innovation can make the force-fields practically a hundred
percent functional.)

This intuition is a part of the general conviction that in just war-time decisions, one prioritizes friendly non-combatants
to friendly combatants to enemy combatants (where exactly enemy non-combatants fit is more controversial).??refs
But the prioritization is not lexicographic: it is wrong to kill a million enemy combatants to save one friendly combatant.
Thus, as many other cases, we have weights in proportionality computations---both in the \textit{ad bellum} considerations of 
proportionality as well as in \textit{in bello} decisions---and there do not appear to be any ``natural and non-arbitrary''
values for these weights. Once again, then, whatever these weights are, we have Mersenne questions as to what makes them 
be what they are.

\section{Arbitrariness and continuity}
Whatever the values of the parameters in the ethical Mersenne questions are, these values appear likely to be such 
that if we knew their exact values, we would find them arbitrary, like the 1102 earth-radii that Mersenne says
is the distance between the earth and the sun at apogee, or the actual number of 23872.
In physics, only a few seem to hold out a hope that the fundamental constants in the fundamental laws of nature may be ``nice numbers'' like
$2$, $\pi$, $\sqrt 2$ or $e$. It seems intuitively even less plausible that things would so turn out in ethics. 

And even if the parameters turned out to be such ``nice numbers'' in ethics, that would itself be a very surprising fact, because while, say,
$\pi$ seems very natural in physics, it seems rather less natural in ethics. Imagine that you should benefit your 
parent over a sibling just in case the ratio of benefits to parent vs.\ to the sibling is no lower than $1:\pi$. That would itself seem arbitrary.
It seems that whatever the numbers turn out to be, they will have an appearance of arbitrariness and of contingency.

Many of my examples involve thresholds, such as the amount of rational capacity needed for respect or the degree to which a government
needs to care for the common good to have authority. It is plausible to reject the idea that there are discrete thresholds, and instead hold 
that there are continuous functions, say a function $r(x)$ specifying the degree of respect required to be shown to a being with rationality
of degree $x$. 

But then instead of explaining one threshold, one needs to
explain the whole complex shape of the ``respect function''. On the most naive version of this, rational capacity will be graphed along one axis
and respect on another, which will raise Mersenne questions about the slope of the graph at various points, the positions of any inflection points, and so on. 
And in reality, both rationality and respect have many dimensions, so what we have is a complex function of many arguments and whose
values are multidimensional. In general, moving from thresholds to continuous functions only multiplies the degrees of freedom
among the ethical parameters, and hence multiplies what is needed for explanation. Continuity does not solve the problem.

\section{The human nature solution}
\subsection{The solution}
On our Aristotelian picture, the nature of an organism grounds norms about what the organism's structure and behavior 
should be. In particular, the nature of the organism will ground many arbitrary-seeming norms, such as those governing
the range of appropriate sizes of Indian elephants, the migratory behaviors of monarch butterflies, and the lengths of 
human femurs. Possession of thes nature makes the organism be the kind of organism it is, and imposes on it the associated norms.

In the case of humans, behaviors include voluntary ones, and so it is unsurprising that there are norms governing these
as well. And just as there are many parameters governing bodily structure and sub-voluntary behavior, there are many parameters
governing moral behavior, all grounded in the form. 

At the same time, Aristotelian optimism provides us with evidence as to what the parameters approximately are. The actual
bodily structures of humans give defeasible evidence as to what normative human bodily structure is and the actual behaviors
of humans give defeasible evidence of moral norms. And in both cases, we have ways of identifying healthier or more virtuous 
paradigms, using the optimistic idea that the various ways of doing well tend to hang together with some
degree of unity, and the structure and behavior of such paradigms gives us further evidence as to the norms.

Admittedly, there appears to be a disanalogy between health and virtue. We might use a moral exemplar 
like  Mahatma Ghandi or Mother Teresa to 
figure out moral norms, but we wouldn't use an athletic exemplar Usain Bolt or a Serena Williams to figure out physical norms. One explanation 
of the difference is that Bolt and Williams have highly-developed traits that are specialized to a forms of life quite
different from that of the typical human---namely, the life of a professional athlete---while Ghandi and Teresa's excellences in justice, fortitude and mercy are as important in circumstances as in theirs. 

The Aristotelian response to the Mersenne questions also raises the question of why the form includes these norms and not others. But here there is an easy 
answer available. The form is at least partly defined by the norms it includes. Thus, Mersenne's question about the lion and
the ant when reformulated into normative terms, as the question of why the lion's strength \textit{ought to} be greater
than the ant's, is easily answered: this follows fron defining features of what make lions be lions and ants be ants. 

The appearance of arbitrariness and of contingency in the ethical Mersenne problems is somewhat misleading: it is like the appearance of arbitrariness and
contingency in the facts that water is H$_2$O and that carbon atoms have six protons. Water couldn't have a different chemical
structure and carbon atoms couldn't have a different number of protons. But it is also an important truth  here that there 
could be other substances that could have a different chemical structure or a different number of protons and similar behavior.??ref:Kripke Similarly, \textit{we}
couldn't have norms of preferential treatment other than the ones written on our nature, but there could be---and perhaps in
this vast universe are---other rational animals with other such norms.

\subsection{The incredulous stare}
At this point, an objector may offer an incredulous stare. How can our human nature contain such a vast amount of 
moral information as the solutions to all the Mersenne questions require? 

However, as Durham??ref:dissertation notes, it is plausible that a substance's nature already grounds many complex dispositional facts about the substance. Furthermore, it is also plausible that it grounds many non-moral normative facts,
such as about what size of heart is normal for a given body size, what force a bone ought to be able to withstand, and so on. Given that it is already this informationally rich, why can't we also have great informational richness in the moral regime? On robust Aristotelianism, the form is not a mere summing up of the arrangement of the parts, so it can be
extremely rich informationally.

In addition to this, there is also a somewhat technical line of thought. Let's try to quantify how much information
needs to specified for the Mersenne questions. In some cases, we have a finite number of alternative options, and 
the question is which of them is right (e.g., which hierarchical ordering on some finite set of incommensurable goods 
is the right one). Here, the information content is finite, and hence is that of specifying a single integer in a 
finite range. In other cases, we can encapsulate the answer to a question
in a finite number of real numbers, say ones specifying the relative moral weight of some set of considerations.
In these cases, the information content is that of a finite sequence of real numbers, which can be proved to have 
at most the information content of a single real number.\footnote{\label{fn:encode}A single real number can encode two real numbers.
One way to see this is to note that a real number $x$ strictly between $0$ and $1$ can encode any real number
$y$, say by the formula $x = (1/2)+(1/\pi)(\arctan y)$. And a single real number $x$ in that range can also encode 
any \textit{pair} of real numbers $y$ and $z$ in that range. If $y=0.y_1y_2y_3...$ and $z=0.z_1z_2z_3...$ in decimal,
we can just write $x=0.y_1z_1y_2z_2y_3z_3...$. Iterating, any finite sequence of reals can be encoded as a single real.} In other cases, perhaps we can encapsulate the information needed
for a Mersenne question as a finite collection of continuous real-valued functions each of a finite number of 
real-valued arguments. But any such function can be encoded as a single real number.\footnote{To specify a 
continuous function of real valued arguments, we only need to specify its values---which are real numbers---at points that have rational number coordinates, since at irrational ones the value can be determined by a limiting procedure. There are
only countably many rational numbers. But a countable sequence of real numbers can be encoded as a single real
number.??ref} In other cases, perhaps, the information for a Mersenne question may be specified as an open subset of
some finite-dimensional Euclidean space, and that can be encoded as a single real number, too.\footnote{$^*$An open subset
can be specified by specifying all the points with coordinates that are rational numbers. The set of points whose
coordinates are rational numbers is countable. Thus, an open subset can be specified by giving a subset of the natural
numbers, and any subset $S$ of the naturals can be encoded as a single real number---say, by putting a one in the
$n$th place after the decimal point if and only $n$ is in $S$ and putting a zero there otherwise.} As a result, it
may well be the case that all our ethical Mersenne distinctions can be specified to giving a finite sequence of 
real numbers, and, as before, a finite sequence of real numbers can be encoded as a single real number. 

Thus, as far as our arguments go, the total informational content in human nature need only be equal to a single real number. And it is plausible that even a very simple substance can have a nature whose informational content contains a 
single real number. For instance, an electron's natural causal powers might encode the fine-structure constant 
that measures the strength of electromagnetic interactions, and for all we know, the fine-structure constant has
the informational content of a real number. Indeed, this observation about electrons shows
that there can well be great complexity in the nature of a simple thing.

Given Aristotelianism and if we live in a continuous world, it is plausible that apart from ethics our 
human nature does contain the informational content of at least one real number---say, a number specifying 
what breaking strength a human femur is supposed to withstand, or maybe the fine-structure constant since the human
body engages in electromagnetic interaction. Since any two real numbers can be encoded as one\footnote{See note~\cite{fn:encode}, above.}, it follows that
our solution to ethical Mersenne problems does not require human nature to have any more information than it 
may have independently of these. 

And it might be that we don't need to go all the way to a real number, that in the end the amount of information in
human nature is finite, since maybe we live in a discrete rather than continuous world (Cf.??ref:Pruss:infinity), and
all the parameters we need to specify can be finitely specified. If so, then maybe the fine-structure constant is also
capable of finite specification---but it is still epistemically possible that it has a finite but extremely long specification, and hence that the answers to the Mersenne questions in us do not exceed the informational content
of an electron.

\section{Other solutions}
We have many Mersenne questions pointing to arbitrary-seeming parameters in ethical rules.
I will now argue that a broad spectrum of non-Aristotelian ethical theories and solultions are unlikely to yield good answers to the Mersenne questions
or else raise new Mersenne questions of their own.

\subsection{Kantianism}
Kantianism  derives moral rules from the very concept of objective rationality. Famously, this leads to difficulties in
accounting for the substantive content of rules. For instance, from the point of view of objective rationality, it is difficult
to generate a presumption in favor of causing pleasure and against causing pain. The more tightly connected a moral rule is to the
specifics of the human condition and of the circumstances, the more difficult it will be for the Kantian to account for it. But the Mersenne questions above
thrive precisely on such detail. Consider, for instance, the improbability of a good Kantian account of how much we 
should, other things being equal, favor siblings over cousins, or of why proxy consent is sufficient for surgery but insufficient for
sex. The ``logical distance'' between the high level principles, like the categorical imperative to treat others as ends or to act according to universalizable rules, and such specific moral content appears unlikely to be bridgeable in a pure Kantian account.
Thus, precisely those cases that we have seen to raise compelling Mersenne problems make pure Kantianism an implausible ethical theory.\footnote{There is some hope that combining Kantianism with Aristotelianism would help??ref:Maglio, but then
we will need to have Aristotelianism, which is what I am arguing for. }

Of course, such appearances can be deceiving. One might well have antecedently thought that the relatively simple axioms of set 
theory are unlikely to generate the richness of mathematical theorems that we have seen to come from them. So it would be good
to go beyond an intuition of ``distance''.

There are at least four ways to do that. First, proceed by intuitions regarding a specific example. Consider two different moral rules regarding to the relative treatment
of siblings and cousins:
\begin{enumerate}
\item[(i)] Benefits to siblings are to be slightly preferred to benefits to first cousins
\item[(ii)] First cousins and siblings are to be treated on par.
\end{enumerate}
Neither rule requires us to treat anyone as a mere means or 
takes away from treating people as ends. Both rules are universalizable. So we are not going to be able to derive one rule rather
than the other from pure Kantianism.

Second, we can makes use of a heuristic that I employ in checking whether a numbered argument 
given by undergraduate students is valid, i.e., whether its conclusion logically follows from its premises.
I look to see if the conclusion of the
argument contains any substantive terms that do not appear in any of the premises. If it contains such terms, it is in unlikely that the argument is valid, though of course there are possible exceptions. If the premises are contradictory, then the logical rule of explosion makes every conclusion a valid consequence; alternately, the conclusion might be disjunctive and the
substantive term that did not occur in the premises occurs in one disjunct while another disjunct follows from the premises (though 
I have yet to see this happen in a student paper). Now an argument from premises about the nature of rationality as such
with a conclusion about specific familial relationships or about specific human activities such as sex or surgery fails the heuristic,
and hence is unlikely to be valid. And the cases do not seem to be like the most obvious exceptions---the premises are not contradictory
and the conclusion is not disjunctive.

Third, all or most of the examples that raised Mersenne questions have an appearance of contingency to them, in a way that does not
fit with the hypothesis that they derive from necessary principles about the nature of rationality. One way to formulate this
contingency is to note that many of the rules are ones that we would not expect to apply to other rational species. If we
came across an alien species that regarded familial ties as somewhat more or somewhat less important than we think permissible
for humans, we should not judge them immoral. It would not surprise us if other rational animals---perhaps ones occupying
other niches---were rationally or morally required to take greater or smaller risks than we.\footnote{One thinks, for instance,
of the Klingons and Kelpians from the Star Trek universe, respectively.}

Finally, we have an epistemological argument. While clearly we do not know the exact values of the parameters in the Mersenne
questions, we have some approximate knowledge, as already indicated above in a number of the cases. We clearly did not come
to this approximate knowledge by logically deriving it from Kantian first principles. Nor did we even do so by means of an
intuition that they follow from these principles. For I take it that we do not in fact have an intuition that, say, the
preference for siblings over cousins follows from Kantian principles. If anything, we have an intuition that it does not.
So, it seems that if these rules in fact follow from Kantian principles, it's just a coincidence that our beliefs about
the parameters are correct, a coincidence that makes the beliefs be mere justified true belief rather than
knowledge. But the beliefs are knowledge. So, the Kantian explanation does not work. 

The epistemological argument has some force, but less than the other arguments. First, the argument is related to the highly controverted
literature on evolutionary debunking arguments.??refs Second, a theistic reader has an easy way out of the argument:
God knows what values of parameters in fact logically follow from Kantian principles and could either directly instil in
us correct beliefs about them or ensure that we evolve in a way that yields such true beliefs. Nonetheless, we have
a strong cumulative case against there being a Kantian solution to the problem.

\subsection{Act utilitarianism}
The main problem with act utilitarianism is that it generates incorrect moral claims. It says that a healthy patient
whose organs can save three others can be killed when doing so doesn't have any other countervailing consequences such
as making others more callous. It says that if you and I are loners who make no contribution to society, but I own
a dog and you don't have any pets, then you have a duty to sacrifice your life for mine, to save my dog from being
ownerless; and if neither of us has a pet, but you enjoy chocolate a little more than I do while everything else is
equal, then I have a duty to sacrifice my life for you, since your life would include slightly more utility.

The act utilitarian, of course, can counter that such idealized cases are rare to non-existent and our intuitions
regarding rare cases are unreliable. A perhaps more serious worry is that while utilitarianism resolves some
Mersenne questions, it shifts the bump under the run to others. For instance, we already saw in ??back that for utilitarianism to be plausible and not swinish requires a hierarchy of goods,
and there will be Mersenne questions regarding the hierarchy.

But now note that even hard-nosed desire-fulfillment or hedonistic utilitarianism that does not posit a hierarchy
of goods will fail to escape Mersenne
questions. For there are multiple mental state concepts that are good candidates for corresponding to the words ``desire'' and ``pleasure'', with none of them morally privileged in a clear way.

When the psychotherapist tells Jones that she always unconsciously wanted to kill her mother, is that a ``desire''
in the sense of desire-fulfillment utilitarianism or not? Reflection the complexity of human life, e.g., as seen in literature??ref:ColinAllen?, shows that there are likely
to be many ``desire''-type concepts. We have desire as a felt attraction, inchoate or clear, as an overt behavioral
tendency, as a hidden pattern among one's behaviors, as a belief in the worthiness of a goal, or as a personally
stipulated explicit utility assignment. We have an orthogonal distinction between first, second, and higher order desires. 
If only one precisely specified option---say, second-order felt attraction---counts as defining our well-being, we
have a Mersenne question as to which is the one that is the true goal of moral pursuit. But if, as is more plausible, more
than one such option is worth pursuing, the proper ``combination function'', or method of generating a weighted
overall value, raises Mersenne question.

Next, consider the different ways of quantifying the strength of a desire. The decision theorist has perhaps the
only clean option. An ideal agent's strength of a desire is identified with the utility assignment which can be computed from binary
preference relations between different probabilities of outcomes: if you would choose a 30\% chance of $A$ over a
60\% chance of $B$, then you want $A$ more than twice as much as you want $B$. But deriving a utility assignment
from such binary comparisons only works if your binary preference relations satisfy an appropriate set of axioms,
such as the ones for the von Neumann--Morgenstern Representation Theorem. However, likely no human being's 
binary preferences satisfy the axioms, and so we need a function from the binary preferences
of non-idea human beings to utility assignments. There does not appear to be a privileged way of choosing such
a function, and so whichever function in fact defines the human good raises the Mersenne question of why this
function and not some other that fits about as well with our intuitions. 

And even binary preferences are rather problematic to define. Quite likely if you slightly change the wording
of your question as to whether I choose a wager with a probability $p$ of a philosophical achievement $A$ or 
a wager with a probability $q$ of a certain large sum of money, I will give different answers. What way of presenting
the wagers defines my actual preference between them raises another Mersenne question.

And as for pleasure and pain, we will again have a broad variety of concepts and a multiplicity of ways of quantifying them.
This can perhaps best be seen if we think about the mental life of possible and actual non-human sentients. Does a particular
state of an earthworm count as a pleasure? It is unlikely to be exactly like a state of ours. There will likely be many ways
of classifying mental states across species, and on some the worm's state will be a pleasure and on others it won't. So we have
a degree of freedom in our act utilitarianism as to what we count as pleasure or pain in non-humans. And even within humans 
there are complex questions. Consider for instance masochism or the subtle morose ``satisfaction'' of the pessimist who sees 
everything going downhill. There are likely to be different ways of classifying states as pleasures or pains, and the hedonistic
utilitarian will have a Mersenne question as to why one rather than another classification is the one that defines ethics.

Finally, if our utilitarian is also a physicalist about the mental, the problems increase further: there are
presumably infinitely many functions from brain-like states (brain-\textit{like}, since aliens may not literally
have \textit{brains}) to desire-like or pleasure-like states. The functions are apt to have a multiplicity of parameters
(??forwardref), and thus we will have a number of additional Mersenne questions. 

\subsection{Rule utilitarianism}
Instead of requiring that each action optimize total utility, rule utilitarians require that each action
follow rules that are themselves optimized for total utility. The main advantage of this is that we have a hope
of escape from the counterintuitive consequences of act utilitarianism. The rule not to kill the innocent may well be the
optimal rule for us, even if in a lifeboat situation it would maximize utility for the two stronger people to kill
and eat the weaker third. For a rule that allows such occasional killings of the innocent would lead to a lower
sensitivity to human life, and humans possessed of such a rule would likely misjudge the applicability of the rule
in a way that would reduce total utility.

Rule utilitarianism can explain the apparently arbitrary specifics of the moral rules. The optimal rule of preferential
treatment is likely to be messy and complicated, because optimizing utility for humans with their messy lives is
complicated. We thus have a neat response to Mersenne questions. Furthermore, we have a good explanation of the
appearance of arbitrariness and contingency. For the optimization procedure that would define the moral rules would be a vast
and complex one, taking into account the impact of the actions falling under the rules both in the short and the long run, 
both on humans and on non-humans. It is unsurprising if a complex optimization procedure produces results that seem
arbitrary but are in fact carefully chosen to their end. A computer-optimized airplane wing will have precise angles and
bends that cannot really be explained without running through the whole computation. Furthermore, the precise values
are likely to be contingent, in that they will depend on details of the laws of natures, and the laws of nature appear
to be contingent (and if laws of nature \textit{merely} appear to be contingent, then we still have an explanation of 
the \textit{appearance} of contingency).

Moreover, rule utilitarianism is less prone than Kantianism to make our limited but true beliefs about the moral rules be 
merely coincidental. For we have evolved biologically and mimetically in the service of survival and reproduction, and 
because of the perhaps contingent connections between these goods and other aspects of utility, evolution put pressures on us 
that directed our moral beliefs in a truthful direction. There are deep and difficult questions whether this is enough
to make the connection between our beliefs and the truth be sufficient for knowledge??refs, but there is more hope here
than on the Kantian side.

However, famously, rule utilitarianism divides into two varieties, depending on exactly what the rules are optimized for.
On ideal rule utilitarianism, the rules are such that everyone's successfully following them would be optimal, even if in
fact they are too difficult for us to follow. Ideal rule utilitarianism, however, is widely held to reduce to act
utilitarianism, since if everyone were to actually follow the rule of maximizing utility, that would be optimal
with respect to maximizing utility. But act utilitarianism has already been put aside.??backref

Non-ideal rule utilitarianisms, on the other hand, inject a note of realism into the optimization procedures. For instance,
what might render a set of rules correct is that if everyone were to \textit{try} to follow them, optimal results would
result. This already raises a Mersenne question. For trying is something that comes in degrees, and it is very likely that
different rules will be generated when we optimize for the utility resulting from everyone's trying hard to follow them
than if if we optimize for the utility resulting from everyone's trying with minimal effort. And there will be a vast
number of intermediate cases, so there will be a Mersenne question of what grounds the fact that $\alpha$, say, is
the right degree of effort for defining the optimization procedure that generates the moral rules.

Furthermore, specifying the degree to which the hypothetical agents try to follow the moral rules is not enough to specify the
optimization procedure. For instance, one has to specify the level of intelligence of the hypothetical agents, their non-moral 
interests and the non-human environment, which yields multiple Mersenne questions as to what the requisite levels of these for the 
hypothetical optimization procedure are. 

Perhaps we can try to avoid such questions by requiring the counterfactual world to match our
world in respects of the level of intelligence of the agents, their non-moral interests and the
non-human environment, but this runs into two problems.
First, when we try to imagine a world where all agents try to follow the moral rules, the world we imagine
will be one whose agents have different non-moral
interests and higher levels of intelligence than we do, since such a world would have a much more just educational system
than ours and hence would nurture children into greater intelligence, and a rather different non-human environment. If 
we keep the three factors fixed while having the hypothetical agents try to follow the moral rules, we are likely to get
some very unlikely counterfactual results, just as keeping too much of our world fixed in a counterfactual situation
results in the odd claim that if Oswald did not kill Kennedy, Kennedy would have been buried alive. 
Second, we have to say that if our history had gone slightly differently, so that (say) the distribution of intelligence in the
general population were slightly different, the optimization procedure would have generated different rules, and hence different
moral rules would have been true. Indeed, on this view we would get the very strange idea that what we morally do can affect morality
itself.

Besides this, there are other non-ideal aspects that we should probably introduce. Some of our important moral rules discuss how
we should deal with culpable malefactors. But in a world where everyone tries to do the right thing, depending on the strength
of trying, there might well be \textit{no} culpable malefactors, or at least very few. And it is unlikely that moral rules optimized
for such a very different situation would be likely to be the right ones for us. So we probably need to optimize the rules with
respect to a hypothetical situation where not everyone tries to follow them. And that raises Mersenne questions as to 
the grounding of the specification of how many
people in the hypothetical case follow these rules and how the non-followers behave.

In short, ideal rule utilitarianism collapses to act utilitarianism, while developing the non-ideal rule utilitarian project raises many Mersenne questions as to the details of the counterfactual situation.

\subsection{Social contract}
Social contract theories ground ethical rules in agreement between agents.  We can divide this based on whether the agreement 
is actual or hypothetical. Actual agreement theories face obvious problems. First, it is highly implausible to think of the members of society as having \textit{actually} agreed to live by moral rules, apart from special cases such as a religious person vowing to God to sin no more. Second, actual agents can agree to live by unjust rules, even rules unjust to themselves,
and such rules would not constitute morality. 

Contemporary social contract theories tend instead to be based on duties grounded in hypothetical agreement between agents in situations of 
ignorance.??refs Anyone who has been in a long committee meeting knows that actual agreement between agents can result in 
complex rules with much apparent arbitrariness, and it would be unsurprising if hypothetical agreements were similar. Thus far, 
social contract fits our Mersenne-style data well. 

But the hypothetical agreement condition involves multiple parameters such as how rational and smart the hypothetical agreers are (and there are
multiple dimensions of intelligence), what exactly are they ignorant of, how many of them are there, what are their attitudes towards
risk and uncertainty, etc. 
We have here an explanation of the Mersenne parameters in terms of other Mersenne parameters, and the
problem remains fully entrenched. 

The risk and uncertainty point is worth emphasizing. Some hypothetical agreement theorists think that 
rational agents would only agree to rules that do not treat anyone inhumanly.??refs But a rational agent who is more accepting of
risk and does not have prior moral constraints will be willing to tolerate rules that create a minority group that is treated inhumanly if the risk of being a member of 
that group is sufficiently small---i.e., if the group is a small enough fraction of the general population---and the benefits 
to the majority are sufficiently large. There will be types of inhuman treatment and levels of risk that it would not be 
rational to accept for the sake of a high probability of a large benefit, but the lines between these and the ones that it would 
be rational to accept do not seem derivable from any plausible set of basic principles of rationality. 

Granted, typical Kantian constructivists will insist that certain kinds of inhuman treatment would never be rationally acceptable. But 
now consider the Mersenne questions about these kinds of treatment. For instance, destroying the autonomy of another person might
be taken never to be rationally acceptable. But a minor limitation on another's autonomy clearly is acceptable for a sufficiently
great good: if the only way to save a country from nuclear destruction by evil enemy would be to acquiesce in the enemy's demand 
that everyone wear jeans on Friday, then this limitation on sartorial autonomy should be enforced. Somewhere there is a line between
minor limitations of autonomy and such deep destruction of autonomy that could not be tolerated no matter the price. The only
``natural'' place to draw this line would be at \textit{complete} destruction of autonomy. But if it is only complete destruction
of autonomy that is prohibited by the Kantian, then this does not place a sufficient constraint on the rules that could
be accepted. For instance, the enslavement of persons would not be prohibited, as long as the enslaved persons were still capable of some minimal autonomous agency, say, a choice of color of clothing.

Furthermore, even prohibiting treatment that completely annuls someone's autonomy will
not avoid Mersenne questions in the vicinity. For we will have probabilistic questions. Is it permissible to perform an action that
has a 99.9\% chance of destroying someone's autonomy, a 99\% chance, a 90\% chance, a 51\% chance, a 10\% chance, etc.? The only natural
lines to draw seem to be at 100\%, 50\%,\footnote{There are actually two options here: more than 50\% and at least
50\%.} or 0\% but none of these will do. A prohibition on an action that has a 100\% chance of destroying
autonomy prohibits nothing: any action we perform can fail. A prohibition on an action that has more than a 0\% chance of destroying autonomy
prohibits everything: I scratch my head, and there is a tiny chance that due to some weird sequence of events this causes an earthquake
that leads to you getting hit on the head by a beam that results in your life being reduced to a vegetative level. And while 50\% is 
much more reasonable, in some difficult cases it is excessively restrictive. If a child is certain to die within a day, and is suffering
from horrific pain that can only be relieved by a drug that has a 50\% chance of reducing the child to a vegetative state for the rest
of the day, administering the drug can be permissible.

\subsection{Virtue ethics}
Aquinas himself invoked the virtuous agent as providing at least the epistemic path to an answer to the preferential treatment
question. We could also take virtue ethics to provide an answer to the Mersenne question: What makes these parameters, rather
than others, hold is that the virtuous agent's patterns of behavior are thus and so parameterized.

But this of course simply shifts the Mersenne problem to that of why the virtuous agent's patterns of behavior are parameterized as
they are. 
Virtue ethics helps with the epistemic aspects of the Mersenne question, but the metaphysical aspects remain.
The answer to these most consonant with the virtue ethics tradition appears to be the one given in the Aristotelian tradition which grounds the parameters in
the agent's nature. Thus, the parameters can be grounded directly or via the intermediary of virtues.

\subsection{Divine command}
On divine command ethics, the right is what is commanded by God.
Divine command ethics, like social contract and rule utilitarianism, carries with it significant hope for explaining the apparent
arbitrariness in ethical parameters. We would not be surprised if laws governing complex embodied beings
like us and coming from an infinitely intelligent and good legislator 
had significant complexity that to us would look like arbitrariness. 

It may initially seem the divine command ethics runs into the same problem of pushing the Mersenne questions back to the
question of why God legislated these parameters and not others. But notice that the Mersenne problems I have been discussing
are \textit{grounding} questions. Even if God's legislation were completely arbitrary in a way that ultimately violated the
Principle of Sufficient Reason, on divine command ethics we would have a \textit{ground} for the parameters in preferential
treatment and other ethical rules being what they are. To say that we should prefer siblings over first cousins in a ratio
of $17:10$ because God commanded it is to give a ground for the obligation, even if that ground itself needs an explanation.
Compare the moral prohibition on adding cyanide to friends' drinks. There would be something absurd if that prohibition were
ungrounded. But it has a ground, or at least a partial ground: cyanide is fatal to humans. Imagine now that, contrary
to fact, it was an unexplained brute fact that cyanide is fatal to humans. Nonetheless, the grounding problem for the moral prohibition would
have been solved by citing the danger of cyanide, even though we would be relying on something unexplained. 

In this way, our ethical grounding Mersenne problem is quite different from Mersenne's merely explanatory problem. In Mersenne's case
to explain why the distance between the earth and the moon is what it is in terms of other parameters of earlier states of the
solar system does not make significant progress, since the earlier states need an explanation. But when we have given a plausible \textit{ground} to the moral obligation, we have
indeed made progress. Mersenne's original argument depends for its plausibility on a fairly general Principle of Sufficient
Reason. Here we just use a heuristic principle that moral truths with
an appearance of arbitrariness need a deeper ground.

Moreover, the divine command theorist has nice answers available to the question of why God chose these rules. For instance,
God could be an act consequentialist and could have optimized the rules to produce the best consequences, including perhaps
such consequences as the value of following and disvalue of breaking moral rules in addition to first order values and 
disvalues like pleasure and pain.  We would expect a complex optimization for a precise end to produce results with an appearance of arbitrariness.
A sailboat hull computer-optimized to minimize drag is likely to have many parameters that look arbitrary to those who do not
know how it was generated.

At the same time, we still have some serious Mersenne grounding problems given divine command theory. The plausibility of divine command ethics rests in
the idea that God is a legitimate authority and legitimate authorities need to be obeyed. This suggests that logically prior
to divine command ethics there is some sort of a proto-ethical general rule about obedience to legitimate authority. That 
rule itself will have to have parameters specifying which authorities are legitimate and what the scope of their authority is. 
And we will have the Mersenne problem of grounding these parameters.

Moreover, even if we do not have such a general rule about all authority, but a specific rule about divine authority, this
will still raise some Mersenne problems. For, as Aquinas noted??ref, legislation only has a claim on our obedience when it 
is appropriately promulgated. And promulgation is a complex concept involving thresholds and parameters. It is not necessary
for promulgation that all those subject to the legislation have heard of it. But it is not enough for the legislators to
meet secretly, and write the legislation on a stone  buried on public land. Intuitively, we need the legislation to be 
reasonably accessible to those governed by it, but there are many parameters hidden behind the word ``reasonably'', and 
we need grounds for them all.

The divine command theorist needs a promulgation condition that intuitively has normative force, and to escape the
Mersenne problems needs to have no apparently arbitrary parameters. But, further, to have moral realism, it needs to
be a promulgation condition that it is plausible to think has actually been met.

A strict promulgation condition like that everyone governed knows of the rules has not plausibly been met, , since
there is no agreement on what God's commands are---or even on there being a God.
What is worse, when we focus on the Mersenne cases in ethics that were enumerated earlier in this chapter, it unclear that divine commands instituting the parameters
would even satisfy a fairly modest promulgation that requires those who try really hard to be able to find what the legislation 
is when it is relevant to life. There surely are cases where we have tried really hard to figure out what is the right thing
to do and we didn't succeed. Perhaps it could be argued that we didn't try ``hard enough'', but now we are the true
Scotsman territory. And of course there would be a Mersenne question about the parameters defining the ``hard enough''.

Divine command theory, like other major theories, fails to offer a plausible solution to the Mersenne problems.

\section{Other attempts at escape}
\subsection{Particularism}
By pointing out the complexity and context-sensitivity of moral problems, the Mersenne questions may seem to be
evidence for particularism. On particularism, while there may be general \textit{ceteris paribus} rules
like ``Other things being equal, don't torture people'', the application of these general rules to specific situations is not
rule-governed. Hence, there won't be a rule specifying when one, say, favors a sibling over a cousin. Instead, there are 
particular facts about what to do in particular situations. 

However, particularism only multiplies the Mersenne questions. For whereas on rule-based systems we had Mersenne questions
about why the parameters in the rules had the values they do, now we will have Mersenne questions about why in particular actual
circumstances $C_1$ we should act one way while in slightly different particular actual circumstances $C_2$ we should act a
different way.

Furthermore, plausibly, there will still abstractly speaking be a function that assigns to each maximally
specific set of nomically possible circumstances a hypothetical determination
of how one would be obligated to act in that circumstance.
There may, of course, be no formula specifying the function, but that does not
affect the Mersenne question of why this function rather than another, perhaps similar one, is the correct one.

\subsection{Brute necessity}
Perhaps we could say that it is a brute, unexplained but necessary truth that the answers to the ethical Mersenne questions are as they are.
The boundaries lie where they do, but there is no special ontology behind them: it's just a necessary truth that we should prefer parents to cousins,
that an armed up-rising up against a regime responsible for Nazi-style atrocities is permissible while only non-violent protest against the
faults of modern-day Canada is permitted, and so on. 

Brute necessities should never be a first resort in theorizing, but sometimes they might be acceptable as a final resort. Consider
Mersenne-type questions one could ask about set theory. If the Zermelo-Fraenkel with Choice (ZFC) axioms for set theory are consistent, then for every natural number
$n$ they are compatible with the hypothesis $CH_n$ that there are exactly $n$ cardinalities strictly between the cardinality of the natural numbers and
the cardinality of the real numbers (the hypothesis $CH_0$ is the famous Continuum Hypothesis).??ref:check Suppose it turns out that in fact $CH_{15}$ is true.
We would have an excellent Mersenne question as to why it is $CH_{15}$ that is true rather than say $CH_0$ or $CH_8$, but the mind boggles as to what could be a satisfactory answer to that
question, much as it does in the ethical questions. Perhaps the truth of $CH_{15}$ could be a brute fact, albeit a necessary one since it is generally held that mathematical truths are necessary (though see ??Pruss for a
controversial Aristotelian metaphysical story on which some mathematical truths---perhaps even including things like the Continuum Hypothesis---are contingent). 

Some brute necessities can perhaps be admitted in ethics. For instance, if $CH_{15}$ is necessarily true, then it is necessarily impermissible 
for us to punish someone for falsely informing us that $CH_{15}$ is true. This impermissibility would derive from the impossibility of $CH_{15}$ being
false (and hence the impossibility of falsely informing someone of that it's true) and the impermissibility of punishing people for actions that they did not 
do. (It is possible, of course, to insincerely inform someone of a necessary truth. But that's a different wrong action, even if equally bad.) 

But truly ethical brute necessities are deeply implausible. Here is one way to see this. Suppose there is a sequence $s$ of one or more English sentences expressing your 
favorite set of fundamental and necessarily true ethical norms. For instance $s$ might be a single injunction like ``Love your neighbor as yourself'' or ``Maximize total pleasure minus 
pain of all sentients'', or it might be a longer list. Encode $s$ into a sequence of decimal numbers in some natural way, for instance by encoding each symbol in
$s$ into a three digit ASCII number from $032$ to $126$. It is widely believed---though it has not been proved---that $\pi$ is a ``normal number'', i.e., every possible sequence of
digits occurs in $\pi$. If so, then the decimal encoding of $s$ occurs somewhere inside $\pi$---and even if not, it may well still do so. Suppose that the decimal
encoding of $s$ occurs in $\pi$ as the $n$th through $(n+m)$th digits. Now consider this metaethical theory: 
\ditem{pi-metaethics}{To do the right thing is to follow the English injunctions in three decimal-digit ASCII encoding between the $n$th and $(n+m)$th digits of $\pi$.}
Call this $(\pi,n,m)$-metaethics. On the hypothesis that the fundamental ethical injunctions are necessary and can be expressed in English, there are $n$ and $m$ for which $(\pi,n,m)$-metaethics necessarily
yields all and only the correct ethical injunctions. But, nonetheless, no $(\pi,n,m)$-metaethics 
has any plausibility. For there is no plausible normative connection between an injunction occurring at some
location in $\pi$ and its being binding on us.

Admittedly, if we in fact found a sequence of English injunctions ``near'' the beginning of $\pi$ in ASCII, say from the tenth to the 300th digits\footnote{Alas, in fact, the three digits starting with the tenth digit of $\pi$ are 358, which is out of range for ASCII.}, we would have some reason 
to follow them. But the reason would be something like this: The best explanation for why these injunctions are found in $\pi$ is found in a powerful being or beings that
can somehow control mathematical truths or, more plausibly, the evolution of our Indo-European languages 
and ASCII code, and there is good pragmatic reason to follow the commands of such powerful beings. Perhaps they have
our good in mind, perhaps they will get mad if we don't follow their commands, or perhaps they are trying to inform us of the true ethics. But nonetheless
$(\pi,10,300)$-metaethics would be false. The reason these injunctions would have force wouldn't be that they are 
found in $\pi$, but something else, such as that a powerful being wants us to follow them.

In other words, a metaethics where the ethical claims are grounded in something intuitively of no relevant to our moral activity, such as the content of the
digits of $\pi$, is not plausible. To be a candidate for a grounds of ethical claims, a thing needs to be ethically compelling. For a more controversial
illustration of this point, consider that it has been argued that no collection of the traditional attributes of God is such 
as to make it plausible that the commands of a being with those attributes are what ethics is (??ref:MacIntyre??), and this is a strong reason to doubt divine
command metaethics, even if it turns out that God exists and has necessarily commanded all and only the fundamental moral
truths.  And while this claim about divine command metaethics is controversial, given that the traditional attributes of 
God include that God has created us and is perfectly good which might be held to ground obligations (cf.\ ??Evans's
gratitude-based account), if one were to replace God with a God$^*$ who has all the attributes other than goodness and lovingness\footnote{This is a \textit{per impossibile} conditional if the other attributes entail goodness or lovingness, as Swinburne, for instance, has argued in the case of goodness.??ref}, the point would be even more plausible. Divine$^*$ command metaethics would clearly be unsatisfactory
even if it turned out that necessarily God commanded all and only the fundamental moral truths.

But now take some theory $T$, like $(\pi,n,m)$-metaethics, that  grounds arbitrary-seeming ethical principles on 
a non-compelling but necessary ground, say the digits of $\pi$. But now modify the theory by keeping all the same
ethical principles, but removing all claims of grounding.  Removal
of the ground surely does not make the story any better. Someone who said that what explained why we should favor siblings over cousins by a margin of
twenty percent by saying that it is thus written starting with the $n$th digit of $\pi$ would be ethically ridiculous (though if $n$ is small, finding
the injunction might be some evidence for its correctness). But suppose we drop the spurious $\pi$-based ground: surely the ungrounded ethical claim is no better off than the spuriously grounded one. 

There may be fundamental ethical truths that are not themselves grounded. But these truths need to be compelling ethically---perhaps the Golden Rule is like that---and not 
have an appearance of arbitrariness. And there may be arbitrary-seeming truths in ethics, but they are not 
fundamental. The arbitrary-seeming truths need an ethically compelling ground.

A variant of a brute necessity approach is a Platonic approach on which ethics is grounded in the brute features
of the universal humanity. Since this universal can be thought of as just another theory about what a robust human
nature is, we can think of this for now as a variant on the Aristotelian story---a variant that will be more carefully
evaluated in ??ch10:forwardref.

\subsection{A two-step vagueness strategy}
It is very tempting to dismiss the Mersenne questions in ethics with a two-step strategy. \textit{Step one:} In each of the Mersenne-tyle
problems, give non-arbitrary grounds for 
an approximate and vague determination of the parameters involved. Thus, while it is implausible to think that, say, social contract theory
will generate a precise answer to the preferential treatment question, it is reasonable to think it will generate claims like: ``Benefits to 
siblings are to be \textit{somewhat} preferred to benefits to cousins.'' \textit{Step two:} Insist that the Mersenne question as to the grounds
of the exact dividing line has the false presupposition that there is an exact dividing line---instead, we have insuperable vagueness.

An initial concern with the two-step strategy is to worry whether other ethical theories can actually generate sufficient non-arbitrary grounds
that have the degree of precision that we think really is there. 
This concern has two variants. One involves cases where we know what is the right moral judgment.
Kantianism, for instance, is unlikely to generate even a vague morally-relevant 
distinction favoring siblings over cousins, and yet we know that we should favor siblings over cousins. The problem of ranking types of goods generates difficult 
Mersenne questions as to what grounds comparisons for utilitarianism, yet we know that fundamental philosophical truths are more valuable than the 
pleasures of chocolate. The second variant of the concern involves cases where we agonize over what to do. The agonizing is a sign of intuition
that there is an answer to a moral problem, albeit one we cannot discern. While we may not be seeking absolute precision, and may be willing
to accept some level of vagueness, in a number of cases we seek for more precision than the various alternatives to the form-based theory can ground. 

Suppose the initial concern can be allayed in both of its forms, perhaps by clever development of a theory that does generate
the known vague moral claims and by biting the bullet and admitting that moral agonizing is out of place in the other cases. 
There is still another question: how do we account for the vagueness at all. There are three main contemporary accounts of vagueness:
(i)~non-classical logic, (ii)~supervaluationism and (iii)~epistemicism. 

On non-classical logic approaches to vagueness, one typically increases the number of truth values beyond two. 
Consider an ethical Sorites
series, where we fix some circumstances $C$ and then say:
\begin{itemize}
\item[($A_0$)] Giving $ \$1000$ to a stranger is better than giving $ \$0$ to one's parent.
\end{itemize}

Now for each positive integer $n$, the following material conditional is very plausible:
\begin{itemize}
\item[($A_n$)] If giving $ \$1000$ to a stranger is better than giving $ \$(n-1)$ to one's parent, then giving  $ \$1000$ to a stranger is 
    better than giving $ \$n$ to one's parent.
\end{itemize}

From $A_0$ and $A_1$, one concludes by \textit{modus ponens} that giving $ \$1000$ to a stranger is better than giving $ \$1$ to one's parent.
From this and $A_2$, by \textit{modus ponens} one concludes that this is true even if what one gives one's parent is $ \$2$. Continuing onward,
once we get to $A_{1001}$, we conclude that it's better to give $ \$1000$ to a stranger than $ \$1001$ to one's parent, which is clearly false. Thus,
we need to reject one of the premises $A_n$. Presumably it's one with $n>0$, since $A_0$ is clearly true. But a material conditional
$p\rightarrow q$ is false just in case $p$ is true and $q$ is false. Thus if $A_n$ is false and $n>0$, we have:
\ditem{2-trans}{Giving $ \$1000$ to a stranger is better than giving $ \$(n-1)$ to one's parent and giving $ \$1000$ to a stranger is not better than giving
    $ \$n$ to one's parent.}
And that is exactly the kind of sharp transition that the vagueness theorist wishes to deny.

The non-classical approach to vagueness??refs will involve a logic with one or more additional truth values inserted
between \textit{false} and \textit{true}. On a three-valued option we insert a single such truth value, say 
\textit{indeterminate}, but it is probably more natural to insert a continuum of truth values and have a truth
value for every number between
$0$ (fully false) and $1$ (fully true).
 Then the statement:
\begin{itemize}
\item[($B_n$)] Giving $ \$1000$ to a stranger is better than giving $ \$n$ to one's parent 
\end{itemize}
is true for $n=0$ (note that $B_0$ is just $A_0$), but will become less true as $n$ increases (where in the
three-valued case, I take indeterminate to be less true than true, and false to be less true than indeterminate). 

But $B_n$ is fully true not just for $n=0$, but is also fully true for $n=1$ and $n=2$ and at least a few other small
values of $n$. On the other hand, not only is $B_n$ fully false for $n=1001$, but it's also fully false for
$n=1000$, for $n=999$, and at least for a few other values that are a bit less than $999$. On the other hand, somewhere 
for $2<n<999$ on our non-classical view we have $B_n$ taking on one or more intermediate truth values, or else the
view is useless. And hence whereas in the classical logic reading we had one transition to be explained, from true to false, now we have at least two: from full truth to the first $n$ where $B_n$ has an intermediate truth value, and
then from the last $n$ where $B_n$ has an intermediate truth value to where it has full falsity.
Thus we have doubled 
the number of Mersenne questions. 

We might try to say that where the transitions from full truth/falsity and intermediate truth values occur 
is itself an instance of second-order vagueness, and hence there is no point where it's fully true that we have
such a transition. But if we use our non-classical multivalent logic to account for this second-order vagueness
again, then we have a further multiplication of Mersenne questions. For where we started with with a case where
grounding the transition from false to true among the $B_n$ was our only Mersenne question, now we have a
such transitions as fully truly fully false to intermediately fully false, from fully truly intermediately
truly to intermediately truly intermediately truly, and so on. Going second-order doesn't just shift the 
Mersenne bump under the rug, but makes it grow.

Perhaps, though, one can stay on the first-order level, go for a continuum of truth values, and dig in one's heels 
by insisting on strict decrease of truth value for all $n$: thus $B_1$ is slightly truer than $B_0$, and so on. 
But the precise assignment of 
intermediate truth values---say, $B_{505}$ getting a truth value of $T_{0.51}$---also calls for an explanation. Thus it seems we have a vast multiplication
of Mersenne questions. But there is a response to this argument: some hold that exact truth values are a mere 
feature of the logical model of vagueness and all that has reality is their ordering.??ref And
the ordering of the truth values is, perhaps, quite non-arbitrary in that $B_{m}$ is truer than $B_n$ precisely when $m<n$. But the insistence that the ordinal
properties of truth values is what has reality still does not escape the multiplication of Mersenne questions. For consider a different set of ethical
questions involving a threshold. For instance, let $C_x$ say that one has a duty to obey the orders of a government that cares to degree $x$ about the common
good, for some method of quantifying care about the common good, where, say, $x=-100$ corresponds to the Nazi German state and $x=5$ corresponds to modern
Finland. Then $C_{-100}$ is very much false while $C_{5}$ is very much true. But even if all we insist on is the ordering of truth values, then we will still have a vast,
perhaps infinite, number of Mersenne  questions about comparing the truth values of the $C_x$ to the $B_n$ like: 
\ditem{2-Mers-BC}{What grounds the point $N$ at which $B_n$ becomes less true than $C_{0.24}$?}
For clearly $B_0$ is truer than $C_{0.24}$ while $B_{1001}$ is falser.

However, the most common response to vagueness these days is supervaluation. The terms of a sentence can have multiple precisifications,
with a precisifications corresponding to different classical truth values. ``Bob is bald'' may be
true if we precisify ``bald'' as having less than half a cubic centimeter of scalp hair and false if we precisify it as
having fewer than a meter of hair. Then we have vagueness. When, on the other hand, a sentence is true (respectively, false) under all precisifications,
we say it is super-true (super-false).

In the ethical examples, such as whether it is better to give $ \$1000$ to a stranger or $ \$500$ to a parent in circumstances $C$, presumably the 
supervaluationist escape from Mersenne questions will be that no matter how far we precisify $C$, the statement will be vague due to a vagueness in 
ethical terms such as ``better'' or ``right'' or ``wrong'' which have multiple precisifications yielding different truth values for the ethical
claim. For instance, it may be better$_{17}$ to give the double amount to the stranger but not better$_{40}$. Indeed, on a view like this, we will have precisely specified cases where for some precisifications of ``better'' it will be better to 
give to the parent and for others it will be better to give to the stranger.

Just as in the multivalued logic case, this multiplies Mersenne questions. For where previously it looked like we have a transition from its being true that it's better
to favor the stranger to its being not true, now we have two transitions: from its being super-true that it's better to favor the stranger (say, when the amount of
benefit to the stranger is extremely large) to its being vague whether it's better to favor the stranger to its being super-false that it's better to favor the stranger.
And supervaluating at the next level up---say, supervaluating ``super-true''---only multiplies the Mersenne questions, 
as we saw in the non-classical logic case.

And there are some additional problems for the supervaluationist response. A standard objection to supervaluationism in general is that it implies that it is
super-true that there is a sharp boundary of ``bald'': for, given any precisification ``bald$_i$'', there is a sharp boundary for it. In doing this, supervaluationism
explicitly forces the denial of its governing intuition that there are no sharp boundaries. 

Finally, the application of supervaluationism to ethics is itself deeply problematic. It is truism that we have reason to do what is better. Truisms had better be 
super-true. 
This implies two possibilities with regard to the truism.
Either for every precisification ``better$_i$'' we have reason to do what is better$_i$, and we don't need to 
precisify ``reason'', or else we need to precisify ``reason'' and ``better'' in lockstep
when we precisify the truism, so that for every $i$ it will be true that we have reason$_i$ to do what is better$_i$. Neither option is satisfactory.

If for every $i$ we have reason to do what is better$_i$, given the existence of infinitely many precisifications here, it seems that the choice whether to
favor the stranger and the parent is governed by infinitely many reasons on both sides. This infinite multiplication of reasons is implausible. Moreover, the supervaluationist responder to the Mersenne questions has to say there is 
no overall winner here---no on-balance reason all things considered---for if
there were, then we could raise our Mersenne question with regard to the overall winner, and we would be no further ahead. But saying that there is no on-balance reason
here denies the intuition that cases near the boundary are hard cases, that it is a difficult question to figure out whether to favor the parent or the stranger, since
as soon as one can see that one is in the vague region, one could just conclude that neither action is on balance required by one's reasons. 

But, on the other hand, if there are infinitely many ways to precisify ``reason'', none of them privileged, then this undercuts the very idea of our life being governed in
a non-arbitrary way by rationality. It seems entirely arbitrary whether we follow reasons$_{17}$ or reasons$_{40}$ in our lives. Many questions of rationality
turn into purely verbal questions as to how ``reason'' is to be precisified. And the same goes for related terms like ``morality'' and ``virtue''. This 
does not seem to do justice to the non-arbitrariness that is central to a realist conception of reason, morality and virtue, or to the agonizing that is appropraite in hard cases. The point here is similar to the
one raised in ??backref regarding $(\pi,m,n)$-metaethics: it would be arbitrary to require obedience to the commands that are found starting with the $m$th digit of $\pi$,
rather than the commands found in some other location. 

Furthermore, there are moral reasons that a virtuous agent would rather die than transgress. We can, however, 
find Mersenne cases involving such cases---for instance, one should rather die than torture the innocent, but 
we saw in ??backref that what kind of infliction of pain is absolutely wrong raises Mersenne questions. But 
in the supervaluationist cases, it is a merely verbal question whether one has a certain ``reason''---it depends
on the precisification.??ref:Sider-on-purely-verbal But to ask that people die for what is ``purely verbal'' is
obscene, and to die oneself for such a ``reason'' is a failure to respect one's own humanity (to put it in Kantian
terms).


Finally, consider an epistemicist theory of vagueness according to which there is a true semantic theory that assigns to each term the precise meaning it has in the
light of the patterns of our use of that term, but neither that theory nor the empirical data on the patterns of language use are available
to us in sufficient detail to settle the meaning of vague terms.  Thus, there is a precise fact as to how much hair one can have
and yet have ``bald'' apply to us, a fact grounded in the patterns of our use of the word ``bald'', but it is a fact that is not accessible to us. Similarly, there is
a precise meaning of ``right'', ``better'' and similar ethical terms, a fact grounded in the patterns of our linguistic usage. If the transition between bald and non-bald occurs
between 98 and 99 hairs, there is nothing mysterious about the fact that someone with 98 hairs is bald and someone with 99 is not, just as there is nothing mysterious
about the fact that a backless chair is a stool. 

But the problem here is exactly the same as the last problem with supervaluationism. Ethical questions are turned into purely verbal questions.
Just as on supervaluationism, there is a multiplicity of concepts closely corresponding to our words ``right'', ``better'' and ``reason''.
On supervaluationism, none of these concepts was privileged, which turned ethical questions into purely verbal questions, undercutting our understanding of the way  our lives are to be governed by reasons and morals. On epistemicism, there \textit{are} privileged concepts that exactly correspond to the
words, but they are privileged purely linguistically---it just so happens that these privileged concepts better fit with our usage under the
correct semantic theory. We get an unacceptable arbitrariness on which if our linguistic practices were somewhat different, we would be using the 
word ``better'' differently, and there would be nothing less natural about that usage. If so, then our actions' being governed by the better 
or the right, rather than by some variant property, would be entirely arbitrary.

In summary, non-classical logic violates classical logic, which should only be a last resort, and further multiplies rather than resolving 
Mersenne questions. Supervaluationism likewise multiplies Mersenne questions. And, perhaps most seriously, both supervaluationism and 
epistemicism as applied to ethics turn ethical questions into purely verbal ones, undercutting a robust realism.

In this critique, I was following the Williamsonian understanding of epistemicism as grounding the vagueness in semantic
matters, without privileged moral properties.??ref There is, however, a more general understanding of epistemicism as
any theory on which there are genuine facts of the matter about vague transitions, but these facts are epistemically unavailable to us. Saying this, however, does nothing to solve the Mersenne problems, but has this much going for 
it: it is compatible with the basic metaphysics of the Aristotelian human-nature account that I am defending, and 
hence is compatible with there being a solution. 

The Aristotelian human-nature account can but does not have to be taken to be a species of this more general
epistemicism. The crucial question is whether all specific moral truths---say, that one should in circumstances
$C$ give $\$ n$ to one's parent rather than $\$ m$ to a stranger---are in fact knowable by us. One could have a 
more optimistic theory on which they are, but some require more time to figure out than is \textit{practically}
available to us. Or one could hold that there are some that are utterly beyond us. The second may seem absurd:
how could a consideration beyond our ken count as a reason? But if we understand morality in terms of teleology
as we will in ??forward, and if we grant that we could have aspects of our physical nature whose teleology is
not knowable by us (why not?), then we might be open to this less optimistic option---as long as such cases are
rare enough that most of the time one can make the right decisions. I incline towards the more optimistic view,
but will not predicate anything on either option.

\subsection{Anti-realism}
Retreating from realism in ethics to error theory does, of course, remove all the Mersenne problems in ethics. But the
cost is high: it is incorrect to say that genocide is wrong. Moreover, since some of the Mersenne problems involve not
just morality but also prudential reasoning, this requires one to deny the correctness of standard prudential reasoning.
But perhaps the most serious problem with the error theoretic solution is that we will have parallel Mersenne problems
in other normative areas, such as epistemology (??forward) and semantics (??forward), and the cost of error theory 
there is very high indeed: indeed one will no longer  be able to correctly say that one epistemically \textit{ought to} accept error theory.

A more moderate solution is to opt for a form of ethical relativism. Relativism, of course, suffers from serious and
standard objections.??refs Perhaps the most obvious is that it justifies an ultra-conservative approach: for if what
I (in the case of individual relativism) or my society (in the social variant) thinks is guaranteed to be true, then
I or society has no reason to take variant views into account, since if you disagree with me or my society, you're guaranteed to be 
wrong (from my or my society's point of view).

Moreover, relativism is itself prone to
Mersenne questions. First consider individual relativism, on which a moral claim is true just in case one believes
it. The Mersenne question here will be most obvious if one opts for a view on which belief reduces to having a
credence above some probabilistic threshold, say $0.95$. For then the relativist view comes down to the thesis
that a moral claim is true just in case one assigns it a credence of at least $0.95$. But that seems arbitrary.
Why should one be obligated to do what one has credence of $0.95$ in, but not obligated what one has credence of $0.93$ in?
So we have a threshold problem.

Many, however, resist the reduction of belief to a credential threshold. But if we do not so reduce belief, we should
then see belief as just one positive doxastic state among many, such as surmising, being inclined to think, believing,
being confident that, and being sure that. Moreover, a little reflection shows that
such classifications are too coarse grained to do justice to the richness of our mental life. So we have a Mersenne
question: Why are moral claims made true by my believing them rather than my surmising them or being sure of them?

And thinking that the problem here just involves degrees of confidence is probably neglecting much complexity in the human
mind. There is likely a continuum between fully believing and merely acting as if one believes. Why does moral truth show up
in the continuum where it does? Or think of the case when the psychotherapist diagnoses one with a subconscious belief.
Either such states define moral truths or they do not, and whichever option is right, we have a Mersenne question as to why it is. And consciousness
itself may come in degrees.

Furthermore, a narrow relativism restricted to those moral claims that we actually believe is very implausible. Suppose I believe that it is wrong
to eat animals, and I know that cows are animals, but I do not actually draw the conclusion that it is wrong to eat cows.
On such a narrow relativism, it would be wrong for me to eat animals but it would not be wrong for me to eat cows, even though
I know them to be animals. This is incredible. So we want to extend moral truth at least to things that clearly follow from my moral beliefs.
But probably we do not want to extend it to things that follow in ways that are far beyond our ability to know. For, first,
if do extend it thus far, then a Kantian might end up agreeing with a relativist, since the Kantian thinks that moral truths are
necessary truths, and might well hold that necessary truths follows follow from everything. And, second, such an
approach loses the 
internalist motivations of relativism. But if we restrict moral truth to things that follow \textit{sufficiently easily} from
our beliefs, we will have a Mersenne question of grounding where the line of sufficient easiness lie.

If our relativism is of the social sort, we will have analogues to the above Mersenne questions raised by belief and consequence at the social level.  And we 
will have more Mersenne questions. There is a complex and difficult literature on how to attribute doxastic states to a community.??refs A reasonable
reading of that literature is that there is a multiplicity of concepts that can be expressed with a phrase like ``The committee believes 
that $p$.'' For instance, belief by the vast majority of the committee members is enough on the more reductive concepts, 
while on more procedural versions of the concepts the committee's belief requires some sort of a joint procedure, such as a vote. 
There will be many answers here, corresponding to a broad spectrum of takes on what a community's beliefs is. And a social relativist
will then have a Mersenne question as to why moral truth is defined by the particular take in question.

The second set of Mersenne question arises from the question of identifying what counts as one's community. I am a citizen of two countries
and a permanent resident of a third. Are the moral beliefs of all of these communities---no doubt, mutually contradictory in various ways---true
for me, or only the moral beliefs of one apply to me? Do moral beliefs come to be true as applied to me because I am legally a member of the community, or because I identify
with it emotionally, or because I would like to identify with it emotionally? Or is it, perhaps, that every community's beliefs are true
for the community, but there is no such thing as being true for the members of the community? 
How large does a community have to be to define moral truths? Is a chess
club a commmunity that defines moral truths? What if the chess club goes down to one member? Are a pair of friends a community? It is clear
that there are many degrees of freedom in a social relativistic theory, and we would have a Mersenne question corresponding to each of them.

\section{Hume's objection: Complexity, instinct and nature}
Hume saw the complexity of property, inheritance, contract and jurisdiction, and used this complexity to 
argue \textit{against} a Natural Law account: 
\begin{quote}
For when a definition of \textit{property} is required, that relation is found to resolve itself into any
possession acquired by occupation, by industry, by prescription, by inheritance, by contract, \&c. Can
we think that nature, by an original instinct, instructs us in all these methods of acquisition?
(??Enquiry::Morals)
\end{quote}

To further expand on the complexity, Hume notes the vast variety between these rules in different societies,
and analogizes them to the variety of architectures found in housing across societies. A better view,
Hume insists, is that we simply engineer rules for the sake of social utility, just as we engineer houses
for various ends, and in different environments this results in different solutions, albeit ones with a
lot of commonality.\footnote{It is natural to try to solve the problem by adverting to positive law. But
Hume notes that there is much complexity with respect to the question of which institutions get to make
the positive laws.}

Three responses are possible. 

First, we do actually have good empirical reason to think that our moral 
intuitions and instincts vary quite a bit from case to case, and hence can take into account the moral
complexity that Hume points to. In some cases (etiquette is the most uncontroversial), different rules are 
appropriate in different societies, but the imperfectly intuitively grasped function from society to rules is 
complex. An account of our actual moral instincts 
is likely to have enormous complexity. 

Second, the account I am defending is one on which our forms set norms. These norms can be arbitrarily complex.
Granted, there will be a harmony between these forms and our instincts and intuitions. But this harmony is not
a one-to-one mapping. There is such a thing as normal and abnormal food for a type of organism, and we expect the organism
to have instincts that tend to direct them to normal consumption and away from abnormal consumpton. But just as correctly
functioning sight can still err, so too a correctly functioning feeding instinct can lead organisms to ingest
what is abnormal and to refrain from what is normal, especially in environments that have abundances different from the
ones present where the organisms evolved. Thus, our natural instinctive
preference for high-calorie foods leads humans in affluent Western countries to unhealthy consumption.
Reflection can yield additional information as to \textit{normative} consumption on the basis of
high rates of obesity.??refs By reflecting on our nutritive instincts and \textit{other data}---such as the teleology of
nutrition and medical facts about us---we can get additional information as to what is normative consumption for
an organism, including a human one. This additional information is still fallible, and may fall short of the complexity
of the norms involved. And what goes for our nutritive instincts is even more strongly applicable to our moral ones.

Third, recall Hume's own solution to the problem that the complexity of the rules is a result of our social engineering for social utility. 
Hume's solution is subject to complexity problems of its own. For
instance, what groups count as societies and how do we aggregate the benefits to individuals to get a social utility, etc.? 
But something similar to Hume's solution can be appropriated for the natural law account. We can suppose norms in our nature 
establishing the goals for certain social institutions, such as property or state authority, and perhaps establishing some 
constraints on how these goals are to be pursued, and at the same time requiring us to engineer institutions that satisfactorily
pursue these goals within the contraints. These norms will be complex, but will be less complex than the vast complexity in
our social institutions. Thus, we have a bootstrapping, from fairly complex norms setting the ends and constraints for social
institutions, to the institutions themselves.

\section{Concluding remarks}
We have a vast panoply of Mersenne problems about the grounding of the complex norms that we find ourselves with.
Positing a robust view of a human nature encoding and grounding these norms solves the problems as far as they
go, though it leads to further questions that will be examined in ??forwardrefs. The major alternatives all
have Mersenne questions of their own, and many have serious additional problems.
\chaptertail 
