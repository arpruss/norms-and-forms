\def\mychapter{II}
\input{chapterhead}
\chapter{Ethics}\label{ch:ethics}
\section{Normative ethics and boundaries}
\subsection{Motivating examples}
\subsubsection{The rule of preferential treatment}
Let us begin with a more detailed discussion of an example from Thomas Aquinas's discussion of the order of charity. Aquinas thinks,
along with common sense, that those who are closer to us have a greater moral call on us.
Thus, if it is a question of bestowing the same good on one of two people, where one is more closely
related to us, we should benefit the closer one. But Aquinas writes: ``The case may occur, however, that one 
ought rather to invite strangers [to eat with us], on 
account of their greater want.''??ref And then he raises the question of what one should do ``if of two, one be 
more closely connected, and the other in greater want.''??ref

We might hope that here Aquinas would give us some clever rule for weighing connection against need. But 
instead he writes very sensibly: ``it is not possible to decide, by any general rule, which of them we ought 
to help rather than the other, since there are various degrees of want as well as of connection''.??ref It is
tempting at this point to throw up one's hands and simply say that in these in-between cases there is no
fact of the matter as to what should be done, or both options are permissible, or else relativism applies
to the case. But that would not do justice to the way we agonize when we find ourselves in such a difficult 
situation, trying to discover the truth of the matter. (It is interesting to note that the most common real-life moral dilemmas
tend to be like these kinds of cases, rather than highly controversial questions about trolleys, strategic bombing or
bioethics much discussed by philosophers.)
And indeed Aquinas maintains a realist attitude to
the question while simply offering this advice for how to figure out the answer in a particular case: ``the matter 
requires the judgment of a prudent man.??https://www.newadvent.org/summa/3031.htm\#article2

We can think of this as the problem of specifying a function $f(r,a,s,b)$ of four variables, two of them, $r$ and $s$, being
degrees of relation and the other two, $a$ and $b$, being degrees of benefit, where the function takes one of three values
corresponding to whether it is obligatory, permissible but not obligatory or impermissible to bestow a benefit of degree $a$ on a person with
relation of degree $r$ to the agent in place of bestowing a benefit of degree $b$ on someone related to degree $s$. 

In fact, the problem of a rule of preferential treatment is much more complicated than the above indicates. First, the \textit{kinds} of benefit and relation also matter: ``we ought in preference 
to bestow on each one such benefits as pertain to the matter in which, speaking simply, he is most closely connected with us.''??ref
So the function will depend not merely on quantitative features but qualitative ones. Second, although Aquinas does not mention it here,
the evaluation will no doubt depend on various features of the circumstances. And, third, in practice instead of choosing between
two certain benefits, we are choosing between two probability distributions over the space of possible benefits.

Now, as Aquinas admits, we do not know what the moral evaluation function for choices between benefits to different people is.
But abstractly speaking there is some such function, even if we do not know what it is, just as there is a function that assigns to each person
alive now the number of hairs they now have, even though we cannot specify any of the values of the function.
And we have good reason to expect the moral evaluation function to be very complicated. Indeed, probably the only serious proposal for a
relatively simple function $f$ here is the utilitarian suggestion that $f(r,a,s,b)$ yields obligation when $a>b$,
mere permission when $a=b$ and prohibition when $a<b$. But this utilitarian suggestion betrays the intuition that
the degrees of relation $r$ and $s$, much less the kinds of benefit and relation, are relevant to the moral evaluation.???refs

Indeed, the function is apt to look arbitrary. Fix the degrees of relationship to be one's parent and a total stranger,
and fix a specific and certain financial benefit of \$100 to one's parent, and fix the circumstances. Then as we vary the 
financial benefit to the stranger from zero to infinity, we will presumably initially have a requirement of benefiting the parent
(it would be wrong to give \$1 to a stranger instead of \$100 to a parent in ordinary circumstances), 
then a permission either way, and then a requirement to benefit the second party. There will be boundaries between these regions
of logical space, and these boundaries will look as arbitrary and contingent as the boundaries between different tax brackets.
Like the tax brackets, some proposals for boundaries will be \textit{clearly} unreasonable, but there will be many proposals
that appear reasonable. And whatever the actual boundaries will look arbitrary.

Of course, seemingly arbitrary numbers can come out of an elegant and simple rule: it seems arbitrary that the fifth and sixth 
digits of $\pi$ are $5$ and $9$ respectively, but there is an elegant mathematical explanation. But apart from the
utilitarian proposal, we do not have any at all plausible simple proposal for $f$.

These seemingly arbitrary boundaries in the order of charity raise call out for an explanation at least as much as 
the exact distance between the earth and the moon does. Just as it seems implausible that the distance between the earth
and the moon \textit{must} be exactly what it is, it seems implausible to think that the boundaries must be exactly where
they are---unless the utilitarian is right about $f$ being very simple. 

In fact, the ethics case calls out for an explanation even more than Mersenne's scientific examples did. For we might 
be able to swallow the earth-moon distance being a contingent and brute unexplained fact. But a brute fact seems unfitting for a moral
rule. A claim that it just so happened, with no explanation at all, that you should $\phi$ undercuts the moral force
of the alleged moral obligation. We expect anything seemingly arbitrary in our moral norms to have an explanatory ground.

To further argue for this point, consider a version of Divine Command Theory on which obligations are divine commands, and
God rolled indeterministic
dice to decide which actions to command, and by chance God's commands coincided with our common-sense morality, though they
could just as well as well have commanded cruelty and dishonesty. A Divine Command Theory on which it is mere chance
that cruelty is forbidden rather than commanded provides an unacceptable answer to the Euthyphro problem.??
Intuitively, a set of injunctions that is as arbitrary as that cannot constitute morality. But this point generalizes beyond
divine command theory. Suppose that that we have some preferential treatment rules that are brute and contingent, and could
just as well have enjoined on us the anti-utilitarian rule that we should always prefer the lesser benefit. Then whatever
these rules are, they do not constitute morality, but at best happen to agree with morality in content. 

Thus, even if there is some bruteness in the rules of preferential treatment, the rules in our world must be generated in a way
that makes rules such as the anti-utilitarian rules not be among the possible outcomes. But this makes it very unlikely that
the rules would be brute. For what force would limit the brute rules to avoid unacceptable options? Such a view of limited
bruteness would be akin to a view on which banana peels can come into existence \textit{ex nihilo}, but not where we might trip
over them.

It is important to remember that the Mersenne question here is a metaphysical question: What grounds or explains why this
rule, rather than some competitor, holds? The epistemic question may well have a virtue-theoretic answer like Aquinas's: if
we acquire the requisite virtues, we will be able to judge particular cases fairly reliably, and until then our best bet is
to ask the advice of virtuous others.

But before I continue the discussion of the possible explanation for the above ethical Mersenne question, let me follow
Mersenne's lead and multiply the examples, in order to defend against potential answers that only work in some cases, and
to make clear how widespread the problem is.

\subsubsection{Risk and uncertainty}
Some people---perhaps you---would accept a 92\% chance of winning a thousand dollars at the cost of an 8\% chance
of losing ten thousand. I wouldn't. I say that both I and they are reasonable. On the other hand, someone who 
(in ordinary circumstances) rejects a 99.9999\% chance of winning a thousand dollars at the cost of a 0.0001\% chance
of losing ten thousand and someone someone who accepts a 10\% chance of winning a thousand dollars at the cost of a 90\% chance
of losing ten thousand are unreasonable. 
It is well known that attitudes to risk vary between people, and while there are unreasonable attitudes, it is very plausible
that there is a broad range of reasonable attitudes.??refs
So, as we vary the probabilities of wins and losses, we move between cases
where accepting the risk is unreasonable, to cases where both accepting and rejecting are reasonable, to cases where
rejecting is unreasonable.

This, once again, raises the Mersenne problem of why the transitions between the various evaluative categories lie where they
do.  And of course things are more complicated than described above. The rational evaluation function will depend not just
on the probabilities involves but also on the values of the potential gains and losses. 

While in the previous case, utilitarianism provided a neat but implausible solution, so too in this case, expected utility
maximization provides a neat but implausible solution. On expected utility maximization, you are rationally required to
accept a chance $p$ of a good of degree $\alpha$ despite a chance $q$ of a bad of degree $\beta$ against a status quo of
value zero just in case the
expected utility $p\alpha + q\beta$ is strictly positive; when it is zero, you are permitted but not required; 
and when it is negative, you are not permitted. One problem with this solution is it requires all goods to be neatly
quantifiable (cf.\ the next example for difficulties related to that). But the more serious problem is that it requires an
implausibly negatively judgmental attitude towards ordinary people's attitudes to risk.

Indeed, here is a plausible trio of theses about risk that are incompatible with expected utility maximization:
\ditem{2-nobound}{There is no upper bound on possible finite utilities.}
\ditem{2-finite}{A decade of the worst tortures the KGB could think of has a finite negative utility.}
\ditem{2-notworthit}{There is no possible good $G$ of finite utility such that one would be rationally required in
accepting a certainty of a decade of the worst tortures the KGB could thing of one for a one in billion chance of $G$.}
For as long as $(1/1000000000)\alpha + \beta>0$, where $\alpha$ is the value of $G$ and $\beta$ is the (highly negative)
value of the tortures, one would rationally required to accept the deal on expected utility maximization, and by \dref{2-nobound}
and \dref{2-finite} there exists a possible $G$ that makes $(1/1000000000)\alpha + \beta$ strictly positive.
Hence, we should reject expected utility maximization, and and absent expected utility maximization, it is likely that the rationality evaluation function for risk will be messy
and arbitrary-looking.

The most plausible thing for the apologist for expected utility maximization to reject is the no-upper-bound thesis \dref{2-nobound}.
Here is one way an argument for such a rejection might go. First, there is a maximum intensity of goods that our brain can handle.
Second, goods become significantly less valuable as they are repeated, decreasing in such a way that the sum of the values of any 
goods you could have over an arbitrarily long life has an upper bound.??refs

But the repetition thesis is only plausible when boredom and other memory-based phenomena are in play. Suppose you have 
lived for a very long time. Then you suffer from partial amnesia: you have
lost all episodic memory of your past meals and of your past pinpricks. You are offered what you are reliably informed is 
the most delicious and wholesome dessert every prepared by the best chef on earth, a dessert  which you are told you've eaten some large 
number $n$ times in the past, and you may eat the dessert at the cost of a one in ten chance of a small pinprick. It's clearly worth it,
regardless of what $n$ is. So now suppose this happens to you every day of a very long life. The marginal value of each such 
dessert (i.e., the amount it contributes to total lifelong utility), absent memories of past desserts, must  be at least one 
tenth of the marginal disvalue of the pinprick, at least given expected  utility maximization. But the disvalue of the pinpricks 
clearly does not tend to zero with forgotten repetition. Hence, the value of the desserts does not tend to zero. And hence for any
finite utility bound, enough such desserts will exceed the bound.

In addition to Mersenne questions about risk and prudential rationality, there will be Mersenne questions about risk and morality.
For instance, what risks we may morally impose on others in exchange for a good to ourselves depends in a complex way 
on one's relationship to these others, the probability of the risk, the degree to which these others accept the risk, the 
benefit to self, and so on. When I drive, I risk killing other drivers, their passengers, pedestrians by the side road, and so on.
But the probability of these awful outcomes is very small, and typically other people on or by the road have accepted reasonable
risks (or have had them accepted by proxies, in the case of children), so these dire but unlikely outcomes typically do not render it impermissible for
me to go to the grocery store to pick up ice cream.\footnote{I leave open the question whether concerns about global warming 
render it impermissible.} But when the risk is higher, say because I am tired and sleepy after a long day and hence less likely to be
a safe driver, the matter becomes less clear. At some point, as the risk increases, it becomes impermissible to go to the grocery
store for ice cream. 
A particularly thorny set of issues arises in the special case of balancing the risk that the innocent are punished with the risk that the guilty go free.
And we have the Mersenne question of why the switchovers happen where they do.


Expected utility utilitarians\footnote{As opposed to actual-outcome utilitarians who evaluate actions morally based on the
actual utilities that would result from an action.??refs} will have a nice answer to this problem. But utilitarianism, as already
noted??ref, has many highly counterintuitive implications. 

\subsubsection{Orderings between goods}
Under ordinary circumstances, it would not be reasonable to choose to be a mediocre mathematician rather than a superb musician. 
But suppose one's choice is whether to be a superb
musician or a superb mathematician? Here we are dealing with incommensurable goods and either choice is reasonable.

But now let's ask this general question: Is it is reasonable to choose to be a mathematician of quality $\alpha$ rather than
a musician of quality $\beta$? Again, we have a function that takes a number of variables, including $\alpha$ and $\beta$
and the circumstances, and tells us whether (a)~it reasonable to opt to become a mathematician but not reasonable to opt for
music, or (b)~both are reasonable, or (c)~opting for music is reasonable but opting for mathematics is not. And, just as before,
it is very plausible that the function is extremely complex.

The problem obviously generalizes to all the many kinds of pairings of incommensurable goods there are.  In each case, there 
will be some function of many variables encoding the correct rational evaluation of the situation/, and we will have the Mersenne
question of what grounds the fact that this function, rather than one of the infinitely many others, encodes the correct
rational evaluation.

We also have Mersenne questions here that involve qualitative rather than quantitative comparisons. Other things being equal,
social pleasures are better than solitary ones. This seems rather arbitrary. What makes it be so?
 
In the preferential treatment and moral risk examples, utilitarianism offered a nice solution. But the problem of incommensurable goods is
also going to be a problem for any plausible utilitarianism. Utilitarianism comes in two varieties, depending on whether
the good is pleasure or the good is satisfaction of desire. As Mill famously noted, it is essential to the plausibility
of utilitarianism that one be able to make a distinction between lower and higher pleasures, so as to get the common-sense
conclusion that it is better to be Socrates unsatisfied than to be a satisfied pig.

But once one makes the distinction between lower and higher pleasures, or lower and higher desires, incommensurability
quickly shows up, since different kinds of pleasures and desires do not simply come in a linear ranking. Let's suppose that you get more 
enjoyment and satisfaction of the desire for truth out of mathematics and more enjoyment and satisfaction of the desire for music out of music, and let us suppose (contrary to typical situations)
that your choice of life will not affect anyone else. Then it seems right to say that the mathematical and musical lives are
incommensurable even on utilitarianism. But even if they are not incommensurable, but equal or one is better than the other, 
we still have a Mersenne problem as to what level of quality of mathematical life exceeds, equals or falls below what level of 
quality of musical life. And in fact it will be more complex than that, in that the quality of a mathematical or musical life
is clearly multidimensional.

One might try to get out of this by hoping for some precise definition of the degree of pleasure or the strength of a desire.
Perhaps there is a neural correlate of the degrees of pleasure or the strengths of desire that can be quantified in a single
number. But such an approach is likely to lead to the swinish utilitarianism that Mill wisely rejects. For presumably the
neural correlate can be manipulated directly, and the pig could be given pleasures which, in terms of neural intensity,
exceed the highest of Socrates' refined joys, and could be made to have a degree of intensity of desire for its swill far
exceeding Socrates' desire for virtue.  

Moreover, any neural approach is likely to fall prey to questions of cross-species comparison. While pig and human brains are
similar, they are not the same, and states of pleasure and desire are likely to be merely analogical. It is clear that some
comparisons between human and porcine goods are possible: a tiny human pleasure is worth less than a great porcine one. As one
increases the human pleasure and/or decreases the porcine one, there will come cases where neither of the two is to be
preferred, and then eventually cases where the human pleasure is to be preferred over the porcine one. But where exactly
the cross-over points are is not something we can just read off the neural correlates. And things get even messier when we
compare humans to possible beings that have no brains, such as intelligent robots (if these are possible) or aliens with very 
different biochemistry.

And even if one could give some such precise formulation, we would still have
the Mersenne problem of why \textit{this} formulation corresponds with true value rather than some other. 

\subsubsection{A miscellany of other Mersenne questions}
There are many other cases which involve thresholds or transitions that appear to be arbitrary.

On strict deontological views, one shouldn't torture one innocent person to save any number of lives. But of course
it would be permissible to gently prick someone with a pin to save even one life. Somewhere between the pinprick
and the torture is a transition. What makes the transition be where it is?

On threshold deontological views, it is wrong to torture one innocent to save a small number (say, one or two) of lives,
but it is permissible to do so to save a very large number (say, a billion). Again, we have a transition to be 
explained.\footnote{I am grateful to Philip Swenson for this example.}

The Principle of Double Effect allows one to foreseeably cause bad effects that it would
be impermissible to cause intentionally, as long as these bad effects are not intended either as ends or means. For instance, it seems permissible to bomb Hitler's headquarters even
if one finds out that an innocent prisoner is held captive there. But of course there needs to be a proportionality
condition imposed on this: the good achieved, say the end of a war, must be proportionate to the bad, say the death of the prisoner. 
It would be wrong to demolish an old building while knowing that there is a child playing inside: the good of having a lot to build
on is not proportionate to the death of the child. So there will be some function of variables including harms and benefits that
specifies when the benefit is proportional to the harm in Double Effect contexts. In fact, there will be other variables, such as
one's relationships to those harmed and those benefited. 

The laws of a legitimate government should generally be obeyed. But when a government becomes sufficiently unconcerned
about the wellbeing of the people, it becomes illegitimate. Why does this transition happen where it does?

Punishment should not be disproportionate to a crime. But in a legal system without a strict \textit{lex talionis}, the
proportionality is not going to follow any simple and elegant rule. Nonetheless, there are obvious restrictions. 
A month's imprisonment for an ordinary parking infraction is disproportionate in one direction; a ten dollar fine
for a murder is disproportionate in the other. What grounds the specific rule of proportionality?

Finally, standards of consent necessary to permit one's being treated a certain way vary widely depending on the treatment.
There are multiple dimensions in which we can measure the ``strength'' of a consent requirement: how well informed the 
consenting party needs to be, what age or level of intellectual development does the party need to have, what proxies if
any can offer consent on the party's behalf, how unpressured the consent needs to be, how clearly formulate the consent
needs to be, whether the consent must be specific to the case or whether prior blanket consent suffices, etc.
Under ordinary circumstances, no consent---at most, lack of refusal---is needed for a pat on the shoulder. The permissibility
of major surgery, however, has a consent requirement of significant ``strength'' along many of the above axes. On the other hand,
the permissibility of sex has a consent requiremnt of even greater ``strength'' along some of the above axes---thus, while
proxy consent and prior blanket consent can suffice for major surgery, they do not suffice for sex.\footnote{It is tempting
to explain this in terms of the fact that surgery---or at least the sort of surgery for which proxy consent suffices---benefits 
the patient regardless of the patient's consent, while sex is only beneficial when consented to. But this is arguably false.
Parents can validly consent to an organ transplant between their children, even if the donor is not expected to benefit
on balance (though generally there is a benefit from having one's sibling alive!).} The mapping between
the form of treatment and the multidimensional strength of consent is of great complexity, and has an appearance of significant
arbitrariness. What grounds it?

Some readers will disagree with a number of the examples. Double Effect, for instance, is quite controversial. But it seems
likely that a number of the remaining examples will still compellingly raise Mersenne problems. And the list above is not
exhaustive: the reader should be able to generate more items.

\subsection{Ethical theories}
We thus have many Mersenne questions pointing to arbitrary-seeming parameters in ethical rules.
I will now argue that a broad spectrum of ethical theories are either unlikely to yield good answers to the Mersenne questions
or else raise new Mersenne questions of their own.

\subsubsection{Kantianism}
Kantianism is an attempt to derive moral rules from the very concept of objective rationality. Famously, this leads to difficulties in
accounting for the substantive content of rules. For instance, from the point of view of objective rationality, it is difficult
to generate a presumption in favor of causing pleasure and against causing pain. The more tightly connected a moral rule is to the
specifics of the human condition and of the circumstances, the more difficult it will be for the Kantian to account for it. But the Mersenne questions above
thrive precisely on such detail. Consider, for instance, the improbability of a good Kantian account of how much we 
should, other things being equal, favor siblings over cousins, or of why proxy consent is sufficient for surgery but insufficient for
sex. The ``logical distance'' between the high level principles, like the categorical imperative to treat others as ends and never
as mere means or to act according to universalizable rules, and such specific moral content appears unlikely to be bridgeable.
Thus, precisely those cases that we have seen to raise compelling Mersenne problems make Kantianism an implausible ethical theory.

Of course, such appearances can be deceiving. One might well have antecedently thought that the relatively simple axioms of set 
theory are unlikely to generate the richness of mathematical theorems that we have seen to come from them. So it would be good
to go beyond an intuition of ``distance''.

There are at least four ways to do that. First, proceed by intuitions regarding a specific example. Consider two different moral rules regarding to the relative treatment
of siblings and cousins. One rule says that benefits to siblings are to be slightly preferred to benefits to first cousins and the
second says that first cousins and siblings are to be treated on par. Neither rule requires us to treat anyone as a mere means or 
takes away from treating people as ends. Both rules are universalizable. So we are not going to be able to derive one rule rather
than the other from Kantianism as originally formulated by Kant. 

Second, we can makes use of a heuristic as to the validity of arguments. One heuristic I employ in checking whether a numbered argument 
given by undergraduate students is valid, i.e., whether its conclusion logically follows from its premises, is to see if the conclusion of the
argument contains any substantive terms that do not appear in any of the premises. If it does, it is in practice unlikely that the 
argument is valid, though of course there are possible exceptions. If the premises are contradictory, then the logical
rule of explosion makes every conclusion a valid consequence. And it could also be that the conclusion is disjunctive and the
substantive term that did not occur in the premises occurs in one disjunct while another disjunct follows from the premises (though 
I have yet to see this happen in a student paper).  An argument from premises about the nature of rationality as such
with a conclusion about specific familial relationships or about specific human activities such as sex or surgery fails the heuristic,
and hence is unlikely to be valid. And the cases do not seem to be like the most common exceptions---the premises are not contradictory
and the conclusion is not disjunctive.

Third, all or most of the examples that raised Mersenne questions have an appearance of contingency to them, in a way that does not
fit with the hypothesis that they derive from necessary principles about the nature of rationality. One way to formulate this
contingency is to note that many of the rules are ones that we would not expect to apply to other intelligent species. If we
came across an alien species that regarded familial ties as somewhat more or somewhat less important than we think permissible
for humans, we should not judge them immoral. It would not surprise us if other intelligent animals---perhaps ones occupying
other niches---were rationally or morally required to take greater or smaller risks than we.\footnote{One thinks, for instance,
of the Klingons and Kelpians from the Star Trek universe, respectively.}

Finally, we have an epistemological argument. While clearly we do not know the exact values of the parameters in the Mersenne
questions, we have some approximate knowledge, as already indicated above in a number of the cases. We clearly did not come
to this approximate knowledge by logically deriving it from Kantian first principles. Nor did we even do so by means of an
intuition that they follow from these principles. For I take it that we do not in fact have an intuition that, say, the
preference for siblings over cousins follows from Kantian principles. If anything, we have an intuition that it does not.
So, it seems that if these rules in fact follow from Kantian principles, it's just a coincidence that our beliefs about
the parameters are correct, a coincidence that makes the beliefs be mere justified true belief rather than
knowledge. But the beliefs are knowledge. So, the Kantian explanation does not work. 

The epistemological argument has some force, but not that much. First, the argument is related to the highly controverted
literature on evolutionary debunking arguments.??refs,add?? Second, a theistic reader has an easy way out of the argument:
God knows what values of parameters in fact logically follow from Kantian principles and could either directly instil in
us correct beliefs about them or ensure that we evolve in a way that yields such true beliefs.

\subsubsection{Social contract}
\subsubsection{Act utilitarianism}
\subsubsection{Rule utiltiarianism}
\subsubsection{Virtue ethics}
\subsubsection{Divine command}
\subsubsection{Relativism}
\section{Natural law}
...and Kantianism?...

\subsection{Vagueness}
\subsection{Necessity}
\subsection{No rules}
...function...
\subsection{Bruteness}

\section{Metaethics}
\section{Flourishing}
\section{Supererogation}
\section{The great chain of being}
\chaptertail 
