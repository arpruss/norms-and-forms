\def\mychapter{VIII}

\input{chapterhead}
\chapter{Laws of nature and causal powers}\label{ch:laws}
\section{Humean and pushy laws}
\subsection{Deterministic versions}
There are two main types of theories of laws of nature. On Humean or deflationary views, we have laws simply in virtue of non-causal 
regularities of the behavior of objects in nature, and causation is then grounded in the laws. On pushy or inflationary views, 
we have laws in virtue of metaphysical components of reality that affect or constrain the behavior of objects. 
In this section, I will argue that pushiness is the right view. 

The most prominent Humean account of laws is David Lewis's Best System Analysis (BSA).??ref The non-probabilistic version says that laws are the 
logical consequences of the ``best system''. The systems in question are collections of true sentences all of whose terms
are ``perfectly natural'', in the sense of cutting reality perfectly at the joints. Thus, a system can contain the statement ``All electrons have charge'', but cannot contain
the statement ``All kittens are cute'' because
neither the concept of a kitten nor of cuteness is perfectly natural. (A kitten is an immature cat, and even if 
felinity is perfectly natural, immatureness is not.) We then compare systems along two dimensions:
a briefer system is better and a system that conveys more information about reality is also better.

Brevity is straightforward once we fix a language with perfectly natural terms. Informativeness is more difficult 
to characterize, but there are obvious comparative judgments: that energy is conserved is more informative than that
I am sitting now, since the energy claim tells us (a little) about each of the vast numbers of particles in the universe 
over all of time, while the claim about my sitting tells us only about what is happening in a small region of spacetime. 

Modern Humeans then tend to define causation in terms of laws. For instance, the Lewisian approach is to define laws
in terms of counterfactuals, and counterfactuals in terms of similarity of worlds, with similarity of laws prioritized.??ref
The resulting account has pleasing ideological parsimony: both laws and causation reduce to the arrangement of matter.

We now observe some serious problems with BSA.

\subsection{Problems with explanation}\label{sec:law-explanation}
The most discussed problem with BSA is explanatory circularity. To introduce the problem, let us begin with an explanatory 
problem independent of BSA. The old Deductive Nomological (DN) model of explanation??refs in the philosophy of science had it
that scientific explanations are deductively valid arguments for the particular fact to be explained, at least one of whose premises 
is a law. For instance:
\ditem{DN1}{All massive objects attract gravitationally.}
\ditem{DN2}{The Sun is a massive object.}
\ditem{DN3}{So, the Sun attracts gravitationally.}

Of course, it was soon seen that having this form is not necessary for explanation---statistical explanations do not fit the
deductive schema.??ref It was also seen that having this form is not sufficient for explanation, as one needs somehow to account
for the explanatory direction between the particular facts stated in the premises and the particular fact in the conclusion.
A famous example here is how the laws of nature together with the length of a flagpole's shadow yield a deductive argument
for the position of the Sun in the sky, but obviously the position of the Sun in the sky is explanatorily prior to the length of
the shadow.??ref

But even the seemingly innocent example \dref{DN1}--\dref{DN3} has a crucial problem, which suggests that
explanations of the DN sort rarely if ever work. For consider that \dref{DN1} is a universal generalization, and a universal
generalization is true in part in virtue of its instances. That all massive objects attract gravitationally is partly grounded
by the Andromeda Galaxy attracting gravitationally \textit{and} partly by the Sun attracting gravitationally. But grounding is a 
form of explanation. Thus, the Sun's gravitational attraction is explanatorily prior to the universal generalization about massive
objects attracting gravitationally. And yet the universal generalization is, according to the DN model, prior to the Sun's gravitational
attraction. But loops in explanatory priority are implausible.

The most common kind of response to these kinds of arguments is that explanatory loops are permissible when the explanation in the
two directions is of a fundamentally different sort. For instance, in the above case, the Sun's gravitational attraction is 
\textit{nomically} or maybe \textit{causally} explained by \dref{DN1} and \dref{DN2}, while providing a partial \textit{grounding} 
explanation of \dref{DN1}.??refs Nonetheless, it is not clear that it matters for the anti-loop intuition that the two directions of an 
explanatory loop are of the same kind. It is the circularity of explanation that seems to be the problem, regardless
of the types of explanation.

Furthermore, suppose you light a match in the kitchen. Then the friction applied to the match explains why something is burning in 
your house. But notice that this explanation consists of a chain of two explanations. The friction applied to the match in your 
kitchen \textit{causes} or \textit{nomically explains} (in conjunction with some appropriate laws) the match to be on fire in your kitchen, and the match's being on fire in your kitchen \textit{grounds} the 
fact that something is burning in your house (particular cases ground existential generalizations). The causal/nomic and grounding explanations
combine into a generic explanation that is neither just a causal/nomic explanation nor just a grounding explanation, but is nonetheless an 
explanation. This shows that there is a generic concept of explanation which goes beyond particular types of explanation, like 
causal, nomic or grounding. And an explanatory loop with respect to generic explanation seems problematic.

Thus, even paradigmatic cases of DN explanations can be troubling even without Humean assumptions. However, there is a plausible explanation for the initial intuitive 
appeal of DN explanations like \dref{DN1}--\dref{DN3}. When we offer scientific explanations in terms of a generalization that is a 
law of nature, we are implicitly invoking the fact that the generalization is a \textit{law}, rather than
merely depending on the generalization being \textit{true}.

To support this point, suppose that we live in a world with only one massive object, Bob, but where nonetheless it is a law that all massive objects
bend spacetime in some specific way. One might think that this thought experiment will be rejected by the advocate of BSA on the grounds that no 
universal generalization that has only a single instance can be a law. But this is not clear. First, a single instance can be
extremely informative, and in our example we can suppose that the fact that all massive objects bend spacetime is
highly informative. Thus, many contemporary Humeans accept as a law the ``past hypothesis'' which says that the initial entropy of the
universe is low??ref, because the past hypothesis is so informative when combined with other laws, even though it is a hypothesis that applies
to only one time. Second, on standard Lewisian BSA, the laws are the logical consequences or theorems of the best system.??ref But a universal 
generalization that has only one instance can be a logical consequence of a law that applies to many other instances. For instance,
it follows from the law of gravitation that all massive natural satellites of planets on which a philosopher was executed by being
made to drink hemlock attract gravitationally, and hence this complex universal generalization is a law. But as far as we know, 
there is only one such natural satellite, earth's moon. Similarly, it could be that the fact that all massive objects bend spacetime in a specific way
is a special case of a more general fact about what kinds of objects bend spacetime and how they do so, and the more general fact could have
more than one instance. 

Now in this world with one massive object to say that Bob bends spacetime because he's massive and all massive objects bend spacetime 
is silly: when there is only one instance, we surely cannot explain the instance in terms of the universal.
But it is not at all silly to say that Bob bends spacetime because he's massive and \textit{by law} all massive objects bend
spacetime,  even if Bob is the only massive object. Of course, we do not live in a world where there is only one massive object, but plausibly the logical form of the explanation of a particular massive object bending spacetime is no different in 
our world than in that one.

Now the hypothesis that nomic explanations invoke a law \textit{as a law}, implicitly or explicitly, can help solve the explanatory 
circularity model facing DN explanations, by modifying \dref{DN1}--\dref{DN3} into:
\ditem{mDN1}{By law, all massive objects attract gravitationally.}
\ditem{mDN2}{The Sun is a massive object.}
\ditem{mDN3}{So, the Sun attracts gravitationally.\footnote{Whether this remains deductively valid depends on whether its being the case that
by law $p$ it follows that $p$ (i.e., whether ``by law'' is a modal operator satisfying Axiom T). While this seems a natural assumption, it 
has been disputed (van Inwagen thinks that a false universal generalization can still be a law, if the exceptions are 
miracles??ref). We need not settle this question. If the argument is not deductively valid, it is still a good ampliative argument
form, and it is clearly explanatory. And the Humean thinks laws are true.}}

But now note that the Humean cannot make use of this solution to the circularity problem. For, first, on a Humean analysis, just as 
the Sun's attracting gravitationally was a partial ground of \dref{DN1}, it is also be a partial ground of \dref{mDN1}, 
since it is in part because this universal generalization is true that it is found in the best 
system.\footnote{Here I am assuming that the law of gravitation is fundamental. It is not \textit{always} true on BSA that a universal generalization is a law in part because it's true. 
It might be that a universal generalization is a law because it is a logical consequence of the more fundamental laws that are axioms of the 
best system. But if that is the case for \dref{mDN1}, then just need to replace the example with a more fundamental one or work 
in a world where the universal gravitational attractiveness of massive objects is a fundamental law. For the 
fundamental laws, their inclusion in the system is explained in part by their truth, as the axioms of the best system must
be true.} 

And, second, even apart from the circularity problem, explanations of the modified DN sort are problematic for BSA.
As Salmon noted, while adding irrelevant facts to an argument's premises does nothing to damage the argument, irrelevance
damages an explanation.??ref But on BSA, the proposition that by law all massive objects attract gravitationally tells us
that the proposition that all massive objects attract gravitationally is a logical consequence of the system
of truths that best optimizes a balance of informativeness and brevity. But \textit{this} fact does not seem explanatory.
That there is a system that best balances informativeness and brevity and has as a logical consequence that all massive
objects attract does not seem to contribute to an explanation of why a particular object attracts gravitationally. Facts 
about brevity of expression are relevant to giving physics-based explanations of the movements of planets. 

For imagine a family of worlds where objects with mass attract gravitationally, but so do objects with a certain other 
fundamental property $Q$. 
Suppose further that there is nearly total overlap beween massive objects and objects with $Q$, so that both:
\ditem{m-attract}{All massive objects attract gravitationally}
\ditem{Q-attract}{All $Q$ objects attract gravitationally}
are highly informative, and equally brief. But now imagine two worlds, $w_M$ and $w_Q$. In $w_M$, the number of $Q$
objects is $N$ for some large $N$, and the number of massive objects is $N+1$, while in $w_Q$, the number of $Q$ objects
is $N+1$ while the number of massive objects is $N$. Thus, in $w_M$, it is \dref{m-attract} that is the more informative,
while in $w_Q$, it is \dref{Q-attract} that is the more informative. Suppose there are no other regularities that make it
into the best system, and suppose that we include \dref{m-attract} in the best system for $w_M$, but adding \dref{Q-attract}
would give so little extra information (since there is only one object that has $Q$ but no mass) that only \dref{m-attract} makes 
it in. On the other hand, for analogous reasons, the laws of $w_Q$ include \dref{Q-attract} but not \dref{m-attract}. 

The proposition \dref{mDN1} in $w_M$ is then grounded in part in the fact that there is one more massive object
than $Q$ object in $w_M$ (unlike in $w_Q$). But the fact that there is one more massive object than $Q$ object is 
not explanatorily relevant to the Sun (assuming the Sun to exist in $w_M$) being gravitationally attractive. 
Yet that fact is explanatorily prior to \dref{mDN1}, which in turn is explanatorily prior to the Sun's attractiveness.

In summary, to avoid circularity, we need to invoke laws \textit{as laws} in our explanations. But lawfulness on BSA consists
of facts about truth, informativeness and brevity. The facts about truth ensure that we haven't escaped circularity, and the
facts about informativeness and brevity are not explanatory. BSA is inadequate, thus, if laws are supposed to be explanatory.

\subsection{Coin tosses},
Now imagine a universe consisting of a single quantum ``coin toss'' performed a very large number of times. Observing 
a ``neatly patterned'' full sequence such as $HHH...H$, $TTT...T$, $HTHT...HT$ or $THTH...TH$ would make us confident 
that the coin tosses are deterministic (e.g., $HTHT...HT$
would make us confident that there is a law that each toss is followed by its opposite). But even though this would make us confident of determinism, any such sequence could also
occur without determinism. Not so on our Humean story. On our Humean story, any one of these global regularities \textit{logically 
guarantees} a deterministic law in a world consisting of a single coin tossed repeatedly, since the regularity would
have to be a law. 

This is highly counterintuitive. 
Furthermore, suppose that we in fact have a ``patternless'' sequence that to the Humean yields an indeterministic law that says 
that each coin toss is independent and fair. But while the statement that each coin toss is independent and fair should allow any 
finite sequence of coin tosses, it turns out that certain sequences, such as our four examples above, are logically incompatible with 
independence and fairness on our Humeanism.

\subsection{Magic}
Suppose you live in a multiverse consisting of two physical universes, each of which has laws of nature roughly like 
ours. Now, plausibly, any state of a physical universe like ours can be encoded as a countable sequence of real numbers (i.e., a finite
sequence, or one that can be enumerated using the natural numbers: $x_0,x_1,x_2,...$). For instance, suppose the universe is made up of 
a countable cardinality of particles, each of which has a finite number of properties naturally expressible as one or a finite number of real 
numbers (e.g., mass and charge can be expressed as one number, and position can be expressed as three given a coordinate system), which
properties either change continuously or have a countably infinite number of times of discontinuity. Then all we need in order to fully
describe the system is to specify the properties at a countable number of times (e.g., the times at which discontinuities happen as well as
all times that can be expressed by a rational number), and the resulting description can be expressed as a countable sequence of real 
numbers.\footnote{\label{note:product}$^*$This argument uses the fact that if $A$ and $B$ are countable sets, then the Cartesian product set $A\times B$ of pairs $(a,b)$ with
$a$ from $A$ and $b$ from $B$ is also countable. To see this, note that if we can enumerate $A$ as $\{ a_0,a_1,a_2,... \}$ and
$B$ as $\{ b_0,b_1,b_2,... \}$, then we can enumerate $A\times B$ as $(a_0,b_0), (a_1,b_0), (a_0,b_1), (a_2,b_0), (a_1,b_1), (a_0,b_2),...$.} But a countable sequence of real
numbers can, with a well-known trick, be expressed as a single real number.\footnote{\label{note:seq}$^*$Any real number can be remapped to the range
from $0$ to $1$, exclusive, by taking $x$ to $(1/2)+(1/\pi)\arctan x$. Any countably infinite sequence of numbers $x_0,x_1,x_2,...$ between $0$ and $1$ 
exclusive can then be expressed as a sequence of decimal numbers where the $i$th number is of the form $0.x_{i,0}x_{i,1},x_{i,2},...$ (with 
the convention that an infinite terminal sequence of nines is preferred to an infinite terminal sequence of zeroes, say). One can then 
encode all these numbers into a single number of the form 
$0.x_{0,0}x_{1,0}x_{0,1}x_{2,0},x_{1,1},x_{0,2}...$ (this is basically the same pattern as in Note~\ref{note:product}), so that every
digit of every one of the numbers in our initial series occurs at a determinate position in the encoded number. And \textit{a fortiori}
if one can encode any countably infinite sequence of reals into a single real, one can encode any finite sequence of reals into a single
real.} And if instead of a classical particle system one prefers a quantum story, then note that most models of quantum mechanics make 
the wavefunction be a vector in a separable Hilbert space---and the cardinality of the set of vectors in a separable Hilbert space is the 
same as the cardinality of the set of all real numbers. Adding a countable number of discontinuities in case of quantum collapse, we can still
encode the state of the universe as a single real number.

Now, any real number can be encoded into the tilt angle of a physical rod (e.g., encoding $x$ into a rod tilted---with respect to some
axis---at angle $\arctan x$). Thus, we can encode the complete physical state, over all of time of one of the two universes into the tilt angle of a physical rod
in the other universe. Now fix some encoding that can be specified in a relatively brief way. Suppose now that you are in one of the 
universes, and at a precisely specifiable moment (say, one that is a precise number of Planck times since the beginning of your universe)
you tilt the rod at angle that, \textit{mirabile dictu}, happens to match the state of the other universe. Then including the information that the 
rod angle at this moment matches the complete state of the other universe will indeed provide a vast amount of information about your 
multiverse in a fairly brief compass, and hence would be included in the optimal Lewis-Ramsey description, and will be a law. Thus, by
tilting a rod at a specific angle---and surely any rod tilt angle is physically possible for you (though maybe not possible to induce
\textit{intentionally})---you can create a law correlating the rod angle with the other universe. Thus, by waving a rod, you can make
the rod be a vastly informative dowsing rod that by law and not merely by coincidence carries complete information about another universe.
This is highly implausible magic!

\subsubsection{A plurality of bestnesses}
A problem that will be familiar from many of our earlier discussions is that there are many free parameters in the account of 
the bestness of the best system. For instance, given a measure of informativeness $I(T)$ and length $L(T)$ of a system
$T$, we will want to combine them into a measure $f(I(T),L(T))$ of quality of theory by using some kind
of a combining function $f$ such that $f(x,y)<f(x',y)$ 
and $f(x,y)>f(x,y')$ whenever $x<x'$ and $y<y'$. But there are infinitely many functions satisfying these inequalities. Perhaps
with some thought we can find some more reasonable constraints, but it is very implausible to think we can reduce the space
of reasonable candidates to one.

Moreover, we can see that neither $I(T)$ nor $L(T)$ has a unique privileged candidate.

If a world has a phase space---say, defined by values of various natural determinables like charge, position and momentum at various 
times---it makes sense to think of the informativeness of a theory $T$ as inversely related to the size of the set of trajectories through 
phase space (i.e., functions from time to phase space) compatible with $T$. If we could get the set of allowed trajectories down to one,
that would be maximal informativeness. 

But how do we measure the size of the set of trajectories compatible with the theory? The set-theoretic cardinality of the set of allowed
trajectories for many theories that intuitively vary significantly in their informativeness will be the same. For suppose our determinables
are position. Now consider theory $T_1$ according to which there is a single particle which for all time is found in the same position
in three-dimensional Euclidean space, and $T_2$ according to which the the particle moves through three-dimensional Euclidean space over
a continuous trajectory. Clearly, $T_1$ is much more informative than $T_2$. But the cardinality of the space of trajectories allowed by
$T_1$ is \textit{the continuum}, the cardinality of the real numbers.\footnote{A position can be encoded as three real numbers, which can then be recoded as one real number (see note~\ref{note:product}, above).} However, that is also
the same as the cardinality of the space of all continuous trajectories, which is what $T_2$ allows.\footnote{To specify a continuous
trajectory, one only needs to specify its values at all times that are represented by rational 
numbers, of which there are countably many. But a countable sequence of real numbers can be specified by a 
single real number (see note~\ref{note:seq} above).}

We might, on the other hand, try to define the size of the set of trajectories as a volume rather than a cardinality. 
An immediate problem may seem to be that of the choice of units of volume. This is not a serious problem for comparing
degrees of informativeness,  as long as we measure the volumes of the set of allowed trajectories using the 
same units. Even completely arbitrary units like the length of Charles III's forearm and the time between his mother's
and his own coronations can be used for comparative purposes. However, the scaling issue is relevant for the combination
problem. For unless our combination function $f$ has some additional special properties, like being linear in the second variable
(i.e., $f(x,\alpha y)=\alpha f(x,y)$), the choice of scaling can be an issue, because it might be that $f(x,y)<f(x,y')$ but 
$f(x,\alpha y')<f(x,\alpha y)$ for some choice of $x$, $y$, $y'$ and $\alpha$. On the other hand, we might use the scaling worry
to justify a linearity constraint on the second variable in $f$, thereby reducing the arbitrariness of the choice of $f$.

A more serious problem is 
cases of two theories that clearly vary in information content but both allow a set of trajectories with zero volume. For instance, 
suppose a world with only one moment of time, and one determinable: three-dimensional position of a single
particle. A theory that requires the particle to be at the coordinates at the origin reduces the space of trajectories to a subset of zero
volume, as does a theory that merely constrains the particle to lie on the $yz$-plane, since both a point and a plane  have zero volume. 

We might find a way of comparing sets of zero volume. For instance, we might try to find the Hausdorff dimension (which may fractional)
of the two sets, and deem the set with higher dimension to convey less information. And then we could use Hausdorff measure to compare
sets of the same Hausdorff dimension. This approach leads to a pair of numbers, $(\alpha,\beta)$, where $\alpha$ is the Hausdorff dimension of the allowed subset of phase space
and $\beta$ is the $\alpha$-dimensional Hausdorff measure. Thus, instead of combining length with a single number, we need to combine length
with \textit{two} numbers. And what do we do in the case of sets whose Hausdorff measure is infinite with respect to the Hausdorff dimension---these
may need to be compared as well. Furthermore, Hausdorff dimension itself
is not the only way to formalize the concept of dimension.???

Thus we should not expect a canonical measure of informativeness: there will be many free parameters.

Now, length may seem more tractable. Indeed, given a fixed language whose sentences are
finite strings of characters from a finite symbol set, there is no difficulty in making sense of the length of the briefest 
expression of a system of propositions in that language. 

But there are many languages. Just think of such decisions as the choice of grouping notation. In recent
use in logic, for instance, we have parenthesis notation, Polish notation and dot notation. Surely there are many more
reasonable grouping notation. Or think which logical primitives should be included. For truthfunctional connectives,
nand is sufficient, as is nor, as is either of the pairs and-not and or-not, but why should we limit ourselves to a minimal 
sufficient set? Perhaps we should allow all the binary connectives. Or perhaps some but not all. Or perhaps all the ternary
ones. Or perhaps just one seven-place nor. It is reasonable to think there is a privileged set of predicates---the perfectly 
natural ones. But is it reasonable to think there is a privileged set of logical operators? This is murky. Or a privileged
grouping notation? That seems even more dubious.

We thus have little reason to hope in a single distinguished measure of bestness. Now, we might hope that for a wide range $R$ 
of measures of the quality of a theory in a world, there is sufficient overlap between the best theories to ensure that
the things our best science will converge on as the laws will in fact be entailed by the theories that are best according to the 
measures in $R$. We could, then, define a law as a proposition $L$ such that for all quality measures $Q$ in $R$, the $Q$-best theory
entails $L$. But specifying the boundaries of $R$ will be subject to difficulties very similar to those in defining a single canonical 
measure $Q$.

It seems we cannot escape the idea that there is significant vagueness in the concept of a law for the Humean.
Is this a problem?

In Section~\ref{sec:law-explanation}, I argued that in paradigmatic cases where universal generalizations enter scientific 
explanations, they do so as laws. In other words, that $L$ is a law is itself a part of the explanation, and not just $L$ itself. 
Whether this \textit{always} so is not clear, but it is clearly sometimes so. Nobody would bat an eye 
at a scientist saying that planets move in elliptical orbits because
by a law of nature objects attract gravitationally in proportion to their mass and the inverse square of distance.

But then if an account of what it is to be a law of nature is a very complex statement involving a measure
of the quality of theories, and various linguistic facts about lengths of expressions, then our scientific explanations in 
terms of laws are much more complicated than we likely thought. While the concepts figuring \textit{in} the laws may be very
natural, the concept of a law is itself rather unnatural given BSA. Moreover, the plurality of bestnesses implies that there is a serious
vagueness in the concept of a law, which makes our scientific explanations, even in the most precise areas of fundamental physics,
full of vagueness. 

Suppose, on the other hand, that we follow the example of the old deductive nomological model of explanation and deny that laws 
enter explanations \textit{as laws}, notwithstanding the argument of ??backref. Still, the vagueness of the concept of law would affect what counts as an explanation, assuming that generalizations that
are not laws are not allowed in explanations, at least not in the place where we would put a law. We end up damaging the
objectivity of the concept of explanation on this approach: it becomes a linguistic question what is and is not an explanation. 
And if explanation is significantly relevant to justification---for instance, via inference to best explanation---we damage the 
objectivity of the concept of justification. This is perhaps less problematic than damaging the explanations themselves, as would
be the case if the nomicity of the laws were a part of the explanations themselves, but it is still problematic.

Furthermore, our now familiar Aristotelian solution to issues of unacceptable vagueness, which is to ground boundaries in human nature,
is not plausible in the case of laws of nature, because doing so would render laws of nature too anthropocentric. 

??refs on vagueness of laws

\subsubsection{Mind and causation}
It is interesting to note that the full Humean package where causation is grounded in laws and laws are grounded in a best-system 
analysis has a serious problem with any philosophy of mind on which certain causal relations are essential to mental events. This will
include, first and foremost, functionalist accounts on which the pattern of causal interconnections \textit{defines} mental events.
But it will also include any view on which causal interconnections of a particular sort are \textit{essential} to mental events.
For instance, while the functionalist may think that what \textit{makes} an event be a pain is that it is typically caused by
damage and typically causes motivations to aversive behavior, one need not be a functionalist to think that a causal connection between 
an event and motivation is necessary for the event to be a pain. Likewise, plausibly, a necessary condition for $x$ to
infer $q$ from $p$ is that a thought (say, a believing or an assuming) of $x$ with content $p$ causes a thought with content $q$. Finally, it is very
plausible that for a wide range of mental events, an essential part of what makes the mental event be \textit{mine} is that it is 
at least partly caused by me, or by my character, etc. 

But now notice that best-system laws are defined globally in terms of the four-dimensional arrangement of stuff in the universe. 
Whether some lawlike generalization does make it into the best system depends, in particular, on what the future is like. One reason 
is that typical lawlike generalizations tend to non-vacuously apply to events past, present and future, and so if the future 
were sufficiently different, the generalizations themselves would be false, and hence not in the best system. If some such generalization
is needed for a causal relation, say between pain and aversive motivation, then if the future were such that the generalization would fail
to hold, there would be no pain now. 

The idea that whether there is pain now depends on what will happen in the future is itself highly counterintuitive. Additionally, it 
allows for implausible deductive predictions about the future. I am in pain now. My being in pain now requires a causal relation between
pain and aversive motivation. Such a causal relation requires a law of sort $S$. A law of sort $S$, on BSA, requires the future
to satisfy certain constraints $C$. Hence, the future will satisfy $C$. But it is highly implausible (and in tension with the 
historical Hume's scepticism about induction??refs) that we can thus reason \textit{deductively} from present pain to facts about contingent future arrangements
of matter, namely that they satisfy $C$. And if introspection counts as \textit{a priori}, this is \textit{a priori} reasoning about contingent future facts.

Moreover, the same intuitions that disallow deductive inferences about future arrangements of matter from my present
phenomenal states should disallow deductive inferences about arrangements of matter outside the solar system from my present
phenomenal states. But given BSA, the existence of the fundamental laws undergirding a causal connection between 
pain and motivation would have non-trivial implications for how matter is arranged outside the solar system. Laws grounding 
causal connections shouldn't be such as to be purely local (i.e., to have merely vacuous implications for how things are outside
of a local region). 

Perhaps our Humean could simply embrace the non-locality of the grounding of our phenomenal states, and allow deductive inferences
of facts about temporally and spatially distant arrangements of matter from our phenomenal states, \textit{pace} the historical Hume.
But the counterintuitiveness of this is a definite cost for the theory.

\subsection{Indeterministic extensions}
\subsubsection{Probabilistic BSA}
BSA was initially formulated for deterministic laws. But what about indeterministic laws, such as that some quantum setup has a
chance $1/3$ of resulting in outcome $A$ or that a certain indeterministic coin toss has chance $1/2$
of heads? The standard move is to go to a Probabilistic BSA (PBSA). In BSA, we required all the claims of the theory to be true.
In PBSA, we only require non-probabilistic claims to be true. However, now, in addition to optimizing informativeness and 
brevity, we also optimize \textit{fit}, where we check how well the outcomes fit probabilitic predictions. A theory which claims 
there is some large number $N$ of fair (i.e., the chances of heads and tails are equal) and independent  
coin tosses will tend to better fit worlds where the number of heads is closer to $N/2$ than worlds where the number of heads is 
further from $N/2$. Thus, we optimize informativeness, fit and brevity over theories whose non-probabilistic content is true.\footnote{If we 
want a bit more elegance in the formulation, we can drop the requirement that the non-probabilistic content is true and instead
stipulate that a non-probabilistic statement has good fit when it is true but infinitely bad fit when false, so no theory with false
non-probabilistic statements will win the crown of being the best system.} However, PBSA also has a number of serious.

\subsubsection{Violations of the Principal Principle}\label{sec:principal-principle}
Let's make the standard distinction between chances and epistemic probabilities. Chances are objective mind-independent probabilities 
grounded in the laws of nature or causal propensities. Epistemic probabilities, on the other hand, are the probabilities an epistemic agent 
has given evidence. 

Van Fraassen's Principal Principle (PP) holds that there is a tight epistemic relationship between chances and epistemic 
probabilities. Basically, if one's relevant evidence consists in the claim that the chance of some event is $p$, then $p$ should
also be one's epistemic probability,??ref though there are various technical developments that should not be relevant to my examples.??refs
I will now offer two examples that show a tension between PP and PBSA.

For our first example, imagine a world where all the coins have always lain scattered on a large flat surface. There is an 
extremely large number of these coins. One of them is made of gold and all the others are made of copper. You have observed
all the copper coins and found an extremely strong fit between their arrangement and the hypothesis that whether each coin 
is heads or tails is independent across the coins and the probability of heads is always $63/64$. You have not yet seen whether
the gold coin is heads or tails, though you know that it is one or the other.

On PBSA, if the number of coins is sufficiently large, a hypothesis specifying that the chance of a coin being heads is $63/64$ will 
be sufficiently informative to make it into the laws of nature.\footnote{The number of coins has to be sufficiently large to 
offset the loss of brevity due to having to explain what a coin is and what counts as heads.} Thus, if you are sure of PBSA,
given your observations you can be sure that this is a law. For if the number of copper coins is sufficiently large, and the
statistical fit is good enough, you can be completely confident that the fit to the $63/64$ hypothesis will be sufficient for nomicity regardless of whether the gold coin is heads or tails. 

But now if you are sure that the $63/64$ hypothesis a law, then by PP you will have an epistemic probability of $63/64$ for the gold coin being heads. 

And this is wrong. An inductive inference is always weakened by differences of material: if our inductive data is the distribution
of heads/tails across all the copper coins other than one, then it would indeed be reasonable to assign an epistemic probability
around $63/64$ to the remaining copper coin being heads. But if the remaining coin is of a different material, we surely should 
not have the same level of confidence in its being heads. Intuitively, we should shift our epistemic probability closer to $1/2$.

One might object on behalf of PBSA that one should instead posit a law that says that a \textit{copper} coin has chance 
$63/64$ of showing heads. But this is mistaken. The resulting hypothesis is not significantly more informative, but is significantly
less brief since it must mention and define the material of the coin, and hence is not going to be a viable competitor for being 
in the best system.

For our second example, the world consists of some large number $N$ of independent fair indeterministic coin tosses, where $N$ is large
and easily mathematically expressible, e.g., $N=2^{256}$. As we would expect, very close to half of the coin tosses are heads, and 
we suppose that a probabilistic best systems view will have laws that correctly assign an independent chance of $1/2$ (or, if 
one wishes,  something close to $1/2$, a complication I will ignore) to each toss. Moreover, because $N$ is easily expressible,
and expressing it conveys a significant amount of information about the world, the laws also state that the number of tosses is
$N$. 

Now, the following version of van~Fraassen's Principal Principle is very plausible:
\ditem{LPP}{If from a law $U$ it can be proved that the chance of $E$ is $p$, then $P(E\mid \Law(U)) = p$,}
where $\Law(U)$ says that $U$ is a law and $P(A\mid B)$ is the conditional epistemic probability of $A$ given $B$.
Here is another very plausible thesis:
\ditem{INC}{If $E$ and $F$ are logically incompatible and $F$ is logically possible, then $P(E\mid F) = 0$.}
(The restriction to the case where $F$ is logically possible is to handle the intuition that perhaps $P(F \mid F)=1$
even if $F$ is logically impossible.)

Now, let $U$ be our law that says that the world consists of $N$ independent fair indeterministic coin tosses. Let $E_0$ be the event
of not getting any heads. Then it can be proved from $U$ that the chance of $E$ is $1/2^N$, so by \dref{LPP} we have:
\ditem{non-zero-prob}{$P(E_0\mid\Law(U))=1/2^N$.}
But on our probabilistic best systems analysis, it is logically impossible to have $U$ be a law when no heads have ever occurred.
Thus, $\Law(U)$ and $E_0$ are logically incompatible, and so by \dref{INC} we have:
\ditem{non-zero-prob}{$P(E_0\mid\Law(U))=0$,}
a contradiction.

The point here is quite simple. Orthodox probability reasoning shows that it is possible but unlikely that we will have no heads given
a large number of independent fair coin tosses, but our probabilistic best-systems analysis must categorically reject such a possibility.

One might think that assigning zero probability to things so incredibly unlikely is unproblematic. But we also have some other strange
probabilistic results. In defining the laws, we are maximizing a balance of fit and brevity. If in our world half of the coin tosses 
are heads, then a law assiging chance $1/2$ to heads is likely the best balance. But suppose that the frequency of heads isn't $1/2$
but something very close to $1/2$. Then the law may still be that the chance is $1/2$, because a slight decrease in fit due to having
a chance not quite matching the exact frequency might be more than offset by the gain brevity if chance $1/2$ is significantly more
briefly expressible than the actual frequency, say $1/2-3\cdot 2^{-254}$.

Let's suppose $N$ is very large but easily expressible (e.g., $N=2^{2^{2^{2^{2^2}}}}=2^{2^{65536}}$), and let $\scr W_N$ be 
the set of worlds with nothing but $N$ tosses where there are no briefly expressible and highly informative patterns other than 
those conveyed by the independence of the coin tosses, the frequencies and the number of tosses.

Let $F_N$ be the set of all fractions between $0$ and $1$ with denominator $N$. Each member of $F_N$ could be the exact frequency
of heads in some world in $\scr W_N$.

Let $A_{N,1/2}$ be the subset of frequencies in $F_N$ such that there is a world $w$ in $\scr W_N$ in which there is a Humean
law specifying the chance to be $1/2$. For simplicity, suppose $N$ is even, so $1/2$ is a member of $F_N$. For $N$ sufficiently large, we 
should expect $1/2$ to be a member of $A_{N,1/2}$. But for the reasons given above, we would expect some frequencies in $F_N$ that are very
close to $1/2$ to be in $A_{N,1/2}$ as well. If $E_\alpha$ is the event of the actual frequency being $\alpha$, then on our Humean
view we have:
\ditem{PP-zero}{If $\alpha$ is not in $A_{N,1/2}$, we have $P(E_\alpha\mid\Law(U))=0$.}
On the other hand, very plausibly, if $\alpha$ is in $A_{N,1/2}$, then we have a real possibility of having frequency $\alpha$
on the assumption that the law is $U$, and so we would expect:
\ditem{PP-non-zero}{If $\alpha$ is not in $A_{N,1/2}$, we have $P(E_\alpha\mid\Law(U))>0$.}
Frequencies that are sufficiently far from $1/2$ (such as the $0$ in our previous example) are not going to be in $A_{N,1/2}$: the fit 
of these frequencies is too bad. The case of $\alpha=0$ has already been discussed.

What is surprising, however, is that very likely there are some members $\alpha$ and $\beta$ in $F_N$ such that $\alpha<\beta<1/2$
and yet $\alpha$ is a member of $A_{N,1/2}$ but $\beta$ is not a member of $A_{N,1/2}$. For we can suppose that both $\alpha$ and $\beta$
are \textit{extremely} close to each other, and very close to $1/2$, but $\alpha$ is much more briefly expressible than $\beta$, so that 
although the fit of a chance $1/2$ law is slightly poorer for worlds with frequency $\alpha$ than for worlds with frequency $\beta$,
the greater brevity of an expression for $\alpha$ makes a law giving the chance of heads as $\alpha$ have a better balance of fit and
brevity in a world with frequency $\alpha$ than a law giving the chance as $1/2$, while a law with chance $1/2$ has a better balance of fit and brevity in 
a world with frequency $\beta$ than a law giving the chance as $\beta$.

I cannot give precise examples here, because
we do not actually have the measures of fit in hand and estimating the brevity of the shortest expression of some number is often a very
difficult task. But suppose I am right. Then by \dref{PP-zero} and \dref{PP-non-zero}, some frequencies in $F_N$ that are further from
$1/2$, such as $\alpha$, will have a higher conditional probability on $U$ than some frequencies that are closer to $1/2$, such as
$\beta$. I imagined that the counterintuitive result that $E_0$ has zero conditional probability on $U$ fails might be given the ``excuse''
that $U$ entails a chance that is so tiny that we might as well take the probability to be zero. But that excuse won't work for $E_\beta$
getting zero conditional probability on $U$. For if we say that $U$ entails such a low chance for $E_\beta$ that $E_\beta$ deserves
conditional probability zero, then $E_\alpha$ should get conditional probability zero as well, since $E_\alpha$ has an even lower chance
on $U$.\footnote{For a large number of tosses, the chance of getting a particular frequency $x$ of heads has an approximately normal distribution 
centered on $1/2$ if the true chance of each independent toss is $1/2$, and hence drops off as $x$ moves away from $1/2$.}

??refs

\subsubsection{$^*$Chance and propositions}
There is a variety of slightly different accounts of chance that can be given on PBSA. Let ``the mosaic'' refer to the arrangement of
properties that are systematized in the best system. Here are a few options, depending on whether one takes conditional or unconditional
chances to be fundamental:
\ditem{uncond-chance-no-init}{Event $E$ has unconditional chance $p$ if and only if the mosaic's best system entails that $E$ has chance $p$.}
\ditem{uncond-chance-init}{Event $E$ has unconditional chance $p$ if and only if the mosaic's best system conjoined with a complete
    specification of the mosaic's initial conditions entails that $E$ has chance $p$.}
\ditem{cond-chance-init}{Event $E$ has conditional chance $p$ on $C$ if and only if the mosaic's best system entails 
    that the conditional chance of $E$ on $C$ is $p$.}
There is room for much technical discussion of the details here, but notice that each of these three accounts is viciously circular: it
is attempting to define a chance (conditional or not) by a definition that itself makes use of the concept of chance.

There is a complicated way out of this difficulty that modifies the PBSA.

Suppose that we have a language $\scr L$ whose vocabulary can express all the fundamental physical concepts acceptable to 
the Humean, and has one additional function symbol, $\Chance(x)$. This new function symbol is uninterpreted, and  it is stipulated that sentences using that function symbol are 
neither true nor false. Let $M$ be some appropriate set of mathematical axioms that include the correct axioms of set theory (perhaps indeed
they include all truths of set theory) as well as the 
right axioms of probability formulated using the chance symbol (so these will be axioms of unconditional or conditional probability, depending 
on whether the chance symbol is unary or binary).  Say that a set of sentences or ``theory'' $T$ in $\scr L$ is \textit{non-false} provided that no false sentence can be formally proved from $T\cup M$. This is equivalent to saying that all the provable consequences of $T\cup M$ that do 
not use the chance symbol are true (note that since everything follows from a contradiction in classical logic, if
$T$ violates the laws of probability in its use of the chance symbol, $T$ will automatically count as false). We can now specify that the best system is the non-false theory $T$ that optimizes brevity, fit and 
informativeness. 

Defining informativeness is difficult, but we might define it by looking at two kinds of information conveyed by the 
provable consequences of $T$. First, we have sentences that do not make use of the chance symbol. The informativeness
of that part of the consequences will be measured much as in a non-stochastic BSA. The informativeness of a statement
of form $\Chance(E)=r$, where $E$ does not make use of the chance symbol, will be null if $r=1/2$, and while if $r>1/2$, it will increase monotonically with both $r$ 
and the informativeness of the non-probabilistic claim that $E$ occurs, and if $r<1/2$, it will also decrease monotonically with $r$ while increasing monotonically with the informativeness of the non-probabilistic claim that $E$ does not occur. We might for
simplicity disallow claims using the chance symbol but not of the form $\Chance(E)=r$. And, of course, we
will have to beware of overlap in information between the chancy and non-chancy sentences, and so on. 

Finally, we measure fit by looking at all the provable consequences of the form $\Chance(E)=r$ where $E$ does not contain another
instance of the chance symbol, and saying the fit of each is a measure of closeness between $r$ and the numerical truth value of $E$ (falsehood
being zero and truth being one)---perhaps a measure according to some standard scoring rule.??refs

The details are doubtless very difficult, but that is to be expected.
Given all this, we get a best system $T$ consisting of sentences some of which fail to express a proposition. Given that laws of nature
are not linguistic entities in a particular language, and that they are factual, and hence capable of being true, 
we surely cannot take  a set of uninterpreted sentences like $T$ in $\scr L$ to be a system of laws. 

But we can now make a further move.
Let $B_T$ be the proposition that all the sentences in $T$ are provable consequences of the best system. Then $B_T$ is indeed a proposition
and factual. We can then say that the laws of nature are all the logical consequences of $B_T$. The laws, then, are not the sentences derivable from the best system, 
since these include uninterpreted sentences, but rather the laws are the consequences of the claim that these sentences are sentences of the 
best system.\footnote{An alternative would be
to consider the logical consequences of the proposition that $T$ \textit{is} the best system. But that would be less satisfactory. For it
seems that one could have a world with all the laws we have \textit{and} more. But if it is a law that $T$ is the best system, then it seems
there couldn't be a world with additional laws beyond those in $T$.???}

One problem of this approach, however, lies on the probability side. Suppose we are in a world with a fair coin sequentially tossed a large finite 
number $N$ of times so set up that if $T$ is the best system then $T\cup M$ proves a sentence saying that the coin was indeed tossed 
sequentially $N$ times and proves the sentence ``$\Chance(H_1)=1/2$'', where $H_1$ is the event of the first throw being heads.\footnote{This 
will require $N$ to be a number that is sufficiently briefly expressible to make it into the best system.} Then $B_T$ will be a law of nature.
In the notation of Section~\ref{sec:principal-principle}, it will be law that the frequency of heads is a member of the set $A_{1/2,N}$.
This set contains $1/2$, and contains some frequencies in $F_N$ very close to $1/2$, but also excludes frequencies just as close to
$1/2$ as some of the included ones, on the grounds of that some of the excluded ones are so briefly expressible (e.g., $1/2+1/2^{256}$
in a world with $N=2^{256}$) that the brevity cost of the greater fit is worthwhile, so that if the frequency
were one of the excluded ones, its being what it is would be a law. A law of nature that says that the frequence
lies in the messy set $A_{1/2,N}$ looks little like the 
kinds of laws scientists posit. We do not have laws of nature allowing and disallowing events based on the brevity of the expressibility 
of facts about these events.

\subsubsection{Non-Humean chances}
There is another approach to chances, however, than PBSA. Suppose that we take chances themselves to form part of the Humean
mosaic, perhaps being fundamental properties, rather that being something defined via patterns in the mosaic. Then a 
non-deterministic theory can stick with BSA. The requirement that chancy statements be \textit{true} now is cashed
out in terms of the chances in the Humean mosaic. It is now quite possible for the best system to say that the chance
of heads is $1/2$ even if the coin never comes up heads. For we no longer optimize fit, but informativeness. And the
claim that the chance of heads is $1/2$ might well be quite informative, since it could tell us about the stochastic 
properties of a vast number of coin tosses---even if it does not give us useful information about the outcomes of
these tosses.

From the Humean point of view, the down side of this is that chances are too much like causal propensities, which the
typical Humean wants to reduce to patterns in the mosaic. After all, if our account of chance is not based on frequencies,
then it seems that our best option for what it means to say that the chance of heads is $1/2$ is that the immediate
cause has a propensity of $1/2$ for heads.

Thus, if we allow chances into the mosaic, we probably should allow causation as well. The resulting BSA has a richer
mosaic, and is harder to refute by counterexample. The problem that coincidental correlations become rigidified into
magical laws??backref is less pressing once it is clear that the magical laws are not causal laws---for causation is 
now a part of the mosaic, and hence mere correlations do not yield causation. However, we still have the problem of 
the plurality of bestnesses. And because we have so significantly enriched the mosaic, the main reason to believe the
BSA is weakened. For that main reason is ideological parsimony: the elegant fewness of fundamental concepts. Once we 
have introduced causation and causal propensities, one of the great intellectual assets of BSA---giving
an account of causation??---is gone. On the other hand, since causal facts can be used for explanation, the explanatory difficulties for BSA are mitigated, since at least some causal explanations become available.

Thus, we have lost the parsimony which is the main motivator for Humean views, while retaining some of the 
difficulties, such as magic (albeit now of a non-causal sort).

%% revised up to here

\section{Aristotelian laws}
The most common Aristotelian approach to laws is to ground them in the essential powers of substances, which in turn are
grounded in the substantial forms. If an electron is a substance, then the form of an electron gives it the power to attract positively 
charged things and repel negatively charged ones, and such facts ground the laws of electron behavior. And if a rabbit is 
a substance, then the form of a rabbit gives it the power to reproduce with another rabbit, which grounds laws of rabbit
behavior. 

This makes laws explanatory, without introducing any additional entities beyond the forms that we already need to explain
the normative side of nature. Indeed, the integration of the normative and explanatory is central to the Aristotelian
optimism on which things typically act rightly (we might even speculate that in the case of some things, such as 
subatomic particles, they \textit{always} act rightly), and hence we can defeasibly infer ought from is.

Now let us recall the example from section~??backref:
\ditem{DN1bis}{All massive objects attract gravitationally.}
\ditem{DN2bis}{The Sun is a massive object.}
\ditem{DN3bis}{So, the Sun attracts gravitationally.}
This was problematic, because it is plausible that universally quantified statements are partly grounded in their
instance, and hence \dref{DN1bis} is partly grounded in \dref{DN3bis}, which makes for circularity in the order
of explanation as grounding is a type of explanation. Supposing for simplicity that the sun is a substance and
has a stellar form, the Aristotelian, however, can say that having stellar form \textit{entails} engaging in
gravitational attraction.
\ditem{DN1a}{Necessarily, all objects with stellar form attract gravitationally.}
\ditem{DN2a}{The Sun has a stellar form.}
\ditem{DN3a}{So, the Sun attracts gravitationally.}
But the necessary truth \dref{DN1a} is not even partly grounded in \dref{DN3a}. In other words, the Aristotelian
can solve our initial problem by adding necessity to the explanation.

That said, there is a complication. Arguments in Section~\ref{ch:IX}.??ref will make it plausible that on an
Aristotelian view there are no stars. So we will need to modify the story, e.g., to talk about the plurality of
substances that are Sunwise arranged??ref:PvI:
\ditem{DN1ab}{Necessarily, all substances with forms from among $F_1,...,F_n$ attract gravitationally.}
\ditem{DN2ab}{The plurality of substances arranged Sunwise include substances with forms from among $F_1,...,F_n$.}
\ditem{DN3ab}{So, the plurality of substances arranged Sunwise attract gravitationally.\footnote{I am assuming that
a plurality that includes some attractors counts as attracting. If not, the formulation will be more complex.}}

But there are some further serious complications. The first is that classical Aristotelianism is committed to the principle that 
substances do not have substances as parts.(??cross-refs, ??refs) But then if a rabbit is a substance, the electrons in
it are not substances. Instead, the electrons in the rabbit may be accidents of the rabbit, or ``virtual objects'' 
constituted by bundles of rabbit causal powers. If so, then the laws of electron behavior will be grounded differently
for different electrons. The behavior of electrons in humans and those in rabbits will be grounded by the human and the 
rabbit forms, respectively. Free-floating electrons, on the other hand, would seem to be self-standing substances, and
their behavior will be grounded by electronic substantial forms. On this view, the grounding of the laws of electron 
behavior appears quite complex.

The easiest way to remove this complexity is to allow substances to be parts of other substances. In this book, however, 
I am trying to maintain some neutrality on the details of various robust Aristotelian ontologies, so it is worth seeing
what can be done without such a controversial move.

Forms of different beings can (and presumably do) common patterns of behavior.  This is a puzzle for the Aristotelian. 
Why do oak and rabbit and human
electrons behave the same way? To give a partial answer, we might posit a ``fine structure'' in the forms, whereby the oak, 
rabbit and human forms (and those of standalone electron substances) have a common component coding for the behavior of 
electrons. This common component could be found in a genus that the forms are specifications of. Or we might simply suppose
that the common efficient cause or causes of all of these substances---say, God, or the particles in the early universe---have
so acted as to produce substances with a variety of forms all of which code for common behavior of electrons. 

There are, however, two kinds of laws that pose a special problem for the Aristotelian approach. One kind is high-level
structural laws that depend on all physical things having a common pattern to their behavior. The conservation of energy
is a paradigm example. While the forms of electrons and photons (and/or of larger substances containing them) can make 
it a law that energy is conserved in the interactions of electrons and photons, it is logically possible to have 
physical objects that interact with, say, photons in a way that does not conserve energy. The law of conservation of 
energy appears to be partly grounded in the fact that there in fact are no such things---that all physical substances have a 
forms that satisfy the constraints of the law of conservation of energy. But if $F_1,...,F_n$ are all the forms
of existing physical objects, then $F_1,...,F_n$ are not sufficient to ground the conservation of energy without some
additional metaphysically contingent fact such as that there are no physical objects with a form not among $F_1,...,F_n$. 

A second kind of problematic law is exemplified by the Second Law of Thermodynamics. The Second Law of Thermodynamics
has long presented a puzzle given that the physics relevant to the behaviors of the particles whose joint entropy tends to increase
is invariant under a time-reversal.??ref The usual contemporary solution is that what explains the increase of entropy is a physical dynamics
that can be time-reversal-invariant combined with low-entropy initial conditions. For given low-entropy initial conditions,
entropy, as it were, has nowhere to go but up---there are a lot more possibilities for transitions from low to high entropy states
than from low to low entropy states.??ref Thus the Second Law is not grounded in the natures of the interacting objects, 
but in those natures as combined with the metaphysically contingent fact that we started in a low-entropy state.  We might 
call laws that depend on special boundary conditions ``impure laws''. 

The two families of laws---the high-level structural laws that transcend the natures of particular objects and the impure
laws that depend on boundary conditions---are ideal candidates for BSA. BSA does not care whether the laws depend on boundary
conditions or not, just how elegant and informative they are. But they do create problems for Aristotelian accounts, 
and more generally for other ``pushy'' accounts of laws. For it does not seem that any kind of ``pushy'' law 
forces the initial conditions to have a low entropy. And while a ``pushy'' law can move things around in our world, it is
somewhat mysterious how it can keep the kinds of things that would have the power to violate the structural laws out of 
existence.

There are several options for the Aristotelian at this point. The first is to bite the bullet, and hold that the structural
and impure lawlike generalizations are not actually laws of nature. This is fairly plausible in the case of impure laws, but
less so in the case of structural laws. 

The second option is to notice that at least some of our problematic laws can be accounted for as follows. We
suppose that in fact none of the physical substances in existence have a causal power to produce a violation of the law directly
or indirectly, where indirect production of a violation would be the production of a causal chain leading to such a violation.
In short, there are no physical potential violators of, say, the conservation of energy, and no physical potential producers of 
such violators. We then say that whenever we have a ``lawlike fact'' with the property that nothing physical is a potential violator
or potential producer of a potential violator, that fact is a law. We still need some account of what makes a fact lawlike. One move would be simply to defer to the best version of BSA: a lawlike
fact is one that would be a logical consequence of the best system. And, finally, we just bite the bullet on any generalizations that
cannot be accounted for in this way: these we just do not account to be laws.

This may seem to involve us in all the difficulties of BSA. But that need not be the case. First, we might only make the move for
non-stochastic laws, denying that structural or impure generalizations that are stochastic in nature (such as the Second Law of 
Thermodynamics) are laws of nature. Second, the explanatory difficulties of BSA are defused in our present context. For 
laws grounded in this way are explanatory simply in a causal-privative way. We explain, say, why a violation of 
conservation of energy did not happen by saying that there was nothing physical with the causal power to generate such a 
violation. This is a genuine explanation independently of whether the conservation of energy is a law. The
explanatoriness of the laws here does not flow from BSA. On this story we might happily embrace the plurality of accounts of
bestness, because the laws explain simply in virtue of their content, not in virtue of their being \textit{laws}.

The third option is to embrace theism, and allow for two kinds of laws: laws grounded in the essential powers of physical substances
and lawlike generalizations about physical substances intended by God to hold. This will be a partial occasionalism: some explanations
in terms of laws will go back to created substances, but others will go back to God (and presumably some cases will be explained by
a combination of the two). But such a partial occasionalism does not appear to be particularly problematic, since it
allows for non-occasionalist creaturely causation, and only makes some lawlike generalizations be laws by divine
fiat.

\section{Some non-Aristotelian alternatives to BSA}
\subsection{Armstrongian universals and laws}
Armstrong grounded laws of nature in nomic necessitation relationships between universals.??ref For instance, 
a classical physicist might say that the universal
\textit{having one kilogram of mass} nomically necessitates the universal \textit{exerting a gravitational
force equal to $6.67... \cdot 10^{-11} m_2/r^2$ Newtons on any object of mass $m_2$ kilograms at 
distance $r$ meters}. 

The account has some similarities to an Aristotelian grounding of laws in 
forms, since forms are individualized universals. One difference is that the nomic necessitation relation
is a primitive that on the Aristotelian view is replaced by broadly logical entailment. This difference
favors the Aristotelian, in that nomic necessitation is a mysterious additional theoretical ingredient the
Aristotelian does not need, while (broadly) logical entailment is used all over the place in metaphysics
anyway. However, this
difference can be bridged: we can suppose a variant of the Armstrong view where we have broadly logical
entailment instead.

The bigger difference is that the Aristotelian
approach makes use of causal powers grounded in the forms. This yields an advantage for the Aristotelian.
Causal powers have an explanatory asymmetry: the activation of the causal power explains the effect. 
Entailment relations, however, can go in both directions. The universal \textit{exerting a gravitational
force equal to $6.67... \cdot 10^{-11} m_2/r^2$ Newtons on any object of mass $m_2$ kilograms at 
distance $r$ meters} will broadly logically entail, and hence also nomically necessitate (since nomic
necessitation is a weaker relation than logical entailment), the universal \textit{having one kilogram of mass}.
For it seems to be part of the very nature of the gravity of our world that it is produced by mass in the
above way. But then we do not have an explanatory asymmetry, whereas surely there should be one: mass causes
gravitational force. Perhaps the nastiest version of this problem will occur in a world with two-way determinism, where 
complete past states necessitate complete future states and \textit{vice versa}. 

\subsection{Pushy global laws}
An account on which there are pushy global laws has many of the same advantages over Humeanism that Aristotelianism
does. But it is worth noting that there may be some advantages of an Aristotelian account.

First, causal powers are familiar things: we have them ourselves, and exercise them whenever we move. Globally pushy laws of nature are rather more mysterious. 
They appear to be metaphysically contingent principles that move stuff around, and that is puzzling. The idea of
things moving things is very familiar, but \textit{principles} moving things are more mysterious, though granted
they do have roots going back to pre-Socratic \textit{archai}.

Second, Aristotelianism may have a further slight advantage over pushy global laws given that we should take seriously
the hypothesis that we inhabit a multiverse with different universes having different patterns of physical behavior. 

For if the pushy global laws are fully global, they will exist at the level of the multiverse as a whole, which forces the
laws of physics to be complex, generating different patterns of physical behavior in different universes. They might do this
by having different types of particles or fields in different universes, and imposing different laws on the 
different types of things, or by directly imposing different patterns of behavior on different regions of the multiverse.

The Aristotelian, on the other hand, can simply ground the differences of behavior between universes by supposing 
the substances in different universes to have different forms. There is no need to further suppose complex conditional 
laws that impose different behavior on different kinds or regions. This is perhaps a neater solution.

Could the defender of pushy global laws instead suppose that the enforcers of the laws would be divided between the 
universes, so the ``globality'' is not as broad as to the multiverse, but only extends to the level of each universe?
But now the account is rather Aristotelian. Instead of the laws being something like fully global principles, they are 
more like forms that govern particular concrete entities, namely particular universes. We might even say that such a 
view is just a variant on Aristotelianism where the substances are universe-sized. We would then have two ways of 
developing such an Aristotelianism: we might suppose the universes to be the only substances, or more attractively 
in light of the humanistic arguments in this book we might allow (\textit{pace} classic Aristotelianism) for there to 
be mid-sized substances like ourselves that are parts of the universes. 

\section{Conclusions}
We have multiple good reasons to reject Best-Systems Accounts of laws. An Aristotelian account of laws as grounded
in the forms of objects is a powerful alternative to BSA, though it has some difficulties as well---difficulties
that are perhaps especially way remedied on a theistic version. Armstrongian and pushy global laws also provide
powerful alternatives to BSA, but there are still some reasons to prefer an Aristotelian account to them.

\chaptertail
