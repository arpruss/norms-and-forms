\def\mychapter{VI}
\input{chapterhead}
\chapter{Mind}\label{ch:mind}
\section{Multiple realizability}
Some conscious beings have brains. Start with the hypothesis that it is a necessary 
truth that all conscious beings have brains. 

First, this hypothesis is just implausible: it seems quite plausible that we could have
conscious beings with a very different body plans. 

Second, observe that brains are a specific type of organ in DNA-based animals.
To have a brain, thus, you need to have DNA. To have DNA, you need to have hydrogen
atoms. To have hydrogen atoms, you need to have electrons. A particle with a different 
electric charge would not be an electron, and the charge of the electron is definable in terms of 
the fine structure constant $e^2/(2\varepsilon_0 hc)$. If the fine structure constant
were different, we wouldn't have electrons. We might have shmelectrons that behave almost 
exactly like electrons, but they wouldn't be electrons. If we didn't have electrons, we wouldn't
have hydrogen, but at best shmydrogen. And if we didn't have hydrogen, we wouldn't have DNA,
but at best shmDNA. 

But now imagine a world extremely so similar to ours that no instruments of a sort
humans ever have a hope of constructing could ever tell the difference, but where, nonetheless,
the fine structure constant has a slightly different value. In that world we have beings that
behave, as far as any of us could ever tell by external and internal examination, just as we 
do. But they not only would \textit{be} unconscious zombies, they would \textit{have to be}
zombies---no beings with shmelectrons in place of electrons could be conscious on the hypothesis 
we are considering. That such a slight difference in physical constitution would make the
difference is extremely implausible. 

Third, if the hypothesis is true, we should be quite surprised at the existence
of consciousness. The argument just given shows that consciousness requires the precise value of
the fine structure constant that we have. How likely is that? Well, there are infinitely
many possible values that agree with our world's fine structure constant to within a thousand
significant figures. Unless our fine structure constant turns out to be some very special distinguished
value (for a while, some physicists thought it was exactly $1/137$??refs, but later measurements 
disproved that, and a recent estimate is $1/137.03599921$), the chances of getting the exact value
randomly we have is zero or at best infinitesimal. Given the fact that consciousness has great
value significance (??shvalue??), if consciousness depends on brains, and hence on electrons, then
the fact of consciousness would loudly cry out for explanation. 

The line of thought above is akin to fine-tuning arguments, where narrow ranges of fundamental constants
are claimed to be needed for life, and call out for explanation, with two options being typically offered:
a multiverse (unlikely things will happen if dice are rolled enough times) and an intelligent designer. But there are
some relevant differences in our present case. 

First, our range is much narrower---only one exact
value is compatible with consciousness on the hypothesis we are exploring---which means that objections from
the rescaling of ranges do not apply as they do in the case of the fine-tuning argument.??coarse-stuff 

Second, plausibly an intelligent designer
would be conscious, and if consciousness requires brains as we are hypothesizing, a designer will be of no help
here, on pain of circularity. 

Third, because the consciousness-permitting range has only one point on it, 
and there are uncountably infinitely many possible other values of the fine structure constant, hitting this value
will not automatically be probable even given a multiverse. If you spin a continuous fair spinner once, 
your chance of hitting a particular value is zero or infinitesimal. But the same is true for any finite number of independent spins.
Moreover, in classical probability theory, this is also true for a countably infinite number of spins. And for an
uncountably infinite number of spins, the probability is simply undefined. In light of this, the multiverse hypothesis only
really solves the problem of consciousness in our context if it is a Lewisian or Tegmarkian hypothesis that \textit{every} 
possible cosmic arrangement is realized in reality. But such a hypothesis only solves the problem at the expense of introducing serious
sceptical problems, since there will be cosmoses, just as real as ours, where every coherent sceptical hypothesis hold, and it does
not appear reasonable to think that we got so lucky as to escape them all.??refs

Tying consciousness to brains thus links consciousness to the precise laws of nature we have. That is not only intuitively implausible
but leads to serious problems. We should think that there is some flexibility in what kinds of bodies conscious beings can have.

Perhaps instead of supposing that consciousness is tied to brains, we could suppose that consciousness is tied to a range of brain-like
organs. Thus, consciousness would be compatible with having somewhat different laws of nature, resulting in fundamental particles
slightly different from the ones we have, and behavior somewhat different from the one we have, but not \textit{very} different.
But now consider the Mersenne questions about the boundaries of physical constitution compatible with consciousness. These
questions cannot be settled by invoking human nature, since they are questions that transcend the nature of any one species.
Nor can they be settled the way Mersenne settled his original questions, by invoking God's creative decision, because we are
supposing that the connection between consciousness and brain-like organs is necessary. We should avoid Mersenne questions that
do not seem to have a plausible answer. 

Furthermore, the issue of worlds practically indistinguishable to our instruments but where one has consciousness and the other
does not returns on the range view. Suppose that the upper cut-off for the fine structure constant to be compatible with
consciousness is $1/100$ (recall that our world's fine structure constant is about $1/137$). Then either $1/100$ is the
highest value compatible with consciousness or the lowest value incompatible with consciousness. If it is the highest value
compatible with consciousness, there should be a world $w_c$ with consciousness and fine-structure constant $1/100$ and a 
world $w_z$ that is practically indistinguishable from $w_c$ but where the fine-structure constant is slightly more than 
$1/100$ and hence where there are only zombies. If, on the other hand, $1/100$ is the lowest value incompatible with 
consciousness, then for a value of the fine-structure constant $(1/100)-\e$ for some positive $\e$ less than one divided by a 
googolplex there will be a world $w_c$ with consciousness. Then we should expect there to be a possible world $w_z$ with fine-structure constant
$1/100$ that is practically indistinguishable from $w_c$ (a difference of one in a googolplex should not affect anything observable),
but $w_z$ will be a zombie world, since we have assumed that a fine-structure constant of $1/100$ is incompatible with
consciousness. So in either case there will be a world with consciousness and a world with zombies which are physically
indistinguishable to humans.  But it is implausible that consciousness should depend on physical features that are so insignificant.

This line of thought pushes one to a very liberal view about what kinds of physical constitutions are compatible with consciousness.
It does not appear, in particular, that consciousness should depend on having a physical constitution that includes brains or anything
similar to brains. We thus have very significant multiple realizability.??check-mr-book

\section{Functionalism}
Full-blown dualism, of course, yields significant multiple realizability. Indeed, a minded being's body could be an oak tree or even a 
rock, as long as it had the right kind of non-physical mind on dualism. We will discuss the interaction of dualism with Aristotelian
forms in Section~\ref{sec:dualism}. In the meanwhile, however, let us continue to consider broadly naturalistic accounts of mind.

We have seen that there is good reason to be very liberal about the type of physical aspect that a minded thing can have.  But 
if we are to remain in a broadly naturalistic theory, we need to put some limits on the kinds of physical constitutions that
minds can be based on. We saw earlier that limits based on particular natural kinds---DNA, brains, electrons, etc.---are highly
implausible. The most plausible remaining option is functionalism: to have a mind is to have a certain kind of functional structure,
so that, necessarily, if there is a functional isomorphism between two entities with their respective functionally-specifiable 
causal histories, if one of these entities has a mind, so does the other. Moreover, the isomorphism between causal histories implies
a significant degree of identity between the mental histories. 

On what we may call strong functionalism, their purely internal mental histories
will be the same, and in particular they will have qualitatively the same states in their histories---whenever one felt hot, so did
the other, and whenever one had a perception as of red, so did the other. The restriction to purely internal mental histories allows
for some externalism. Thus, an individual on Earth may be thinking about water, while the analogous thought in an isomorphic individual
on Twin Earth, may be thinking about XYZ, where XYZ fulfills the same causal role on Twin Earth as H$_2$O does on Earth. 

On weak functionalism, the non-qualitative purely internal mental histories will be the same, and whenever one has a conscious state, 
the other has an analogous conscious state, but the exact qualitative phenomenal character of the conscious states may depend on 
the precise physical substrate underlying the two conscious states. On weak functionalism, a silicon-based isomorph of a human being,
will have some sensation in a functional state isomorphic to a human's eating sugar, but that sensation's qualitative character may
be different from the taste of sweet. 

I will now argue that functionalism, whether weak or strong, has serious problems which can be solved by combining it with an Aristotelian hylomorphism.\footnote{The arguments
based on the possibility of malfunction will be based on the ones in ??ref:Koons-Pruss.} 

Begin with the well-known observation that simple causal systems, like the electrons buzzing inside a rock, can be re-interpreted as 
emulating the functioning of our brains, simply because they have such a vast number of states.??refs If this is right, then functionalism 
appears to lead to the absurd thesis that rocks not only think, but think like we do. One version of this argument will be given in ??forward:appendix. 

One might try to get out of this difficulty by insisting that gerrymandered functional systems do not count: only simple causal
systems count as implementing the functions. However, it is very likely that complex evolved brains like ours do have some significantly
gerrymandered states. One might try to draw a distinction between more and less gerrymandered systems, however. The functional states 
that need to be attributed to a rock to re-interpret it as thinking our thoughts are doubtless many orders of magnitude more gerrymandered
than our functional states. But now we have a nasty Mersenne question again: what makes it be the case that the transition between the degrees
of gerrymandering compatible with having a mind those incompatible with having a mind lies where it does?

Next, consider the question of the reliability of functional systems. Whether our universe is deterministic or not, functional systems are imperfectly reliable.
How reliable do they need to be to count as the functional systems they are? Consider a subsystem that given two inputs representing numbers
puts out their sum 99.9\% of the time, and 0.1\% of the time puts out the product. Obviously, it is more reasonable to interpret it as 
a reliable addition system than an extremely unreliable multiplication system. But suppose the system puts out sums 50.1\% of the time
and products 49.9\% of the time. What then? 

There are two natural cut-offs. We could require that a system is defined by how it behaves 100\% of the time or by how it behaves
more than half of the time.??

\section{Vague minds}
??

\section{Dualism}\label{sec:dualism}

??Doesn't functionalism have the same problems?

??vagueness of consciousness

\section{Teleology and representation}
\section{Teleology and mental causation}
%teleosemantics??
\section{Teleological animalism}
\subsection{Animalism}
\subsection{Cerebra}
\section{Soul and body ethics}
\section*{Appendix: Functionalism gone too far}
Consider a deterministic functional system $Q$ consisting of a 
finite number of possible computational subsystems $S_1,...,S_N$, where $S_k$ is always in exactly one state from the finite set $\scr A_k$
of possible states at each of the discrete ``significant'' moments of time $t_1,...,t_m$ over its finite lifetime\footnote{A standard modern digital computer only has defined 
computational states at ticks of its internal clock. Between ticks of the internal clocks, it is in an analogue state that is not
computationally defined. A lot of careful engineering goes into ensuring that the states become properly ``digital'' at the clock ticks.}, and 
a finite number $I_1,...,I_M$ of sensors, where $I_k$ is always in exactly one state from the finite set $\scr I_k$ of possible input
states.  We can think of $Q$ as a finite digital computer. The total state of the system at any given time can be represented as the
$(N+M)$-tuple $(a_1,...,a_N,b_1,...,b_M)$ where $a_k$ is a state in $\scr A_k$ and $b_k$ is a state in $\scr I_k$. We can designate some of
the subsystems as outputs, connected to external effectors (muscles, motors, lights, etc.)  Furthermore, we suppose there are functional laws which provide a 
mapping $f$ from the state of the system at time $t_i$ to the state at time $t_{i+1}$. Thus, $f(a_1,...,a_N,b_1,...,B_M)= (a_1,...,a_N,b_1,...,b_M)$ 
provided that the system would transition from state $(a_1,...,a_N)$ to state $(b_1,...,b_N)$.\footnote{The mapping is independent of time. But we 
can always suppose there is a finite clock, e.g., given by a subsystem $S_k$ such that the set of states $\scr A_k$ is
the set of times during the system's lifetime, and with the transition rule that the time always gets incremented. Or we can suppose an input
from an external clock.} Presumably any deterministic analog system can be approximated by such a system $Q$ to aribitrarily high precision.
We suppose for convenience that there is some fixed initial state of $Q$, with fixed initial input and computational states.

Now consider a different system $P$ consisting of a single particle moving through space parallel to the $x$-axis, with a constant $x$-velocity,
at the same discrete sequence of significant times $t_1,...,t_m$. The system $Q$ has a finite number $J$ of possible input state 
combinations $(b_1,...,b_M)$
where $b_k \in\scr I_k$ for each $k$: the number $J$ is at most the product of the cardinalities of the sets $\scr I_1,...,\scr I_M$ (some
combinations might be impossible).
Let $y_1,...,y_J$ be distinct possible $y$-coordinates. Let $\phi$ be a one-to-one function from all possible input states of $Q$ 
to the set $\{ y_1,...,y_J \}$. We can then encode an input state $(b_1,...,b_M)$ of $Q$ in the $y$-coordinate $\phi(b_1,...,b_M)$ of $P$.
Inputs of $Q$ are then mapped to pushing the particle from one $y$-coordinate to another, sufficiently quickly to do so between 
significant times.\footnote{Not to exceed the speed of light, we may need to make the $y_i$ be fairly close together spatially.}
We still need to define the computational states of $P$ and the state transitions. ???

??????????????????

\chaptertail 


