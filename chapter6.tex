\def\mychapter{VI}
\input{chapterhead}
\chapter{Mind}\label{ch:mind}
\section{Multiple realizability}
Some conscious beings have brains. Start with the hypothesis that it is a necessary 
truth that all conscious beings have brains. 

First, this hypothesis is just implausible: it seems quite plausible that we could have
conscious beings with a very different body plans. 

Second, observe that brains are a specific type of organ in DNA-based animals.
To have a brain, thus, you need to have DNA. To have DNA, you need to have hydrogen
atoms. To have hydrogen atoms, you need to have electrons. A particle with a different 
electric charge would not be an electron, and the charge of the electron is definable in terms of 
the fine structure constant $e^2/(2\varepsilon_0 hc)$. If the fine structure constant
were different, we wouldn't have electrons. We might have shmelectrons that behave almost 
exactly like electrons, but they wouldn't be electrons. If we didn't have electrons, we wouldn't
have hydrogen, but at best shmydrogen. And if we didn't have hydrogen, we wouldn't have DNA,
but at best shmDNA. 

But now imagine a world extremely so similar to ours that no instruments of a sort
humans ever have a hope of constructing could ever tell the difference, but where, nonetheless,
the fine structure constant has a slightly different value. In that world we have beings that
behave, as far as any of us could ever tell by external and internal examination, just as we 
do. But they not only would \textit{be} unconscious zombies, they would \textit{have to be}
zombies---no beings with shmelectrons in place of electrons could be conscious on the hypothesis 
we are considering. That such a slight difference in physical constitution would make the
difference is extremely implausible. 

Third, if the hypothesis is true, we should be quite surprised at the existence
of consciousness. The argument just given shows that consciousness requires the precise value of
the fine structure constant that we have. How likely is that? Well, there are infinitely
many possible values that agree with our world's fine structure constant to within a thousand
significant figures. Unless our fine structure constant turns out to be some very special distinguished
value (for a while, some physicists thought it was exactly $1/137$??refs, but later measurements 
disproved that, and a recent estimate is $1/137.03599921$), the chances of getting the exact value
randomly we have is zero or at best infinitesimal. Given the fact that consciousness has great
value significance (??shvalue??), if consciousness depends on brains, and hence on electrons, then
the fact of consciousness would loudly cry out for explanation. 

The line of thought above is akin to fine-tuning arguments, where narrow ranges of fundamental constants
are claimed to be needed for life, and call out for explanation, with two options being typically offered:
a multiverse (unlikely things will happen if dice are rolled enough times) and an intelligent designer. But there are
some relevant differences in our present case. 

First, our range is much narrower---only one exact
value is compatible with consciousness on the hypothesis we are exploring---which means that objections from
the rescaling of ranges do not apply as they do in the case of the fine-tuning argument.??coarse-stuff 

Second, plausibly an intelligent designer
would be conscious, and if consciousness requires brains as we are hypothesizing, a designer will be of no help
here, on pain of circularity. 

Third, because the consciousness-permitting range has only one point on it, 
and there are uncountably infinitely many possible other values of the fine structure constant, hitting this value
will not automatically be probable even given a multiverse. If you spin a continuous fair spinner once, 
your chance of hitting a particular value is zero or infinitesimal. But the same is true for any finite number of independent spins.
Moreover, in classical probability theory, this is also true for a countably infinite number of spins. And for an
uncountably infinite number of spins, the probability is simply undefined. In light of this, the multiverse hypothesis only
really solves the problem of consciousness in our context if it is a Lewisian or Tegmarkian hypothesis that \textit{every} 
possible cosmic arrangement is realized in reality. But such a hypothesis only solves the problem at the expense of introducing serious
sceptical problems, since there will be cosmoses, just as real as ours, where every coherent sceptical hypothesis hold, and it does
not appear reasonable to think that we got so lucky as to escape them all.??refs

Tying consciousness to brains thus links consciousness to the precise laws of nature we have. That is not only intuitively implausible
but leads to serious problems. We should think that there is some flexibility in what kinds of bodies conscious beings can have.

Perhaps instead of supposing that consciousness is tied to brains, we could suppose that consciousness is tied to a range of brain-like
organs. Thus, consciousness would be compatible with having somewhat different laws of nature, resulting in fundamental particles
slightly different from the ones we have, and behavior somewhat different from the one we have, but not \textit{very} different.
But now consider the Mersenne questions about the boundaries of physical constitution compatible with consciousness. These
questions cannot be settled by invoking human nature, since they are questions that transcend the nature of any one species.
Nor can they be settled the way Mersenne settled his original questions, by invoking God's creative decision, because we are
supposing that the connection between consciousness and brain-like organs is necessary. We should avoid Mersenne questions that
do not seem to have a plausible answer. 

Furthermore, the issue of worlds practically indistinguishable to our instruments but where one has consciousness and the other
does not returns on the range view. Suppose that the upper cut-off for the fine structure constant to be compatible with
consciousness is $1/100$ (recall that our world's fine structure constant is about $1/137$). Then either $1/100$ is the
highest value compatible with consciousness or the lowest value incompatible with consciousness. If it is the highest value
compatible with consciousness, there should be a world $w_c$ with consciousness and fine-structure constant $1/100$ and a 
world $w_z$ that is practically indistinguishable from $w_c$ but where the fine-structure constant is slightly more than 
$1/100$ and hence where there are only zombies. If, on the other hand, $1/100$ is the lowest value incompatible with 
consciousness, then for a value of the fine-structure constant $(1/100)-\e$ for some positive $\e$ less than one divided by a 
googolplex there will be a world $w_c$ with consciousness. Then we should expect there to be a possible world $w_z$ with fine-structure constant
$1/100$ that is practically indistinguishable from $w_c$ (a difference of one in a googolplex should not affect anything observable),
but $w_z$ will be a zombie world, since we have assumed that a fine-structure constant of $1/100$ is incompatible with
consciousness. So in either case there will be a world with consciousness and a world with zombies which are physically
indistinguishable to humans.  But it is implausible that consciousness should depend on physical features that are so insignificant.

This line of thought pushes one to a very liberal view about what kinds of physical constitutions are compatible with consciousness.
It does not appear, in particular, that consciousness should depend on having a physical constitution that includes brains or anything
similar to brains. We thus have very significant multiple realizability.??check-mr-book

\section{Functionalism}
\subsection{Introduction}
Full-blown dualism, of course, yields significant multiple realizability. Indeed, a minded being's body could be an oak tree or even a 
rock, as long as it had the right kind of non-physical mind on dualism. We will discuss the interaction of dualism with Aristotelian
forms in Section~\ref{sec:dualism}. In the meanwhile, however, let us continue to consider broadly naturalistic accounts of mind.

We have seen that there is good reason to be very liberal about the type of physical aspect that a minded thing can have.  But 
if we are to remain in a broadly naturalistic theory, we need to put some limits on the kinds of physical constitutions that
minds can be based on. We saw earlier that limits based on particular natural kinds---DNA, brains, electrons, etc.---are highly
implausible. The most plausible remaining option is functionalism: to have a mind is to have a certain kind of functional structure,
so that, necessarily, if there is a functional isomorphism between two entities with their respective functionally-specifiable 
causal histories, if one of these entities has a mind, so does the other. Moreover, the isomorphism between causal histories implies
a significant degree of identity between the mental histories. 

On what we may call strong functionalism, their purely internal mental histories
will be the same, and in particular they will have qualitatively the same states in their histories---whenever one felt hot, so did
the other, and whenever one had a perception as of red, so did the other. The restriction to purely internal mental histories allows
for some externalism. Thus, an individual on Earth may be thinking about water, while the analogous thought in an isomorphic individual
on Twin Earth, may be thinking about XYZ, where XYZ fulfills the same causal role on Twin Earth as H$_2$O does on Earth. 

On weak functionalism, the non-qualitative purely internal mental histories will be the same, and whenever one has a conscious state, 
the other has an analogous conscious state, but the exact qualitative phenomenal character of the conscious states may depend on 
the precise physical substrate underlying the two conscious states. On weak functionalism, a silicon-based isomorph of a human being,
will have some sensation in a functional state isomorphic to a human's eating sugar, but that sensation's qualitative character may
be different from the taste of sweet. 

I will now argue that functionalism, whether weak or strong, has serious problems which can be solved by combining it with an Aristotelian hylomorphism.\footnote{The arguments
based on the possibility of malfunction will be based on the ones in ??ref:Koons-Pruss.} 

\subsection{Interpretation}
Begin with the well-known observation that simple causal systems, like the electrons buzzing inside a rock, can be re-interpreted as 
emulating the functioning of our brains, simply because they have such a vast number of states.??refs If this is right, then functionalism 
appears to lead to the absurd thesis that rocks not only think, but think like we do. One version of this argument will be given in ??forward:appendix. 

One might try to get out of this difficulty by insisting that gerrymandered functional systems do not count: only simple causal
systems count as implementing the functions. However, it is very likely that complex evolved brains like ours do have some significantly
gerrymandered states. One might try to draw a distinction between more and less gerrymandered systems, however. The functional states 
that need to be attributed to a rock to re-interpret it as thinking our thoughts are doubtless many orders of magnitude more gerrymandered
than our functional states. But now we have a nasty Mersenne question again: what makes it be the case that the transition between the degrees
of gerrymandering compatible with having a mind those incompatible with having a mind lies where it does?

\subsection{Reliability}
Next, consider the question of the reliability of functional systems. Whether our universe is deterministic or not, functional systems are imperfectly reliable.
How reliable do they need to be to count as the functional systems they are? Consider a subsystem that given two inputs representing numbers
puts out their sum 99.9\% of the time, and 0.1\% of the time puts out the product. Obviously, it is more reasonable to interpret it as 
a reliable addition system than an extremely unreliable multiplication system. But suppose the system puts out sums 50.1\% of the time
and products 49.9\% of the time. What then? 

There are two natural cut-offs. We could require that a system is defined by how it behaves 100\% of the time or by how it behaves
more than half of the time. Requiring perfect functioning would have the empirically false consequence that humans don't think. 
A 50\% cut-off, however, may be problematically low. If every subsystem of a complex functional system had a 49\% failure rate, then 
the typical outputs of the system would be largely random, because any output of the system is the result of a causal chain of many 
subsystem states, and any such chain would likely contain multiple failures. Moreover, while 50\% reliability seems to be a natural
and well-defined cut-off, the actual reliability of a system cannot be captured by a single number, if only because reliability
depends on environmental conditions. If we say that a system is an adder provided that the output is the sum of the inputs more
than half of the time, we have to specify the temperature, background radiation, and other conditions under which that reliability
is to be defined. And now we lose the neat elegance of specifying the reliability with the single number $1/2$.

We might try to define the reliability with respect to the actual environmental conditions the system was in. But suppose that
Alice finds herself in extremely harmful conditions---say, great heat or toxic fumes---but by a fluke survives with what is intuitively
full brain function for a few seconds longer than we would expect, screaming seemingly in pain. Given that under those extreme conditions
the functioning is extremely unreliable, we would have to say that Alice is in fact a mindless zombie and feels no pain. This is
implausible. 

Additionally, note that the reliability of a system varies over time, perhaps increasing over an initial burn-in period,
and then eventually decreasing. We can then define the reliability of a system instantaneously or via a time-average.
If we proceed via a time-average, then we will have the highly counterintuitive consequence that an individual who died
at a hundred was actually conscious through their life, but had they lived a decade longer, they would \textit{never even have been} 
conscious, because some crucial subsystem's reliability average over a 110-year lifespan would have been below the cut-off, but over
the hundred-year lifespan was above the cut-off. So we should define the reliability instantaneously. 

In any case, the very idea of a sharp cut-off in reliability for mindedness seems counterintuitive. Imagine two humanoids whose 
brains are nearly identical, with the exception that one brain structure crucial for consciousness in one of the humanoids has 
50.000001\% reliability and the other has merely 49.999999\% reliability. Suppose that both brains \textit{in fact}
function exactly the same way, so that the sequences of internal states are exactly the same---it's just that one is slightly
more likely to fail than the other. It does not seem very plausible to think that one would be conscious and the other not.

Finally observe that a functional system can retain its function even when highly defective and unreliable. A car that starts only 
on one of three mornings is still a car. 

\subsection{Many functions}
It is not enough that a subsystem always outputs the sum of its inputs for it to be an adder. After all, if the only inputs ever
given are pairs of zeroes, then the subsystem could just as well be a multiplier. As is well-known from Wittgenstein and Kripke??refs,
no finite amount of data is sufficient to determine the function of the system, since any finite collection of inputs and outputs is
consistent with infinitely many possible functions---admittedly, perhaps messy ones.

We might try to define the function of a system or subsystem in terms of counterfactuals. Perhaps an adder is something that \textit{would}
output the sum for all inputs in the range of allowable inputs. However, Frankfurt's counterexamples to the Principle of Alternate
Possibilities??ref can be adapted to show that this is untenable. Imagine that Bob counts as thinking that $5+7=12$ in virtue of the 
fact that his thought involves the operation of an adding subsystem in his brain. But suppose that a neuroscientist has placed a neural
scanner and bomb in Bob's vicinity in such a way that if the scanner detects the adding subsystem getting any input other than 
$5$ and $7$, the bomb blows up Bob. Now counterfactuals like ``If the inputs were $4$ and $3$, the output would be $7$'' are false.
If the inputs were $4$ and $7$, there would be no output, just an explosion.

One might try to define functions by asking what the output would be given the inputs \textit{absent external interference}. 
But what counts as interference with a system, as opposed to, say, a helpful or neutral effect, depends precisely on the system's
function. If the purpose of a wood-pulp product is to preserve inscribed information, then fire is an external interference;
but if the wood-pulp product's function is to be kindling, then fire activates its the object's function. 
Furthermore, one can internalize Frankfurt-like cases. Imagine that through science-fictional genetic modification Bob's liver
comes to behave just like the scanner-and-bomb system. 

\subsection{A neo-Aristotelian solution}
If functionalists do not have the resources to define functionalism, then this is presumably the most fundamental possible flaw
in functionalism. However, the problem of defining functions is exactly one of the one that Aristotelian forms are designed to solve.  The proper function
of a subsystem in an organism is that the fulfillment of which constitutes the organism's flourishing with respect to that subsystem.
This does not require the subsystem to be reliable, though Aristotelian optimism predicts that most systems will be reliable. 

A robust view of organisms as having forms that specify normative, and hence functional, features thus solves a central
problem with functionalism. And as we saw, there is very good reason for a naturalist to adopt functionalism---it is the
best naturalist option for saving multiple realizability. Thus, we have an argument from naturalism to Aristotelianism.
Of course, whether Aristotelianism is compatible with naturalism is not clear. If naturalism is understood to say that 
there are no fundamental normative properties, then of course there is no compatibility.

???refs

\section{Supervaluationism about minds}
A tempting solution to the problem of the multiplicity of functionalist (or other) theories of mind differing with respect
to fine details is to treat each theory as a precisification of concepts like \textit{mind} or \textit{pain}, none of which
is privileged over the others. And,
if all has gone well, then typical adult humans fall under all the precisifications of ``has a mind'' and in paradigmatic
cases of being in pain they fall under all the precisifications of ``is in pain''. 

One difficulty with this approach has already been discussed in the ethical?? context??backref, namely that similar problems
arise at the meta level: What \textit{range} of, say, standards of reliability yields a functionalist concept that is in fact a 
precification of our concept of mind? 

Furthermore, recall the argument??backref that the centrality and overridingness of ethical norms to our lives makes it 
deeply implausible to think that there is a plurality of closely related concepts, none of which is privileged over the others.
But given the deep importance of the mental to ethics, the same concern applies to the mental. That an action causes severe
pain to a non-consenting individual with no significant benefit is a conclusive moral reason not to perform the action. 
But if there are many concepts of pain with none of them privileged, and similarly of consent, then whether one has a
conclusive moral reason for an action will depend on the choice of precisification as well.

???

\section{Dualism}\label{sec:dualism}
The neo-Aristotelian teleological twist on functionalism allows one to remain to a significant degree a naturalist (more on that
in ??forward). The account reduces mental properties to functional properties, but the functional properties are not 
reducible to the kinds of properties that physics studies. A more traditional Aristotelian approach, however, is to refuse
to reduce the reduction of mental properties to non-mental ones. This is certainly also compatible with the robust view of
human nature that has been defended in this book. 

Whether the more dualist or the more functionalist view is more plausible depends on how satisfying the reduction of mental
properties to teleologically laden functionalist properties is, and one way to evaluate this reduction is to consider
whether the arguments against the reduction of mental to physical properties apply to this neo-Aristotelian functionalism.
These arguments can be divided into three categories, depending on which aspect of mental life they object to the reduction
of: content, consciousness, or freedom of will. 

As has been noted??backref, the neo-Aristotelian teleology offers hope for a reductive account of mental content. Teleological properties
can be hyperintensional---it is the purpose of eyes to see rather than to see or be such that $2+2=5$, even though necessarily
everything that sees is such that it sees or is such that $2+2=5$. Thus concerns that there is no way to reach the hyperintensionality
of mental content from the extensional or at best intensional content of the physical world do not transfer to the neo-Aristotelian
account. Similarly, concerns about the need to account for purpose in the mind---beliefs are states that are there 
\textit{in order to} mirror the world---and that the only source of purpose for the physicalist, namely evolution, is inadequate
do not apply to the neo-Aristotelian theory. Thus, content-based arguments do not tell against a neo-Aristotelian functionalism.

The case of consciousness is less clear. We can try to run standard knowledge arguments transposed to the neo-Aristotelian context.
Suppose Mary is raised in a black and white environment, and knows all the physical facts as well as all the normative facts about
human beings. In particular, she knows that certain states of the human being are such that they should occur precisely when the
human being is looking at a red object, and that these states typically occur when people see a ripe tomato. She, further, knows all
the normative interconnections between these states and other states, including all the inferential connections. Will she learn anything
further by seeing the tomato? Here intuitions may differ. It is at least easier to hold out for a negative answer to the learning question
here than in the case where Mary has a purely physical state.

Similarly, we can try for imaginability arguments. These come in two versions: zombie arguments that one can have our physical
constitution without consciousness and afterlife arguments that our consciousness could survive the destruction of the body.
On these arguments, the imaginability of a scenario provides defeasible evidence of its metaphysical possibility. 

Could there be beings that have isomorphic physical and normative properties to ours
but that are unconscious zombies? Again, this is not completely clear. Among our normative properties are moral duties. Could one
have something isomorphic to moral duties without consciousness? If not, then zombie arguments against neo-Aristotelian functionalism 
do not work as they stand. 

Alternately, what can the neo-Aristotelian functionalist say about surviving the destruction of the body? Those ``fainthearted'' 
neo-Aristotelians who hold that the form is nothing but a kind of arrangement of matter, of course, will find it troubling to
suppose the form to survive the destruction of the matter. Though even there, there is a potential precedent for a view
that would allow the form to survive. Aquinas's account of transsubstantiation holds that in the Eucharist, the accidents of
bread and wine continue to exist after the cessation of the existence of the substance. Among the accidents there will be 
\textit{shape}. If a shape can exist without that of which it is the shape, then why not an arrangement as well? Similarly,
some contemporary trope theories hold that there are ``unaffiliated'' tropes, tropes that exist without their substance. If a 
trope can exist without a substance, why can't an arrangement exist without matter? 

But of course the view defended in the book is more robust: forms are not mere arrangements. The more reality the form itself
has, the more plausible that there is no metaphysical impossibility about the form existing without the matter. 

One may, however, wonder whether \textit{we} could continue to exist without a body. ???????

Finally, we have freedom of will. Those convinced that an action that comes solely from the causal powers posited by a completed
physics??ref would not be free will not be moved by being told that these causal powers have a normative organization. Such 

??Doesn't functionalism have the same problems?

??vagueness of consciousness

\section{Teleology and representation}
\section{Teleology and mental causation}
%teleosemantics??
\section{Composition}
??Tomaszewski

\section{Identity over time}
One of the classic questions of metaphysics is about the grounds of identity over time. 
A very general way of posing the question is to ask for an explanation or ground of claims
of the form:
\ditem{identity-ground}{Object $x_1$ is identical to object $x_2$,}
where $x_1$ and $x_2$ respectively exist at times $t_1$ and $t_2$, which are presupposed to be different\footnote{One may worry that this
presupposition cannot be stated without using identity, i.e., without denying that
$t_1=t_2$. However, it is not clear that even if this is true, it affects the
significance of the question. Moreover, if time turns out to be linearly ordered,
then we can state the presupposition disjunctively: $t_1$ is earlier than $t_2$ 
or $t_2$ is earlier than $t_1$.} and where we require the explanation not to
involve identity and to involve only purely qualitative properties and 
relations.\footnote{If we allow the account to involve non-qualitative properties 
like Socrateity (the property of being Socrates), then we can offer a infinite 
account: $x_1$ is identical to $x_2$ provided that any property had by $x_1$ is 
had by $x_2$.}

Put in that very general way, it seems unlikely that we will have a solution, 
absent some extremely controversial metaphysical assumptions, such as 
Leibniz's Principle of Identity of Indiscernibles (PII).\footnote{The PII says
that two things are identical just in case they have the same purely qualitative
properties. If the PII holds, then we can say that $x_1=x_2$ if and only if 
for every purely qualitative property we have $Q(x_1)$ iff $Q(x_2)$.} 

But there is a somewhat less general way of putting the diachronic identity question.
Suppose that at time $t_1$, some proper plurality of items, the $x$s, compose an 
object and at time $t_2$ the $y$s compose an object. Then we ask for the grounds of:
\ditem{identity-comp-ground}{An object composed of the $x$s at $t_1$ is identical 
    with an object composed of the $y$s at $t_2$.\footnote{The reason for the
    indefinite pronoun is that, first, there might be more than one object composed
    of the same parts and, second, using the definite pronoun introduces another 
    instance of the identity relation given the Russellian analysis of ``The $F$ is
    $G$'' as saying that some $F$ is $G$ and has the property of being
    \textit{identical} with every $F$ that is $G$, whereas we only want an account
    of a single identity relation.}}
This is not asking for an account of diachronic identity in general, but of diachronic
identity of complex objects (note that we only require the $x$s to be a proper
plurality, i.e., for there to be more than one of them).
    
Here we \textit{can} give an Aristotelian account:
\ditem{identity-Arist}{There is a form $F$ such that at $t_1$, $F$ unites the $x$s,
    and at $t_2$, $F$ unites the $y$s.}
Here we might stipulate that a form $F$ unites the $z$s just in case $F$ is one of 
the $z$s, $F$ informs each one of the $z$s other than $F$, and anything informed 
by $F$ overlaps at least one of the $z$s.

\section{Teleological animalism and cerebra}
Each premise of the following argument is very plausible.
\ditem{human}{We are humans.} 
\ditem{human-mammal}{Humans are mammals.} 
\ditem{mammal}{So, we are mammals.}
\ditem{mammal-animal}{Mammals are animals.}
\ditem{animalism}{So, we are animals.}

Nonetheless, the conclusion---labaled as ``animalism''---is denied by many philosophers. One traditional path to 
this denial is a Cartesian dualism on which we are immaterial souls that inhabit human animals. The other path 
is modern colocationist views on which we are a special kind of material object---a person---constituted by a human 
animal. The third path is brain views on which we are brains, or parts of brains (namely cerebra), which in turn are a 
proper part of a human animal. 

On all three paths, one will want to deny \dref{mammal}. Perhaps the best way to do so would be to distinguish ``humans'' 
in \dref{human} and \dref{human-mammal} into  human animals and human persons, and insist that premise \dref{human} is true 
only of human persons, while \dref{human} is true only of human animals. Thus the argument is unsound if ``humans'' is used 
consistently and otherwise invalid. 

In any case, the intermediate step \dref{mammal} gets denied. Instead, we are 
\textit{associated} with mammals, by ensoulment, constitution or parthood.   Yet \dref{mammal} is by itself extremely plausible. 
To deny that we are mammals seems akin to denying that earth
is round. Further, both the Cartesian and brain views imply that we
rarely if ever see or touch another person. One needs extremely good arguments for such counterintuitive theses. 

Probably the main candidate here is a family of arguments about the difficulties of accounting for cerebrum transplants
on animalism.??refs The simplest version is that if your cerebrum is removed from your skull and placed in a vat in such a way
that it can continue functioning, then intuitively you continue to think and come along with the cerebrum. But a cerebrum
is not an animal. On the contrary, the cerebrumless body appears to be an animal. After all, some animals (e.g., fish??) lack
cerebra, and it seems that the destruction of a cerebrum in an animal that normally has one would result in the animal 
becoming severely disabled rather than ceasing to exist. Thus, animalism points to the cerebrumless body as you---the cerebrumless
body is the same animal as you---and the cerebrum in the vat as something  or someone else, contrary to our intuitions.

Cartesians, colocationists and brain theorists who identify with the cerebrum have no such problem. They can all say that
the cerebrumless body is not you, and instead you inhabit or are colocated with or are just plain identical with the cerebrum
in the vat. 

Here I want to argue that an Aristotelian about humans can embrace a teleological variety of animalism on which it is natural to say that 
we go along with our cerebra. Now animalism is highly plausible as 
a view of the human person that does justice to the intuitions that humans are mammals, weigh about 80~kg and can be seen 
without surgery, but faces a serious cerebrum transplant problem. If Aristotelianism can help animalists overcome that problem,
that is some further evidence for Aristotelianism about humans.

There are two teleological features in the animalism I will sketch. The first teleological feature is the thesis 
that what defines somethingas an animal (and indeed an animal of a particular type) is its teleology. While it is usual to 
think of animals as things that nourish themselves, grow, reproduce, and have a certain level of autonomy from the enviroment, 
the teleological animalist instead insists that animals need not engage in these activities, but need only have a teleological
orientation towards them, need to be the sorts of things that \textit{should} engage in these activities. 

The second teleological feature is to see organisms, including humans, as having a teleological \textit{hierarchy}, with some
\tele{} subordinated to others. Sometimes the subordination is instrumental: our teeth rend food in order to nourish us. 
But there can also be a value-based subordination, where an activity, while not merely instrumental towards another activity, 
is less central to the flourishing of the organism and to the organism's identity. On the teleological animalism I am sketching,
it is postulated that in humans, activities common to all animals are subordinated to specifically personal activities, namely 
rational and moral behavior. ???can we have this in non-theistic versions??? wouldn't we have subordination to reproductive
activity???

When there is a hierarchical subordination, it is plausible to think that in cases of splitting, an organism is more apt
to come along with the organs supporting the higher-level features. If in humans it is moral and intellectual capacity 
that is at the top of the teoleological hierarchy, and the cerebrum is much more directly supportive of these capacities
than the lower brain, heart, lungs, etc., then we would expect the human organism to come along with the cerebrum. If you
cut a worm in such a way that one end contains just the head and the other contains the rest of the body, and both parts
behave as if they were alive, it is reasonable to suppose that the form of the original individual may go with the larger
piece which contains more in the way of life-supporting organs, rather than with the head, because the head is less 
teleologically central to the worm than to us. But given human teleology, we would expect us to go along with the head,
if the head were given life support, or even with the brain or cerebrum.

What about the remaining cerebrumless human body, which maintains its vital functions? If the original individual goes along
with the cerebrum, and if we take the idea of one human form in two bodies to be absurd, there are three possibilities 
for the cerebrumless body:
\begin{itemize}
\item[(a)] the cerebrumless body gains a new human form, or 
\item[(b)] it gains some non-human form, or 
\item[(c)] it becomes a non-living substance or a formless heap of matter. 
\end{itemize}

If the cerebrumless body gains a new human form, then we have the counterintuitive consequence that a temporary removal
of a cerebrum followed by reimplantation would either result in conjoined twins---two human beings joined at the edges
of the cerebrum---or would result in the death of one or more of the two human beings. None of these options seems very
plausible, but at the same time, we should not be too surprised if strange things happen when you move cerebra around!

If the cerebrumless body gains a non-human form, this is presumably an organismic form, since the entity is capable of
nourishment, reproduction, etc. But we have something moderately puzzling: a non-human organism
that would produce a human being if it were to mate with a human or with another organism of the same sort but of the
opposite sex. Moreover, it seems plausible that if the cerebrumless body has a teleology at all, that teleology impels
it to try to support the cerebrum: oxygen would be directed by the body towards the missing cerebrum, presumably. This
suggests that the being is incomplete without the cerebrum. But a being that ought to have a human cerebrum seems to be 
a human being. 

The last option is a formless heap of matter or a non-living substance (such as a body of water might be on some 
Aristotelian theories??refs). This does not 
seem particularly puzzling in the transfer case. 
If the form departs, by going along with the cerebrum, then formlessness would seem to be the obviously expected result, 
barring some special reason to the contrary.

What if instead of the cerebrum being removed and put on life-support, the cerebrum is simply destroyed? This corresponds
to a tragic real-life scenario: upper brain death. Bioethicists disagree on whether upper brain death is death.??refs
We have four moderately plausible options for cerebral destruction:
\begin{itemize}
\item[(i)] the original individual continues to live, i.e., the cerebrumless body retains the original human form??backref-to-identity, or
\item[(ii)] the original individual dies and the cerebrumless body gains a new human form, or
\item[(iii)] the original individual dies and the cerebrumless body gains a non-human form, or
\item[(iv)] the original individual dies and the cerebrumless body has no organismic form, and is either a non-living substance or a heap.
\end{itemize}

Between (a)--(c) and (i)--(iv), there are twelve combinations with various intuitive connections between them. 
However, we can intuitively reduce the number of options quite significantly. I have assumed that in the transfer
case, the original form goes along with the cerebrum in the transfer case, departing from the cerebrum. Suppose that
(i) is false. Then the original individual dies and the cerebrumless body is deprived of its original form. It seems
very plausible that what happens to the cerebrumless body upon deprivation of its original human form should not depend
on what that form does after departing the cerebrumless body---whether it continues to inform a reduced body (the cerebrum), 
or survives disembodied as many religious people think, or perishes. Thus, if (i) is false, then (a), (b) or (c) holds
respectively if and only if (ii), (iii) or (iv) holds. 

We thus have three plausible options with (i) false:
\begin{itemize}
\item[($\alpha$)] (a) and (ii)
\item[($\beta$)] (b) and (iii)
\item[($\gamma$)] (c) and (iv)
\end{itemize}

What if (i) is true, so that in the case of cerebral destruction, the original form continues to inform the cerebrumless body? 
Does this  tell us anything about what happens in the transfer case? One might initially think that the falsity of (i) fits
poorly with any of (a), (b) and (c). After all, if destruction of the cerebrum results in the form staying with the cerebrumless
body, shouldn't we expect the form remain with the cerebrumless body when the cerebrum is transfered?

But this is not clear on teleological animalism. For we might think that in division or partial destruction of the body, the form 
goes along with the part that is the best candidate for being informed by it, at least when there is a unique best candidate and that best 
candidate is ``good enough''. On the teleological account, the quality of candidacy for being informed is measured by how high in the teleological hierarchy are
the goals directly promoted by the part. If the part promotes goals at the top of the hierarchy, then the candidate is automatically
good enough. Thus, when the cerebrum survives, it is reasonable to think the form goes along with the cerebrum, because the cerebrum
is good enough as a candidate, and higher in the hierarchy than the cerebrumless body. But if the cerebrum is destroyed, the cerebrumless
body is the unique best candidate, because it is the only candidate. Whether the cerebrumless body is good enough as a candidate is not clear, but neither is it
clear that it is not good enough. It is, after all, on the next step down in the hierarchy after the cerebrum, having among its tasks the full
support of the cerebrum's functioning, as well as many important purely animal functions. Thus, all of the following are at least somewhat reasonable 
epistemic possibilities:
\begin{itemize}
\item[($\delta$)] (a) and (i)
\item[($\e$)] (b) and (i)
\item[($\zeta$)] (c) and (i)
\end{itemize}

On teleological animalism we thus have six combinations for making sense of what happens to the form and cerebrum, namely ($\alpha$)--($\zeta$), 
and none of them appear immensely problematic, though (a) and (b) have some moderately counterintuitive consequences. But even if we feel
the need to reject these, that still leaves us with ($\gamma$) and ($\zeta$): in cerebral transfer cases, the cerebrumless body is not a living
thing, while in cerebral destruction cases, the cerebrumless body may (if we have (i)) or may not (if we have (iv)) continue to be a living
human being. 

One may think ($\zeta$) is implausible, because it implies that whether the body-minus-cerebrum continues to have the original human
form depends on what happens to the cerebrum---whether it is merely removed or actually destroyed. But such dependence is, as already
noted, to be reasonably expected. For if we think of the cerebrum as a magnet for the original human form, then transfer of that ``magnet''
might be reasonably be thought to pull the form along, hence depriving the cerebrumless body of it, while destruction of the ``magnet'' might
well leave the form in place. 

Teleological animalism, thus, has multiple ways of making sense of what happens in cerebral transfer and destruction cases, while these cases
are highly problematic to other types of animalism. Given the plausibility of animalism as such, this gives us another reason to accept
teleological animalism.


??conjoined twins--metaphysics?


\section{Soul and body ethics}
\section*{Appendix: Functionalism gone too far}
Consider a deterministic functional system $Q$ consisting of a 
finite number of possible computational subsystems $S_1,...,S_N$, where $S_k$ is always in exactly one state from the finite set $\scr A_k$
of possible states at each of the discrete ``significant'' moments of time $t_1,...,t_m$ over its finite lifetime\footnote{A standard modern digital computer only has defined 
computational states at ticks of its internal clock. Between ticks of the internal clocks, it is in an analogue state that is not
computationally defined. A lot of careful engineering goes into ensuring that the states become properly ``digital'' at the clock ticks.}, and 
a finite number $I_1,...,I_M$ of sensors, where $I_k$ is always in exactly one state from the finite set $\scr I_k$ of possible input
states. 

We can think of $Q$ as a finite digital computer. The total state of the system at any given time can be represented as the
$(N+M)$-tuple $(a_1,...,a_N,b_1,...,b_M)$ where $a_k$ is a state in $\scr A_k$ and $b_k$ is a state in $\scr I_k$. We can designate some of
the subsystems as outputs, connected to external effectors (muscles, motors, lights, etc.)  Furthermore, we suppose there are functional laws which provide a 
mapping $f$ from the state of the system at time $t_i$ to the state at time $t_{i+1}$. Thus, $f(a_1,...,a_N,b_1,...,B_M)= (a_1,...,a_N,b_1,...,b_M)$ 
provided that the system would transition from state $(a_1,...,a_N)$ to state $(b_1,...,b_N)$.\footnote{The mapping is independent of time. But we 
can always suppose there is a finite clock, e.g., given by a subsystem $S_k$ such that the set of states $\scr A_k$ is
the set of times during the system's lifetime, and with the transition rule that the time always gets incremented. Or we can suppose an input
from an external clock.} Presumably any deterministic analog system can be approximated by such a system $Q$ to aribitrarily high precision.
We suppose for convenience that there is some fixed initial state of $Q$, with fixed initial input and computational states.

Now consider a different system $P$ consisting of a single particle moving in the $xy$-plane in space, with a constant 
(sublight) non-zero $x$-velocity $v$, starting at $x$-coordinate $0$ at time $t_1$. For ease of visualization, suppose the $x$-axis
runs left to right, and the $y$-axis runs down to up.
Let $\scr J$ be the set of all possible
single-time input state vectors $(b_1,...,b_M)$ where $b_k \in\scr I_k$ for each $k$, and let $K$ be a positive integer such
that $\scr J$ has at most $10^K$ members. 

We now recode $Q$'s inputs into $P$'s inputs as follows. Suppose our particle is at $y$-coordinate $0$ at a time $t_0<t_1$. 
For $1\le n\le N$, let $\psi_n$ be a one-to-one function from $\scr J$ to integers between $0$ and $10^K-1$, both inclusive. The analogue 
to inputting the sensor state vector $(b_1,...,b_M)$ into $Q$ at significant time $t_n$ will now be this. We take
the $y$-coordinate value $y_{n-1}$ at $t_{n-1}$, and add to it the value $10^{-Kn} \psi_n(b_1,...,b_M)$, shifting the particle
upward along the $y$-axis as needed to ensure it reaches that by time $t_n$.\footnote{We may need to ensure the units in which
these are measured are such that the particle can be shifted in the requisite time without exceeding the speed of light.} Thus, 
the first set of inputs of $Q$ will be encoded in the first $K$ digits after the decimal point, the second set of inputs
will be encoded in the second $K$ digits, and so on.

Next, suppose $Q$'s computational states begin with the fixed initial state vector $(a_{1,1},...,a_{1,N})$ at time $t_1$. Given 
the determinism of the system, there is a mathematical function $f$ that takes a sequence $s$ of length $n$ of members of $\scr I$ 
and returns the computational state $f(s)$ that $Q$ would be in at time $t_n$ if it were to have received the sequence of inputs $s$ 
at the times $t_1,...,t_n$. Let $n(x)$ be the integer $n$ such that $x=v t_n$, if there such an integer, where we recall that
$v$ is the velocity of our particle along the $x$-axis. Let $s(x,y)$ be the sequence of $n(x)$ sensor state vectors encoded 
by the $y$-coordinate value $y$, assuming $n(x)$ is defined. Thus, the first member of $s(x,y)$ will be the sensor state vector
$s_1$ such that $\psi_1(s_1)$ is equal to the decimal number given by the first $K$ digits after the decimal point in $y$, and so on.
We now deem $P$ to be in a computational state corresponding to $f(s(x,y))$ when $P$ is at $(x,y)$. This is defined at all the
significant times $t_1,...,t_n$. We have, thus, an isomorphism between the computational states and sensor inputs of $Q$ and $P$.

We now go for one final twist. Suppose that in the actual world, the
system $Q$ gets the sensor input vectors $s_1,...,s_N$ at times $t_1,...,t_N$, respectively. We can now choose the 
function $\psi_n$ such that $\psi_n(s_n)=0$ for all $i$. Then a single particle moving with constant velocity 
$v$ along the $x$-axis, with $y$-coordinate always equal to $0$ is an isomorph to the actual functioning of $Q$.
But the very same particle will be an isomorph of any other system $Q'$ with the same significant times. Thus,
the particle will think your thoughts \textit{and} my thoughts!

??do we need a clock?

\chaptertail 


