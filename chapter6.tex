\def\mychapter{VI}
\input{chapterhead}
\chapter{Mind}\label{ch:mind}
\section{Multiple realizability}
Some conscious beings have brains. Start with the hypothesis that it is a necessary 
truth that all conscious beings have brains. 

First, this hypothesis is just implausible: it seems quite plausible that we could have
conscious beings with a very different body plans. 

Second, observe that brains are a specific type of organ in DNA-based animals.
To have a brain, thus, you need to have DNA. To have DNA, you need to have hydrogen
atoms. To have hydrogen atoms, you need to have electrons. A particle with a different 
electric charge would not be an electron, and the charge of the electron is definable in terms of 
the fine structure constant $e^2/(2\varepsilon_0 hc)$. If the fine structure constant
were different, we wouldn't have electrons. We might have shmelectrons that behave almost 
exactly like electrons, but they wouldn't be electrons. If we didn't have electrons, we wouldn't
have hydrogen, but at best shmydrogen. And if we didn't have hydrogen, we wouldn't have DNA,
but at best shmDNA. 

But now imagine a world extremely so similar to ours that no instruments of a sort
humans ever have a hope of constructing could ever tell the difference, but where, nonetheless,
the fine structure constant has a slightly different value. In that world we have beings that
behave, as far as any of us could ever tell by external and internal examination, just as we 
do. But they not only would \textit{be} unconscious zombies, they would \textit{have to be}
zombies---no beings with shmelectrons in place of electrons could be conscious on the hypothesis 
we are considering. That such a slight difference in physical constitution would make the
difference is extremely implausible. 

Third, if the hypothesis is true, we should be quite surprised at the existence
of consciousness. The argument just given shows that consciousness requires the precise value of
the fine structure constant that we have. How likely is that? Well, there are infinitely
many possible values that agree with our world's fine structure constant to within a thousand
significant figures. Unless our fine structure constant turns out to be some very special distinguished
value (for a while, some physicists thought it was exactly $1/137$??refs, but later measurements 
disproved that, and a recent estimate is $1/137.03599921$), the chances of getting the exact value
randomly we have is zero or at best infinitesimal. Given the fact that consciousness has great
value significance (??shvalue??), if consciousness depends on brains, and hence on electrons, then
the fact of consciousness would loudly cry out for explanation. 

The line of thought above is akin to fine-tuning arguments, where narrow ranges of fundamental constants
are claimed to be needed for life, and call out for explanation, with two options being typically offered:
a multiverse (unlikely things will happen if dice are rolled enough times) and an intelligent designer. But there are
some relevant differences in our present case. 

First, our range is much narrower---only one exact
value is compatible with consciousness on the hypothesis we are exploring---which means that objections from
the rescaling of ranges do not apply as they do in the case of the fine-tuning argument.??coarse-stuff 

Second, plausibly an intelligent designer
would be conscious, and if consciousness requires brains as we are hypothesizing, a designer will be of no help
here, on pain of circularity. 

Third, because the consciousness-permitting range has only one point on it, 
and there are uncountably infinitely many possible other values of the fine structure constant, hitting this value
will not automatically be probable even given a multiverse. If you spin a continuous fair spinner once, 
your chance of hitting a particular value is zero or infinitesimal. But the same is true for any finite number of independent spins.
Moreover, in classical probability theory, this is also true for a countably infinite number of spins. And for an
uncountably infinite number of spins, the probability is simply undefined. In light of this, the multiverse hypothesis only
really solves the problem of consciousness in our context if it is a Lewisian or Tegmarkian hypothesis that \textit{every} 
possible cosmic arrangement is realized in reality. But such a hypothesis only solves the problem at the expense of introducing serious
sceptical problems, since there will be cosmoses, just as real as ours, where every coherent sceptical hypothesis hold, and it does
not appear reasonable to think that we got so lucky as to escape them all.??refs

Tying consciousness to brains thus links consciousness to the precise laws of nature we have. That is not only intuitively implausible
but leads to serious problems. We should think that there is some flexibility in what kinds of bodies conscious beings can have.

Perhaps instead of supposing that consciousness is tied to brains, we could suppose that consciousness is tied to a range of brain-like
organs. Thus, consciousness would be compatible with having somewhat different laws of nature, resulting in fundamental particles
slightly different from the ones we have, and behavior somewhat different from the one we have, but not \textit{very} different.
But now consider the Mersenne questions about the boundaries of physical constitution compatible with consciousness. These
questions cannot be settled by invoking human nature, since they are questions that transcend the nature of any one species.
Nor can they be settled the way Mersenne settled his original questions, by invoking God's creative decision, because we are
supposing that the connection between consciousness and brain-like organs is necessary. We should avoid Mersenne questions that
do not seem to have a plausible answer. 

Furthermore, the issue of worlds practically indistinguishable to our instruments but where one has consciousness and the other
does not returns on the range view. Suppose that the upper cut-off for the fine structure constant to be compatible with
consciousness is $1/100$ (recall that our world's fine structure constant is about $1/137$). Then either $1/100$ is the
highest value compatible with consciousness or the lowest value incompatible with consciousness. If it is the highest value
compatible with consciousness, there should be a world $w_c$ with consciousness and fine-structure constant $1/100$ and a 
world $w_z$ that is practically indistinguishable from $w_c$ but where the fine-structure constant is slightly more than 
$1/100$ and hence where there are only zombies. If, on the other hand, $1/100$ is the lowest value incompatible with 
consciousness, then for a value of the fine-structure constant $(1/100)-\e$ for some positive $\e$ less than one divided by a 
googolplex there will be a world $w_c$ with consciousness. Then we should expect there to be a possible world $w_z$ with fine-structure constant
$1/100$ that is practically indistinguishable from $w_c$ (a difference of one in a googolplex should not affect anything observable),
but $w_z$ will be a zombie world, since we have assumed that a fine-structure constant of $1/100$ is incompatible with
consciousness. So in either case there will be a world with consciousness and a world with zombies which are physically
indistinguishable to humans.  But it is implausible that consciousness should depend on physical features that are so insignificant.

This line of thought pushes one to a very liberal view about what kinds of physical constitutions are compatible with consciousness.
It does not appear, in particular, that consciousness should depend on having a physical constitution that includes brains or anything
similar to brains. We thus have very significant multiple realizability.??check-mr-book

\section{Functionalism}
\subsection{Introduction}
Full-blown dualism, of course, yields significant multiple realizability. Indeed, a minded being's body could be an oak tree or even a 
rock, as long as it had the right kind of non-physical mind on dualism. We will discuss the interaction of dualism with Aristotelian
forms in Section~\ref{sec:dualism}. In the meanwhile, however, let us continue to consider broadly naturalistic accounts of mind.

We have seen that there is good reason to be very liberal about the type of physical aspect that a minded thing can have.  But 
if we are to remain in a broadly naturalistic theory, we need to put some limits on the kinds of physical constitutions that
minds can be based on. We saw earlier that limits based on particular natural kinds---DNA, brains, electrons, etc.---are highly
implausible. The most plausible remaining option is functionalism: to have a mind is to have a certain kind of functional structure,
so that, necessarily, if there is a functional isomorphism between two entities with their respective functionally-specifiable 
causal histories, if one of these entities has a mind, so does the other. Moreover, the isomorphism between causal histories implies
a significant degree of identity between the mental histories. 

On what we may call strong functionalism, their purely internal mental histories
will be the same, and in particular they will have qualitatively the same states in their histories---whenever one felt hot, so did
the other, and whenever one had a perception as of red, so did the other. The restriction to purely internal mental histories allows
for some externalism. Thus, an individual on Earth may be thinking about water, while the analogous thought in an isomorphic individual
on Twin Earth, may be thinking about XYZ, where XYZ fulfills the same causal role on Twin Earth as H$_2$O does on Earth. 

On weak functionalism, the non-qualitative purely internal mental histories will be the same, and whenever one has a conscious state, 
the other has an analogous conscious state, but the exact qualitative phenomenal character of the conscious states may depend on 
the precise physical substrate underlying the two conscious states. On weak functionalism, a silicon-based isomorph of a human being,
will have some sensation in a functional state isomorphic to a human's eating sugar, but that sensation's qualitative character may
be different from the taste of sweet. 

I will now argue that functionalism, whether weak or strong, has serious problems which can be solved by combining it with an Aristotelian hylomorphism.\footnote{The arguments
based on the possibility of malfunction will be based on the ones in ??ref:Koons-Pruss.} 

\subsection{Interpretation}
Begin with the well-known observation that simple causal systems, like the electrons buzzing inside a rock, can be re-interpreted as 
emulating the functioning of our brains, simply because they have such a vast number of states.??refs If this is right, then functionalism 
appears to lead to the absurd thesis that rocks not only think, but think like we do. One version of this argument will be given in ??forward:appendix. 

One might try to get out of this difficulty by insisting that gerrymandered functional systems do not count: only simple causal
systems count as implementing the functions. However, it is very likely that complex evolved brains like ours do have some significantly
gerrymandered states. One might try to draw a distinction between more and less gerrymandered systems, however. The functional states 
that need to be attributed to a rock to re-interpret it as thinking our thoughts are doubtless many orders of magnitude more gerrymandered
than our functional states. But now we have a nasty Mersenne question again: what makes it be the case that the transition between the degrees
of gerrymandering compatible with having a mind those incompatible with having a mind lies where it does?

\subsection{Reliability}
Next, consider the question of the reliability of functional systems. Whether our universe is deterministic or not, functional systems are imperfectly reliable.
How reliable do they need to be to count as the functional systems they are? Consider a subsystem that given two inputs representing numbers
puts out their sum 99.9\% of the time, and 0.1\% of the time puts out the product. Obviously, it is more reasonable to interpret it as 
a reliable addition system than an extremely unreliable multiplication system. But suppose the system puts out sums 50.1\% of the time
and products 49.9\% of the time. What then? 

There are two natural cut-offs. We could require that a system is defined by how it behaves 100\% of the time or by how it behaves
more than half of the time. Requiring perfect functioning would have the empirically false consequence that humans don't think. 
A 50\% cut-off, however, may be problematically low. If every subsystem of a complex functional system had a 49\% failure rate, then 
the typical outputs of the system would be largely random, because any output of the system is the result of a causal chain of many 
subsystem states, and any such chain would likely contain multiple failures. Moreover, while 50\% reliability seems to be a natural
and well-defined cut-off, the actual reliability of a system cannot be captured by a single number, if only because reliability
depends on environmental conditions. If we say that a system is an adder provided that the output is the sum of the inputs more
than half of the time, we have to specify the temperature, background radiation, and other conditions under which that reliability
is to be defined. And now we lose the neat elegance of specifying the reliability with the single number $1/2$.

We might try to define the reliability with respect to the actual environmental conditions the system was in. But suppose that
Alice finds herself in extremely harmful conditions---say, great heat or toxic fumes---but by a fluke survives with what is intuitively
full brain function for a few seconds longer than we would expect, screaming seemingly in pain. Given that under those extreme conditions
the functioning is extremely unreliable, we would have to say that Alice is in fact a mindless zombie and feels no pain. This is
implausible. 

Additionally, note that the reliability of a system varies over time, perhaps increasing over an initial burn-in period,
and then eventually decreasing. We can then define the reliability of a system instantaneously or via a time-average.
If we proceed via a time-average, then we will have the highly counterintuitive consequence that an individual who died
at a hundred was actually conscious through their life, but had they lived a decade longer, they would \textit{never even have been} 
conscious, because some crucial subsystem's reliability average over a 110-year lifespan would have been below the cut-off, but over
the hundred-year lifespan was above the cut-off. So we should define the reliability instantaneously. 

In any case, the very idea of a sharp cut-off in reliability for mindedness seems counterintuitive. Imagine two humanoids whose 
brains are nearly identical, with the exception that one brain structure crucial for consciousness in one of the humanoids has 
50.000001\% reliability and the other has merely 49.999999\% reliability. Suppose that both brains \textit{in fact}
function exactly the same way, so that the sequences of internal states are exactly the same---it's just that one is slightly
more likely to fail than the other. It does not seem very plausible to think that one would be conscious and the other not.

Finally observe that a functional system can retain its function even when highly defective and unreliable. A car that starts only 
on one of three mornings is still a car. 

\subsection{Damage}
Plausibly, a functionalist account of (nociceptive) pain will include two essential aspects of its causal role: 
the pain is caused by bodily damage and the pain causes a tendency to aversive behavior.\footnote{This may be necessary, but is 
unlikely to be sufficient for the causal role of pain, or else a computer that monitors the state of a storage drive and 
moves data from damaged to undamaged portions feels pain when it detects damage and acts on the detection by averting 
putting data in the damaged location.} But imagine that the brain pathway leading to the tendency towards aversive behavior 
is broken. David Lewis??ref thought that in such cases the subject would still feel pain. The phenomenon of pain asymbolia
where pain is felt without aversive behavior---and apparently even without unpleasantness!---provides empirical evidence 
for this plausible hypothesis.??ref Similarly, if a patient's nerves are severed short of the tissue whose damage they are 
supposed to signal, surely the patient can feel pain originating in the nerves themselves (``neuropathic pain''??refs) rather 
than in damaged tissue---the case of phantom limb pain may be like that.?? Presumably, if \textit{all} of an unfortunate
patient's nerves were so severed, the patient would still be able to feel pain given stimulation of the nerves themselves,
even though the patient's pain would no longer actively play a part in a damage-detection system.

To account for this, it seems we need to define the functions of functionalist theories in terms of normal or typical 
behavior. David Lewis's own proposal is population-based: the function of a state is defined by what the state does in the 
relevant population.??ref This faces the notorious reference class problem: should we take the relevant population to be 
the biological species or the more narrow subgroup of people who have the particular condition that disrupts the causal 
network? If we take the relevant population to be the species, then we can make the empirically supported judgments about 
pain asymbolia and the intuitive judgment about total severing of nerves, namely that there is pain present in these cases.
If we took the narrower population as the reference class, we would have to say that there is no pain in these cases. 

So suppose we take the species to be the right reference class. But now imagine that a functionalist tyrant has 
all pain-signaling nerves severed, but does not wish to feel any pain originating from these nerves, and hence 
kills every organism that does not share  her unfortunate condition. Then there seems to be no reference 
class according to which her states homologous with pain have the causal role of pain, and so it seems that on 
Lewisian functionalism, the genocidal tyrant's plan has succeeded---by killing others, she has killed her pain.
But it is obviously false that one can end one's pain by killing others.

One might object that the relevant reference class should be counted diachronically, and so the many generations of 
past humans outnumber the survivors of the tyrant's genocidal plan, and hence ensure that the state still has the 
functional role of pain. But on such a diachronic interpretation, we need only change the story. Move the story into
the realm of science fiction, and suppose that the tyrant institutes a massive cloning program which within a few 
years produces more human beings---living doubtless in appalling conditions---who outnumber all the humans who lived
previously, and the tyrant ensures that each such human's nerves are severed just as her own are. Now on a diachronic
species-based account, the function of the tyrant's states has changed, and hence the tyrant ceases to be capable of 
feeling pain. This, too, seems patently false.

An alternative to population-based approaches is evolutionary approaches. The function of a trait is the
causal role that it evolved for---the causal role such that its historical fulfillment led to the present 
existence of organisms with that trait. There are many details to fill out to make this a tenable account,
but they won't matter for the critique that I will offer, based on Koons and Pruss??ref.  

Imagine a world $w_1$ where super-powerful aliens secretly snatch each organism from earth just before it dies and transport it in
some form of stasis to another galaxy, while replacing it with a fake corpse. There they cure whatever 
was ailing the organism and ensure that the organism succeeds in reproducing many times over. All the descendants of that 
organism are in the care of the aliens, who ensure that no genetic line goes extinct---that every organism in 
the care of the aliens succeeds in reproducing with its offspring also being taken care of. Moreover, in $w_1$
life on earth is an exact duplicate of how it is in the actual world, $w_0$---nobody ever catches on to idea that the organisms we 
have get snatched before death.

Without extinction, with every organism successfully reproducing, there is no evolution---just exponential population
growth. It is essential to an evolutionary explanation of a phenotype that the phenotype contributes to the the organism's 
probability of passing on its genotype. In $w_1$, there are no evolutionary explanations of any earth organisms, and 
hence on a functionalism where function is defined evolutionarily, no organisms have minds. This already seems somewhat
counterintuitive given physicalism: the physical causal history of every actual organism on earth is the same in $w_1$ 
as in $w_0$, and yet there are no minds in $w_1$. 

To make the counterintuitive aspect of this story even more problematic, now imagine a world $w_2$ which is just like 
$w_1$, except that once the organisms arrive in the other galaxy, they are immediately slaughtered. In $w_2$, we do have
evolution just as in $w_0$, the only difference being the exact location of death. So in $w_2$, on evolutionary functionalism,
there are minded beings on earth. But it is highly counterintuitive that whether the aliens choose to slaughter the 
abducted organisms once they take them to another galaxy affects whether earth primates are conscious.

One might object that in $w_1$ there is still natural selection, but one centered around location rather than 
existence. While the reason there are giraffes has nothing to do with the selective benefits of their necks in 
reaching food---without the necks, they might come close to dying of hunger but would still reproduce in the second 
galaxy---the benefits of their neck explain why there are giraffes on earth. But locational selection is highly
implausible as an account of the function underlying minds. Imagine a world where there is a galaxy full of planets
with randomly generated ecosystems, all of them in their first generation of macrofauna, some of which are 
phenotypically just like us. Suppose next that aliens pick out those macrofauna whose brains function in a way 
congenial to the aliens' plans, these macrofauna including organisms physiologically just like ours, with the 
reason for their selection by the aliens being that the brains function in the way required for mindedness. 
The aliens then take all these macrofauna and transport them to a new planet. On a locational selection version 
of evolutionary functionalism, it seems that as soon as the macrofauna are all in a new location because of the 
mental-like functioning of their brains, they become conscious. But it is absurd to suppose that organisms can 
become conscious simply due to a change of location. 

Granted, on some evolutionary accounts of selection,??ref selected function is only exhibited in the generation
subsequent to the selection. If so, then only the immediate children of the transported macrofauna have minds.
But that is only a little less absurd: it is surely not the case that simply a locational change in the parents
makes the children conscious. Moreover, the aliens need not even move the selected-for organisms. They could just
move the unselected-for organisms.

Besides, it seems \textit{ad hoc} to allow locational selection in addition to the much more natural notion of 
Darwinian existential selection. Once we allow locational selection, we should probably allow selection in terms 
of properties other than location. For instance, the aliens could paint a red circle on the head of those organisms
they like, and set up machines that put red circles on the descendants of any organisms with red circles. We would
then have red-circle selection, and if we allowed that to count for defining function, then we get the absurdity
that simply by painting red circles on some animals one can make them, or their descendants, conscious. 

??locational selection

\subsection{Many functions}
It is not enough that a subsystem always outputs the sum of its inputs for it to be an adder. After all, if the only inputs ever
given are pairs of zeroes, then the subsystem could just as well be a multiplier. As is well-known from Wittgenstein and Kripke??refs,
no finite amount of data is sufficient to determine the function of the system, since any finite collection of inputs and outputs is
consistent with infinitely many possible functions---admittedly, perhaps messy ones.

We might try to define the function of a system or subsystem in terms of counterfactuals. Perhaps an adder is something that \textit{would}
output the sum for all inputs in the range of allowable inputs. However, Frankfurt's counterexamples to the Principle of Alternate
Possibilities??ref can be adapted to show that this is untenable. Imagine that Bob counts as thinking that $5+7=12$ in virtue of the 
fact that his thought involves the operation of an adding subsystem in his brain. But suppose that a neuroscientist has placed a neural
scanner and bomb in Bob's vicinity in such a way that if the scanner detects the adding subsystem getting any input other than 
$5$ and $7$, the bomb blows up Bob. Now counterfactuals like ``If the inputs were $4$ and $3$, the output would be $7$'' are false.
If the inputs were $4$ and $7$, there would be no output, just an explosion.

One might try to define functions by asking what the output would be given the inputs \textit{absent external interference}. 
But what counts as interference with a system, as opposed to, say, a helpful or neutral effect, depends precisely on the system's
function. If the purpose of a wood-pulp product is to preserve inscribed information, then fire is an external interference;
but if the wood-pulp product's function is to be kindling, then fire activates its the object's function. 
Furthermore, one can internalize Frankfurt-like cases. Imagine that through science-fictional genetic modification Bob's liver
comes to behave just like the scanner-and-bomb system. 

\subsection{A neo-Aristotelian solution}
If functionalists do not have the resources to define functionalism, then this is presumably the most fundamental possible flaw
in functionalism. However, the problem of defining functions is exactly one of the one that Aristotelian forms are designed to solve.  The proper function
of a subsystem in an organism is that the fulfillment of which constitutes the organism's flourishing with respect to that subsystem.
This does not require the subsystem to be reliable, though Aristotelian optimism predicts that most systems will be reliable. 

A robust view of organisms as having forms that specify normative, and hence functional, features thus solves a central
problem with functionalism. And as we saw, there is very good reason for a naturalist to adopt functionalism---it is the
best naturalist option for saving multiple realizability. Thus, we have an argument from naturalism to Aristotelianism.
Of course, whether Aristotelianism is compatible with naturalism is not clear. If naturalism is understood to say that 
there are no fundamental normative properties, then of course there is no compatibility.
???refs

\subsection{A spectrum}
There is a spectrum of functionalisms, depending on the level at which we have the functions. Think here of the analogy of a computer. 
We could suppose that proper function is specified at a fairly low
level: say, the level of  components like transistors, which are specified by their datasheet---the transistor is designed to 
function in this range of temperatures, voltages, and currents, saturates at this level, has this current gain, has this 
transition frequency, etc.---and by having the correct data bits in the memory units. On this picture, for mental function
it doesn't matter how the components are implemented, say at the level of doped semiconductor materials. Or, at 
higher level, we could specify the CPU as a whole, rather than its transistors, defining the syntax and semantics of the 
machine code that it runs (with or without some abstraction, say with respect to edge cases), and then specifying what machine 
code and data are loaded. Or at a yet higher level we could specify the details of the algorithms, say in a high level 
programming language or even pseudo-code, for instance specifying the precise sorting algorithm that is being used for 
operationalizing preference rankings in an autonomous robot. Or we could abstract from details of the algorithms, and specify 
what the computer accomplishes at a very high level, e.g., simulate $n$-body Newtonian gravitational interaction, or 
translate spoken Cantonese into written ecclesiastical Latin. A similar hierarchy can be imagined in the case of minds
implemented wholly or partly by brains, though we know much less about the details of how high level function is implemented
by our neurons, so it is easier to illustrate the point with computers.

There is thus an important degree of freedom in functionalist theories of mind with regard to the level at which the 
mind-constituting functions are specifying. Some levels are more plausible than others. The lower the level of specification,
the less there will be of multiple realizability. In fact, in the computer analogy, it seems that even the fairly high level
at which details of algorithm choice are specified is too high. Surely an elephant and a dog could equally exhibit preference-driven
agency, with some efficiency differences, if one employs a merge sort and the other a bubble sort in generating its preference 
ordering. Furthermore, the lower the level of specification, the more Mersenne problems we raise, because the more degrees of 
freedom we have when we require the specification of detail.

There is thus reason to prefer the highest levels of specification. And Neo-Aristotelianism can rise to that challenge, because 
forms can specify proper function at an arbitrarily high level of abstraction. 

\section{Integrated Information Theory}
An up-and-coming alternative to functionalism as a theory of consciousness that has caught the attention of some 
neuroscientists is Integrated Information Theory (IIT) by Tononi, ??ref. IIT is based on the core insight that some complex causal systems 
can be physically separated into subsystems that interact only weakly or not at all with one another, in the sense that each 
subsystem only makes a small contribution---measured in information-theoretic ways---to predicting the functioning 
of the other subsystem. Such systems have little or no consciousness. For instance, a digital camera sensor contains a 
light sensor for each pixel. If we partition these pixel sensors into those on the left and the right halves of the 
full sensor, the data on each side makes relatively little contribution to the data on the other side (it makes \textit{some}
contribution, in that sensors heat up, and their heat generates noise in the data of neighboring
sensors). As a result the system has little consciousness.

But when a complex causal system has no such decomposition, it has ``integrated information'',
which typically constitutes consciousness. Given a mathematical model of the probabilistic functioning of the system, 
there is a specific measure, $\Phi$, of the degree of integrated information in the system. There is also a specific
proviso on which if two systems are nested---one is a part of the other---then only the one with the higher $\Phi$
value gets to be conscious. 

IIT has multiple realizability, but significantly less of it than functionalism does. Those isomorphisms 
between systems $S_1$ and $S_2$ that preserve transition probabilities of states and map physically separate 
components of each system to physically separate components of the other system will preserve $\Phi$. 
Functionalism, on the other hand, allowed for functional isomorphisms that changed transition probabilities 
(e.g., replacing a component with a more reliable one), as well as ones where multiple physically separate 
components got mapped to one, or one physical component got implemented by several physically separate ones. 
For instance, in a computer simulation of a brain system, an electrical potential in a single nerve may be 
modeled by a 64-bit floating point number. Each of the 64 bits of the floating point number is stored in a
separate tiny storage capacitor in a memory chip: thus one component is implemented in 64 physical components.
The simulating system thus allows for decompositions (say, between the even- and odd-numbered bits) that the original
physical system did not allow for. 

We could in principle implement a consciousness in an electronic system by mimicking a human brain given IIT. However, we would 
not only have to simulate the functioning of the brain, but we would have to implement this functioning with  
physical components that have a one-to-one mapping with the physical components of the brain, e.g., neurons mapped to 
computer chips, nerves to wires, etc.

Thus, IIT avoids some of the difficulties facing functionalism. In particular, while I have argued that any 
finite deterministic system can be mapped to the states of a single particle moving in a straight line, and 
hence every particle has human-level consciousness on functionalism??backref/forwardref, no such conclusion 
follows for IIT. For while the different states of a brain can all be encoded into, say, the position of one
particle---the position can be specified as a number with infinitely many decimal places, and these suffice
for encoding the states---the particle is not a complex system capable of physical subdivision, while the brain is. 

Furthermore, the difficulties about specifying the degree of reliability needed for consciousness in a functional
system disappears entirely: the $\Phi$ measure precisely takes into account the transition probabilities, and how
they modify the amount of integrated information.

Unfortunately, IIT is subject to fatal counterexamples.??Ref:Aaronson,Tononi For instance, consider a very large square grid.
At each point of the grid is stored a zero or one. At each step of the evolution of the system, the value at 
each grid point is replaced by the \textit{logical or} of the values of the two to four neighboring points (the 
points in the middle of the grid have four neighbors; the points at the corners have two; the points at the 
edges have three). Thus, if a point is surrounded by zeroes, it becomes zero; otherwise, it becomes one. If 
there are any ones in the system, they spread all around until they fill the grid. If the system starts at zero, 
it stays at zero. It turns out??cf:refs that even if the system is all at zero, it has a 
degree of integrated information proportional to the number of points in the sides of the square grid. As a 
result, a sufficiently and impractically large grid---say, one with something like $10^{15}$ points per side---will 
have human-level consciousness, even when the grid is unchangingly fixed at zero. While Tononi embraces such 
conclusions??ref, they seem highly counterintuitive.

For a much simpler case, imagine a world with two eternally particles, each having an electric charge specifiable
as any real number. Suppose, further, that the laws of nature specify that every second, the two particles swap their charges. This system has only one decomposition into two subsystems---the two particles. Each particle contains 
an infinite amount of information for the prediction of the other particle's subsequent charge, since a real
number contains infinitely many bits of information (it can be expressed by a sign, a binary integer, a ``binary 
point'', and then infinitely many binary digits). The $\Phi$ measure of the system is infinity. Thus, the two 
particles that eternally interchange their charges are infinitely conscious. While a panpsychist might be willing to 
admit that two particles are collectively conscious, this would surely be a minimal level of consciousness, rather than 
the god-like infinite consciousness that IIT predicts.

IIT has a pleasing mathematical precision, and a tempered version of multiple realizability. However, it leads to 
implausible consequences. Perhaps some future IIT-like theory will avoid the extravagancies of functionalist multiple
realizability while avoiding the implausible consequences of IIT. But at this point we should not a particular 
limitation of IIT. As it stands, IIT is a theory of the amount of consciousness in a system. But what we really 
want in a theory of consciousness is an account of what constitutes particle phenomenal experiences: what makes 
an entity feel pain, hear a middle C, see red, or experience a sense of awe at the mystery of existence. We can 
sketch toy functionalist stories about some of these---say, pain being a state that mediates between damage detection
and avoidance behavior---but IIT's information-theoretic character does not seem to lend itself well to analysis of 
phenomenal experience. Moreover, ``information'' in information theory is binary bits, abstracted from their specific 
meaning. But any reduction of phenomenal experience to information would, plausibly, have to focus on the specific 
meanings (damage, say). 

Functionalism, thus, still rules among naturalist theories that allow for significant multiple realizability.

\section{Supervaluationism about minds}
A tempting solution to the problem of the multiplicity of functionalist (or other) theories of mind differing with respect
to fine details is to treat each theory as a precisification of concepts like \textit{mind} or \textit{pain}, none of which
is privileged over the others. And,
if all has gone well, then typical adult humans fall under all the precisifications of ``has a mind'' and in paradigmatic
cases of being in pain they fall under all the precisifications of ``is in pain''. 

One difficulty with this approach has already been discussed in the ethical?? context??backref, namely that similar problems
arise at the meta level: What \textit{range} of, say, standards of reliability yields a functionalist concept that is in fact a 
precification of our concept of mind? 

Furthermore, recall the argument??backref that the centrality and overridingness of ethical norms to our lives makes it 
deeply implausible to think that there is a plurality of closely related concepts, none of which is privileged over the others.
But given the deep importance of the mental to ethics, the same concern applies to the mental. That an action causes severe
pain to a non-consenting individual with no significant benefit is a conclusive moral reason not to perform the action. 
But if there are many concepts of pain with none of them privileged, and similarly of consent, then whether one has a
conclusive moral reason for an action will depend on the choice of precisification as well. Hence, whether one has a 
conclusive moral reason becomes a verbal question in such cases, and that is not compatible with the force of moral
reasons.

There is also something counterintuitive about the implications of this kind of vagueness about mind. There is some 
plausibility in thinking that there can be vagueness about the exact phenomenal character of a mental 
state. But vagueness about whether there is a phenomenal character at all seems more problematic. For instance,
maybe it can be vaguely true that someone is in severe pain, but in such a case it is definitely true that they are 
at least experiencing a discomfort. This inference, however, is invalid on the precisification approach to solving 
the problems with functionalism. Imagine a perfectly reliable computational system $S_0$ which would definitely implement severe pain---a 
system that, say, emulates what happens in a human brain when one has one's leg amputated without anaesthesia. 
But then imagine a continuum of systems that in the actual world behave just like $S_0$, but are less and less 
reliable. At some point at this continuum we simply have completely random behavior that by chance matches that 
of $S_0$. Somewhere in between it is vague whether the system has the same functions as $S_0$. But we can imagine that 
everywhere along the continuum it is definitely true that \textit{if} the system is conscious of any discomfort at all, it is 
conscious of a severe pain. Thus in the vagueness region, it is vague whether the system is experiencing
severe pain, \textit{and} whether it is experiencing even any discomfort. This kind of state of vagueness about whether
there is severe pain and whether there is discomfort appears impossible.

To make the counterintuitive character of this kind of vagueness clearer, imagine that you start as a conscious human
that is continually observing the visual situation around you, a field of colorful wildflowers. But then your neural system gradually 
becomes less and less reliable, but by luck nothing goes wrong---everything actually behaves as if all the reliability were 
in place. Moreover, all
your subsystems are always equally reliable or unreliable, I suppose. Eventually, according to functionalism, mental states 
stop, and they all stop together, because the level of reliability is the same for all your subsystems. On the vagueness
solution currently under consideration, at any given time in the process it is definite that you are either mindless or 
are having normal human perceptions of a field of wildflowers, but it eventually becomes vague which of the two it is.
We are apt to imagine this process as a one involving fading mental states---the wildflowers get less colorful, your thoughts get 
sluggish---but that's incorrect. It is definitely true that if you are having any mental function at all, you are seeing 
the wildflowers with all their vibrance and are thinking as well as ever, and hence it is definitely true that the 
wildflowers are not getting less colorful and your thoughts are not deteriorating. Yet, allegedly, at a vague point 
it all transitions from full vividness to nothing. We can, of course, imagine a transition from full vividness to nothing
with no in-between states---a sudden loss of consciousness, as when you receive a massive head injury. But what 
seems absurd is the idea of a sudden transition from full vividness to nothing with no in-between states that 
happens at a vague time. 

??ref:fading-qualia?

Additionally, the kind of vagueness involved here could generate a situation where it is vague whether something is a 
person, and hence has the kind of dignity or even infinite value??ref that persons are often thought to have, or whether
it is a mindless thing---with its being definite that it is nothing in between. For suppose it is vague whether
a certain degree of statistical reliability is enough for proper function, and then suppose that all of the computational 
systems of an entity have that degree of reliability, so that it is definite that if that degree of reliability is sufficient
for function, the system is a person, and if it is insufficient, the system completely lacks computational function, and hence
is mindless. One might hold that it is possible to have a system where it is vague whether it has enough mentality to have 
the value of a person, but our best candidates for imagining such a system are ones that have significant mental functioning,
but it is vague whether that amount is sufficient for personhood. But the idea that it is definite that it is either a 
person or mindless, but vague which one, seems highly counterintuitive.


\section{Substances}
The functions necessary for human cognition appear to be housed in each of a multiplicity of entities, including the 
whole human organism, the human organism less hair, the head, the brain, and perhaps just the upper brain. Standard functionalism
seems to imply that all of these entities think the same thoughts. But this has some very implausible consequences. If all 
of the above entities think, then by the same token for any subset $S$ of the set of my hairs, there is an entity $A_S$ that 
contains all of my organism less hair plus precisely the hairs in $S$, and on standard functionalism it seems that $A_S$ thinks
exactly as I do, for all $S$. The number of $A_S$ entities is equal to $2^N$ where $N$ is the number of hairs that I have. If I
pull out one of my hairs, half of these entities perish. Since I have hundreds of thousands of hairs, the loss of conscious
beings is vast---more than $2^{100000}$ beings. Moreover, pain had by a more hirsute person is shared by a much 
greater number of entities, all other things being equal, so it follows that other things being equal (such as the mass 
of the non-hair parts), given a choice between relieving the pain of a bald and a non-bald person, absurdly we should help 
the non-bald person. A similar argument applied to atoms instead of hairs shows that more massive people should typically have 
vastly greater preference in our moral calculus, and that weight-loss is akin to murder.

We might try to escape these ethical worries by insisting that what counts for moral purposes are not the tokens of thought
but the types. This too is counterintuitive. Suppose disease $A$ is had by a million 
people that causes a severe pain, and careful functional analysis shows that every sufferer from the disease feels 
\textit{exactly} the same pain. On the other hand, disease $B$ is had by a two people, and causes in each of them 
a different qualitatively unique pain but equal in severity to that of the sufferers from $A$. If we count types of 
thought, then it seems that it is intrinsically twice as good to distribute pain killers to the $B$ sufferers than to 
the $A$ sufferers. And that seems quite wrong.

Perhaps, though, what we should count for moral purposes are not types of thought, types of lifetime streams of thought.
Even though each $A$-sufferer has the same pain, that pain is embedded in a different type of lifetime of thought. On the other 
hand, the thoughts of each of the $A_S$ entities are embedded int he same type of lifetime of thought. The lifetime stream type
approach, however, is not all that plausible. I think most of us will have the intuition that if disease $A$ is found among
a million mentally duplicate twins, it's still better to relieve the pain of $A$ than of $B$. 
Furthermore, the lifetime stream approach implies that how much moral consideration you now deserve can depend on what 
\textit{will} happen to you, since what type of lifetime stream of thought you have depends on what will happen in the future.

Alternately, we might modify the functionalism to rule out all but one of the nearly-colocated entities as candidates for
thinking. The most natural approach here is to say that what thinks a thought is a minimal or a maximal system exhibiting 
the relevant type of functioning. Neither account seems tenable. The maximal approach seems particulary absurd: my thoughts aren't 
being had by some giant system that includes my brain and the Orion Nebula. Probably the only defensible version of the maximal
approach is a holism on which it is the universe as a whole that thinks---but then the moral counting problems reappear, because
all the thoughts are had by the same entity, and so there does not seem to be a reason to relieve the pain of disease $A$ rather 
than of $B$. 

The minimal approach is problematic in a different way. There need not be a unique minimal functional 
system. Consider a bridge made of planks, whose function is to allow a one ton load to be rolled over the bridge. 
It might turn out that there is some unique subset $S$ of $n$ planks such that the planks in $S$ would suffice to 
bear the load, and such that any other subset sufficient to carry the load would have $n+1$ planks. But this also 
need not be true. It could well be that the bridge is such that there are two or more different subsets with a 
minimal number of planks that can carry the load. 

The question of how many minimal functional systems there are in a given human organism is a potentially interesting
empirical question, but it is intuitively irrelevant to the question of how much moral consideration should be given 
to this human's suffering. Furthermore, there seems to be a rather arbitrary decision to be made as to how one 
defines the minimality of a system. One might try to do so by minimalizing the number of particles, the number of atoms, the number of 
molecules, or the mass.\footnote{The last is a little tricky, because we learn from Einstein that energy is a component in mass,
and the mass of a part then depends on the energy it has as a part of a larger whole.} All of these could result in 
different determinations.

The Aristotelian, of course, has a natural solution: there are only as many substances as substantial forms, and 
substantial forms unify some but not all arrangements of material parts. Something like this can be said, however,
by anyone who believes in non-vague restricted composition, i.e., that only some precise arrangements of material parts 
make up a whole, for instance a variant of van Inwagen's theory those only arrangements of multiple material parts that are 
caught up in a life make up a whole but without van Inwagen's belief in the vagueness of life. However, the Aristotelian
has a further advantage over other restricted compositionalists with respect to functionalism. It is not a coincidence, 
on the Aristotelian view, that those systems of material parts that have a function also have a substantial form, because 
the substantial form defines function. On a non-Aristotelian restricted compositionalism, on the other hand, where 
function is not defined in terms of form but, say, in terms of actual causal interactions or population statistics(??backref,add?,Lewis),
both brains and organisms will be apt to have the kinds of functions that define minds, and yet only organisms 
really exist---are real composites of parts. 

\section{Dualism}\label{sec:dualism}
The neo-Aristotelian teleological twist on functionalism allows one to remain to a significant degree a naturalist (more on that
in ??forward). The account reduces mental properties to functional properties, but the functional properties are not 
reducible to the kinds of properties that physics studies. A more traditional Aristotelian approach, however, is to refuse
to reduce the reduction of mental properties to non-mental ones. This is certainly also compatible with the robust view of
human nature that has been defended in this book. 

Whether the more dualist or the more functionalist view is more plausible depends on how satisfying the reduction of mental
properties to teleologically laden functionalist properties is, and one way to evaluate this reduction is to consider
whether the arguments against the reduction of mental to physical properties apply to this neo-Aristotelian functionalism.
These arguments can be divided into three categories, depending on which aspect of mental life they object to the reduction
of: content, consciousness, or freedom of will. 

As has been noted??backref, the neo-Aristotelian teleology offers hope for a reductive account of mental content. Teleological properties
can be hyperintensional---it is the purpose of eyes to see rather than to see or be such that $2+2=5$, even though necessarily
everything that sees is such that it sees or is such that $2+2=5$. Thus concerns that there is no way to reach the hyperintensionality
of mental content from the extensional or at best intensional content of the physical world do not transfer to the neo-Aristotelian
account. Similarly, concerns about the need to account for purpose in the mind---beliefs are states that are there 
\textit{in order to} mirror the world---and that the only source of purpose for the physicalist, namely evolution, is inadequate
do not apply to the neo-Aristotelian theory. Thus, content-based arguments do not tell against a neo-Aristotelian functionalism.

The case of consciousness is less clear. We can try to run standard knowledge arguments transposed to the neo-Aristotelian context.
Suppose Mary is raised in a black and white environment, and knows all the physical facts as well as all the normative facts about
human beings. In particular, she knows that certain states of the human being are such that they should occur precisely when the
human being is looking at a red object, and that these states typically occur when people see a ripe tomato. She, further, knows all
the normative interconnections between these states and other states, including all the inferential connections. Will she learn anything
further by seeing the tomato? Here intuitions may differ. It is at least easier to hold out for a negative answer to the learning question
here than in the case where Mary has a purely physical state.

Similarly, we can try for imaginability arguments. These come in two versions: zombie arguments that one can have our physical
constitution without consciousness and afterlife arguments that our consciousness could survive the destruction of the body.
On these arguments, the imaginability of a scenario provides defeasible evidence of its metaphysical possibility. 

Could there be beings that have isomorphic physical and normative properties to ours
but that are unconscious zombies? Again, this is not completely clear. Among our normative properties are moral duties. Could one
have something isomorphic to moral duties without consciousness? If not, then zombie arguments against neo-Aristotelian functionalism 
do not work as they stand. 

Alternately, what can the neo-Aristotelian functionalist say about surviving the destruction of the body? Those ``fainthearted'' 
neo-Aristotelians who hold that the form is nothing but a kind of arrangement of matter, of course, will find it troubling to
suppose the form to survive the destruction of the matter. Though even there, there is a potential precedent for a view
that would allow the form to survive. Aquinas's account of transsubstantiation holds that in the Eucharist, the accidents of
bread and wine continue to exist after the cessation of the existence of the substance. Among the accidents there will be 
\textit{shape}. If a shape can exist without that of which it is the shape, then why not an arrangement as well? Similarly,
some contemporary trope theories hold that there are ``unaffiliated'' tropes, tropes that exist without their substance. If a 
trope can exist without a substance, why can't an arrangement exist without matter? 

But of course the view defended in the book is more robust: forms are not mere arrangements. The more reality the form itself
has, the more plausible that there is no metaphysical impossibility about the form existing without the matter. 

Finally, we have freedom of will. Those convinced that an action that comes solely from the causal powers posited by a completed
physics??ref would not be free would not be moved by being told that these causal powers have a normative organization. 
Adding to the normative organization the thesis that form grounds all of the nomic behavior of the entities (perhaps including 
even at the microphysical level)??backref may seem to help recover a more robust sense in which the form is a soul that is in 
charge of the body. But it is not clear that it really solves the problem. For on that account, our form likewise grounds 
the nomic functioning of all of our subsystems including functioning, such as immune function or bone regeneration, that are 
only very indirectly under our voluntary control. It is not clear that adding form's grounding of our behavior is 
sufficient to recover a robust sense of free will.

One move here is to suppose that in rational animals the form has one more explanatory role, not reducible to the explanatory
roles it has in other beings: it voluntarily controls those aspects of the functioning of the organism that are directly
voluntary. An irreducible notion of voluntary control, however, is murky. It would be philosophically preferable to be able to see 
voluntary control as a species of the control that the form already has over all of the organism, and to explain what makes the 
control voluntary in terms of the other explanatory roles of the form. 

For instance, perhaps what makes the control voluntary is that in voluntary control, the form ensures a connection between
actions and \textit{reasons}. And insofar as the form is the central metaphysical component of the agent, a causal story 
going back to the form and to reasons can be plausibly thought of as just a more detailed account of what happens in 
agent causation.

\section{Normativity and representation}
Perhaps the toughest problem for naturalistic theories of mind is the problem of intentionality or representation:
what makes a state of an entity be \textit{about} the external world. Attempts at answering this in terms of the 
causal interaction of the organism with its environment run into difficult problems with the intensionality and 
even hyperintensionality of representations.??refs:Fodor?? We have the same causal interactions with mammals as with earth mammals,
and with triangular and trilateral shapes, and yet thoughts about mammals and those about earth mammals are not the same,
nor are thoughts about triangular and trilateral shapes. 

Fundamental normativity of an organism might provide a very neat reduction of representation. Here is one attempt: 
\ditem{norm-rep-1}{A 
state $S$ of an organism represents the world as being such that $s$ if and only if the organism's nature includes the 
norm that $S$ should occur only if $s$.}
 This is an extremely broad account of representation. If a bacterium is such that it 
should move fast away from a location $z$ only if $z$ contains a hazard, then its movement represents $z$ as containing 
a hazard. Simple as this account is, it has no problem with intensionality and even hyperintensionality, since the 
normative facts are, plausibly, hyperintensional.??backref/forwardref?? It is a different thing to be such that one should move 
away fast away from $z$ only if $z$ contains a hazard and to do so only if $z$ contains a hazard and $2+2=4$, even though
of course necessarily $z$ contains a hazard if and only if $z$ contains a hazard and $2+2=4$.

This account has an intriguing connection with ethics. It is generally true that one $\phi$ing is something one should only do 
if $\phi$ing is permissible. Thus, whenever we act, our action represents something---it represents its own permissibility. 
This fits with an intuition that some thinkers??refs have about how all our action represents something, and how all wrongful
action is akin to a lie, a misrepresentation of normative truth. Furthermore, some specific actions may be taken to be 
misrepresentations in a thicker way. Suppose I use you as a mere means. If Kant is right that using something as a mere means 
is only permissible if that thing is a non-person, then by using you as a mere means, I misrepresent you as a non-person.  
On the positive side, if it is the proper function of our will to pursue the good, then whenever we pursue something, we 
represent it as a good. 

This account also has interesting consequences for philosophy of language. First, an assertion that 
$s$ represents the world as being such that $s$. This immediately implies a version of the truth norm
for assertions: an assertion that $s$ should be made only if $s$. Some opponents of the truth norm??refs 
will take this to be a reason to reject the normative reduction of representation. But it is worth noting
that there are in fact multiple norms governing assertion. Thus, if it is insisted that knowledge is a norm
of assertion, there is no difficulty accommodating this in the account: it could be that one should
assert $s$ only if $s$, but likewise one should assert $s$ only if one knows that $s$. In such a case, 
assertion represents both its being the case that $s$ and its being the case that one knows that $s$, 
so an assertion represents more than is being asserted---a familiar Gricean thought. In fact, the breadth
of what is represented goes further. One should only assert something 
if it is either good manners to assert it or there is some reason overriding the reasons of 
etiquette. Thus, on our reductive account, an assertion that $s$ not only represents the worlds as being
such that $s$, but also represents the world being such that it is good manners to assert that $s$ or 
there is some reason in favor of asserting this that overrides the reasons of etiquette. That's a 
surprising.

Perhaps, though, the above account of representation is too broad. Maybe bacteria don't represent hazards by 
moving away from them, and maybe saying ``Your argument is unconvincing'' doesn't represent claims about etiquette.
There is a slight tweak to the reductive account that narrows the field of representation significantly:
\ditem{norm-rep-2}{A state $S$ of an organism represents a proposition $p$ if and only if the organism's nature includes the 
norm that $S$ should occur only if $p$ is true.}
Norms are hyperintensional, so there is a difference between the norm that one should run away from $z$ only if
$z$ contains a danger and the norm that one should run away from $z$ only if it is \textit{true} that $z$ contains 
a danger, even though of course there is a danger there if and only if it is true that there is a danger there.

It is plausible that lower organisms have no normativity concerned with truth as such, and hence the alethic
version \dref{norm-rep-2} of our reduction will not apply to bacteria. Furthermore, we lose the ethical 
applications of \dref{norm-rep-1}. For instance, while the Kantian will insist that it is one of the norms 
in a rational being that we should treat something as mere means only if it is a mere means, this does not 
mean that we have a norm of treating something as a mere means only if it is \textit{true} that it is a mere
means. Moral norms rarely make reference to truth as such. Finally, while the modified reductive account still
forces us to accept that there is a truth norm on assertion, it allows us to accept, say, a knowledge norm as 
well, without having to say that in asserting we represent the world as being such that we know, since the 
knowledge norm is that one should assert $p$ only if one knows $p$, not that one should assert $p$ only if 
it is true that one knows $p$. 

The alethic version \dref{norm-rep-2} of our normative reduction thus loses some of the interesting applications,
but these applications were also controversial. 

Of course, we might also just say that ``represents'' is said in multiple ways, and there is both simple representation,
defined by \dref{norm-rep-1}, and \textit{alethic} representation, defined by \dref{norm-rep-2}. The two are 
closely related: a state alethically represents $p$ just in case it simply represents $p$ being true. We can then
divide the substances in the world into non-representers, simple representers and alethic representers. Given how 
broad simple representation is, it is not clear that there are any non-representers. If there are not, then we have 
an account that is rather Leibnizian in nature: our simple representers and alethic representers parallel Leibniz's 
division of the monads into souls and minds. 

In light of Tarski's indefinability of truth theorem??ref and Leon Porter's argumentation??ref, there is very good 
reason to think that truth is not even coextensive with a physical property. A simple version of the argument notes that 
although the Tarski T-schema 
\ditem{T-schema}{Sentence ``$s$'' is true if and only if $s$}
may not hold for sentences using semantic vocabulary, it is true for indexical-free sentences $s$ in the language of physics:

If truth is coextensive with a physical property $\tau$, then consider the sentence of the language of physics:
\ditem{liar-tau}{$u$ lacks $\tau$,}
where ``$u$'' is a physical description of a token of \dref{liar-tau}. E.g., at $t_0$ Alice might whisper ``Alice's whispering 
at $t_0$ lacks $\tau$.'' If the token described by $u$ is true, then by the left-to-right T-schema $u$ lacks $\tau$, and hence
lacks truth, a contradiction. If it is not true, then it lacks $\tau$, and so by the right-to-left T-schema $u$ is true, 
and hence $u$ has $\tau$, a contradiction. 

Now, the best alternative to the Aristotelian account of teleology is an evolutionary account according to which the 
proper function of a system is a function that has an evolutionary explanation. But evolutionary explanations are 
scientific, and hence what they explain is the occurrence of physical properties, or at most properties coextensive 
with physical properties. Therefore, an evolutionary function cannot involve anything coextensive with truth, and hence 
even if evolutionary teleology can do work in other contexts, it cannot be used in the context of our account of 
alethic representation.

\section{Soul and body ethics}
There are four main families of views of what we are: (i)~brains or cerebra, (ii)~souls, (iii)~purely physical bodies or organisms, and 
(iv)~body--soul composites.\footnote{Of course, one can imagine other options. The most radical is that we do not 
exist??ref:Unger. An apparently undefended option is that we are brain--soul composites.} In this section, I will review 
the well-known claim the brain and soul views have implausible normative ethical consequences, and argue that the body--soul view,
especially in its Aristotelian variant where the soul is the form of the body, is the normatively ethically most attractive option.

On soul or brain views, our bodies\footnote{In the context of brain views, I follow the loose common usage
where ``body'' denotes the body minus the brain.} are not a part of us. In particular, you have never seen 
or touched any of the people you love, unless perhaps (on the brain view) you have witnessed brain surgery or 
a horrid head injury. 

There is an important legal distinction between assault and assault with battery, where
battery involves harmful bodily contact. This distinction is difficult to justify morally
if our bodies are not a part of us. On brain and soul views, bodies seem to be very much 
like worn personal property, such as clothing. And the very high ethical importance that we 
attach to avoiding unwanted sexual touch is particularly mysterious.

One might respond that unlike clothes, our bodies are full of sensors that make us directly
aware of what is happening to them. Furthermore, our bodies are under our direct control---we have 
basic actions that control the body, and another's interference in the body thus impacts our 
autonomy much more than interference in clothing or other property. But while this line of thought
has some force, it does not fully explain the importance of interference with a body. 

Battery, sexual or not, is morally noxious even when the victim is completely unconscious, and 
hence the sensors are irrelevant. Moreover, the triggering of harmful sensory impressions
is not what makes battery particularly objectionable, because harmful sensory impressions can 
be triggered without battery. Here, think of the difference between shoving a police officer 
during a protest against police brutality and showing the officer a gory poster depicting 
brutality. Shoving is \textit{prima facie} morally problematic in a way in which showing 
the poster is not, even though both present noxious sensory stimulation. The difference between 
the two treatments appears to rest in the fact that the shove modifies (albeit slightly) the victim's body 
in a way that goes beyond collaboration with the victim's sensing of the environment.

Similarly, while some of the time interference in the body of another, say by shoving, directly 
undercuts the victim's direct control of the body, this is not a necessary condition for battery.
Non-consensual amputation of a permanently paralyzed body part does not damage the victim's direct
control of the body, but has the special harm of battery.

Thinking about taxation is another way to see the ethical distinction between the body and property.
It would be wrong for the government to tax one by forcing a kidney donation, even if there were strong
evidence that on the whole a kidney donation has a smaller impact on the taxpayer's wellbeing than some
clearly permissible monetary tax. There is a dignitary harm in being divested of a part of oneself in the 
kidney removal.

We thus have good reason to think that we have bodies as parts of us. That leaves the question whether
we are \textit{just} physical bodies, or physical bodies with souls or forms. All the previous arguments of this
book point to the need to posit non-physical forms. And on the Aristotelian story about forms, given their
role in governing behavior, it is reasonable to identify them with souls. But is there a specifically 
normative ethical, rather than metaethical, reason to think that we are not just physical bodies? 

Here is one such reason. There is a spectrum of very similar beings between a rock and any 
purely physical being---add a particle here, shift this one, etc. The functional changes, understood 
physicalistically, between neighboring items in the spectrum from can be quite small. Yet persons have 
a qualitative dignity that does not appear to be something that one can generate by an otherwise 
insignificant change from something that lacks that dignity. On the other hand, it is far from clear that 
we can have a spectrum of souls or forms ranging from rocks, or at least trees, to people, with only 
small difference in between. The transition, for instance, from being such that one should pursue various
particular goods (nutrition, shelter, procreation, etc.)\ to being such that one should pursue the good as 
such seems to be something that does not come merely by minor variation, but is a whole new normative property
in the form.

\section*{Appendix: Functionalism gone too far}
Consider a deterministic functional system $Q$ consisting of a 
finite number of possible computational subsystems $S_1,...,S_N$, where $S_k$ is always in exactly one state from the finite set $\scr A_k$
of possible states at each of the discrete ``significant'' moments of time $t_1,...,t_m$ over its finite lifetime\footnote{A standard modern digital computer only has defined 
computational states at ticks of its internal clock. Between ticks of the internal clocks, it is in an analogue state that is not
computationally defined. A lot of careful engineering goes into ensuring that the states become properly ``digital'' at the clock ticks.}, and 
a finite number $I_1,...,I_M$ of sensors, where $I_k$ is always in exactly one state from the finite set $\scr I_k$ of possible input
states.

We can think of $Q$ as a finite digital computer. The total state of the system at any given time can be represented as the
$(N+M)$-tuple $(a_1,...,a_N,b_1,...,b_M)$ where $a_k$ is a state in $\scr A_k$ and $b_k$ is a state in $\scr I_k$. We can designate some of
the subsystems as outputs, connected to external effectors (muscles, motors, lights, etc.)  Furthermore, we suppose there are functional laws which provide a 
deterministic mapping $f$ from the total state of the system at time $t_i$ to the total state at time $t_{i+1}$. Thus, $f(a_1,...,a_N,b_1,...,B_M)= (a_1,...,a_N,b_1,...,b_M)$ 
provided that the system would transition from state $(a_1,...,a_N)$ to state $(b_1,...,b_N)$.\footnote{The mapping is independent of time. But we 
can always suppose there is a finite clock, e.g., given by a subsystem $S_k$ such that the set of states $\scr A_k$ is
the set of times during the system's lifetime, and with the transition rule that the time always gets incremented. Or we can suppose an input
from an external clock.} Presumably any deterministic analog system can be approximated by such a system $Q$ to aribitrarily high precision.
We suppose for convenience that there is some fixed initial state of $Q$, with fixed initial input and computational states.

Now consider a different system $P$ consisting of a single particle moving in the $xy$-plane in space, with a constant 
(sublight) non-zero $x$-velocity $v$, starting at $x$-coordinate $0$ at time $t_1$. For ease of visualization, suppose the $x$-axis
runs left to right, and the $y$-axis runs down to up.
Let $\scr J$ be the set of all possible
single-time input state vectors $(b_1,...,b_M)$ where $b_k \in\scr I_k$ for each $k$, and let $K$ be a positive integer such
that $\scr J$ has at most $10^K$ members. 

We now recode $Q$'s inputs into $P$'s inputs as follows. Suppose our particle is at $y$-coordinate $0$ at a time $t_0<t_1$. 
For $1\le n\le N$, let $\psi_n$ be a one-to-one function from $\scr J$ to integers between $0$ and $10^K-1$, both inclusive. The analogue 
to inputting the sensor state vector $(b_1,...,b_M)$ into $Q$ at significant time $t_n$ will now be this. We take
the $y$-coordinate value $y_{n-1}$ at $t_{n-1}$, and add to it the value $10^{-Kn} \psi_n(b_1,...,b_M)$, shifting the particle
upward along the $y$-axis as needed to ensure it reaches that by time $t_n$.\footnote{We may need to ensure the units in which
these are measured are such that the particle can be shifted in the requisite time without exceeding the speed of light.} Thus, 
the first set of inputs of $Q$ will be encoded in the first $K$ digits after the decimal point, the second set of inputs
will be encoded in the second $K$ digits, and so on.

Next, suppose $Q$'s computational states begin with the fixed initial state vector $(a_{1,1},...,a_{1,N})$ at time $t_1$. Given 
the determinism of the system, there is a mathematical function $f$ that takes a sequence $s$ of length $n$ of members of $\scr I$ 
and returns the computational state $f(s)$ that $Q$ would be in at time $t_n$ if it were to have received the sequence of inputs $s$ 
at the times $t_1,...,t_n$. Let $n(x)$ be the integer $n$ such that $x=v t_n$, if there such an integer, where we recall that
$v$ is the velocity of our particle along the $x$-axis. Let $s(x,y)$ be the sequence of $n(x)$ sensor state vectors encoded 
by the $y$-coordinate value $y$, assuming $n(x)$ is defined. Thus, the first member of $s(x,y)$ will be the sensor state vector
$s_1$ such that $\psi_1(s_1)$ is equal to the decimal number given by the first $K$ digits after the decimal point in $y$, and so on.
We now deem $P$ to be in a computational state corresponding to $f(s(x,y))$ when $P$ is at $(x,y)$. This is defined at all the
significant times $t_1,...,t_n$. We have, thus, an isomorphism between the computational states and sensor inputs of $Q$ and $P$.

We now go for one final twist. Suppose that in the actual world, the
system $Q$ gets the sensor input vectors $s_1,...,s_N$ at times $t_1,...,t_N$, respectively. We can now choose the 
function $\psi_n$ such that $\psi_n(s_n)=0$ for all $i$. Then a single particle moving with constant velocity 
$v$ along the $x$-axis, with $y$-coordinate always equal to $0$ is an isomorph to the actual functioning of $Q$.
But the very same particle will be an isomorph of any other system $Q'$ with the same significant times. Thus,
the particle will think your thoughts \textit{and} my thoughts!

??do we need a clock?

\chaptertail 


