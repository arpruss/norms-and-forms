\def\mychapter{V}
\input{chapterhead}
\chapter{Epistemology}\label{ch:epistemology}
\section{Balancing doxastic desiderata}
I observe one raven, and it's black. I observe another and it's black, too. The story goes on. Every
raven I observe is black. After a certain number of ravens, in a sufficiently broad number of settings,
it becomes reasonable to believe that all ravens are black. But when?\footnote{I am grateful to Sherif Girgis for raising the issue of incommensurable desiderata in connection with these issues.}

William James famously identified two incommensurable doxastic desiderata: attainment of truth and avoidance of falsehood.
The larger the number of black ravens that are needed for me to believe that all ravens are black, the more surely
I avoid falsehood, but the more slowly I attain truth. Intuitively, there is room for differences between reasonable
people: some tend to jump to conclusions more quickly, while others are more apt to suspend judgment. But on either
extreme, eventually we reach unreasonableness. Both someone who concludes that all ravens are black based on one 
observation and someone who continues to suspend judgment after a million broadly spread observations are unreasonable.

There is, thus, a range of reasonable levels of evidence for an inductive belief. And, as in the myriad of ethical
cases of Chapter~\ref{ch:ethics}, this raises the Mersenne question: What grounds facts about the minimum amount of 
evidence required for an inductive inference and the maximum amount at which suspending judgment is still rational? 
Of course, the ``minimum'' and ``maximum'' may may depend on the subject matter, on higher-order evidence such as about 
how well previous inductive generalizations have fared, and even on pragmatic factors(??ref). But that added complexity does
nothing to make the Mersenne question easier to answer. And, as we discussed in ??backref, invoking vagueness does not
solve the problem, but multiplies the complexity even further.

And, of course, my contention will be that conformity to the human form is what grounds the answers for us. The
rational way to reason is the way conforms to our form's specification of the proper functioning of our intellect.

It appears to be quite plausible that different answers to the rationality questions would be appropriate
for species of rational animals adapted to different environments. First, some possible worlds as a whole have 
laws of nature implying a greater uniformity than that found in other worlds, and hence make it appropriate to make 
inductive inferences more quickly. Second, the environments that the rational animals evolved in may have greater or 
lesser uniformity, despite the same laws of nature. Third, the ecological niche occupied by the rational animals may
punish falsehood more or may reward truth more. ??explain with examples  Because of this, the Aristotelian 
species-relative answer to the Mersenne questions is particularly appealing.

\section{Logics of induction}
Attempts have been made to give precise answers to the questions about the reasonableness of inductive 
inferences using a rigorously formulated logics of induction.??refs Let us suppose, first, that some such
logic, call it $L_{12}$, does indeed embody the correct answers. Nonetheless, we will have a Mersenne question
as to why $L_{12}$, rather than one of the many alternatives, is the logic by which we ought to reason 
inductively. 

In the truthfunctional deductive case, there is a system that appears to be both particularly natural and matches
our intuitions so well that it has gained a nearly universal following among philosophers, logicians, mathematicians
and computer scientists: two-valued boolean logic. It is a sociological fact that no logic of induction has anything
like this following, and a plausible explanation of this sociological fact is that no logic of induction has the kind
of naturalness and fit with intuition that would privilege it over the others to a degree where it would seem
non-arbitrary to say that it is \textit{the} logic we should reason with.

Further, observe that logics of induction can be divided into two categories: those with parameters (say, 
parameters controlling the speed of inductive inference--??refs) and those without. 

A logic of induction with parameters raises immediate Mersenne problems about what grounds the fact about which 
parameters, or ranges of parameters, are in fact rationally correct. 

A parameter-free logic of induction, however, is not likely to do justice to the fact that different ways of balancing
rational goods are appropriate in different epistemic and pragmatic contexts. Moreover, it is unlikely to do justice
to the intuition that the balancing should be different in different species of rational agents.

\section{Goodman's new riddle of induction}
All the emeralds we've observed are green, and it's reasonable to infer that all emeralds are green.
But Goodman's famous riddle notes that all the emeralds we've observed are also grue, but it's not reasonable infer 
that all emeralds are grue. Here, an emerald is grue if it is observed before the year 2100 and green, or if it is 
blue and unobserved. According to Goodman, the predicate ``is green'' is \textit{projectible}, i.e., amenable to 
inductive inference, while the predicate ``is grue'' is not. But how do the two differ?

As Goodman notes, the fact that ``grue'' is defined in terms of ``green'' and ``blue'' does not help answer the
question. For if we specify that something is bleen if it is observed before 2100 and blue, or it is never observed
and blue, then we can define something to be green provided it is observed before 2100 and grue or never observed
and yet bleen, and similarly for ``blue'' with ``grue'' and ``bleen'' swapped. 

Whatever the \textit{justification} may be, it is clear that induction with ``green'' is reasonable, but not so with
``grue''. Notwithstanding Goodman's symmetry observations, ``grue'' is a gerrymandered predicate, as can be seen in
accounting for it in terms of more fundamental physical vocabulary. But now observe that ``green'' is also gerrymandered. 
An object is green provided that the wavelength profile of its reflected, transmitted and/or emitted light is predominantly 
concentrated somewhere around $500$ to $570$~nm. The actual boundaries of that region are messy and appear vague, the measure of
predominant concentration is difficult to specify, and accounting for reflective, transmittive and emissive spectra
is a challenge. The full account in terms of more fundamental scientific terms will be complex and rather messy, though 
not as badly as in the case of ``grue'', which is more than twice as complex since it needs to account for blueness
and the rather messy date of ``2100'', which is quite a messy date in more fundamental physics units (perhaps 
Planck times since the beginning of the universe?). Where the boundary between non-projectible and projectible lies---what
counts as too gerrymandered for projectibility---is an excellent Mersenne question.

There is a very plausible way to measure the degree of gerrymandering of a predicate. We take a language the content of 
whose symbols are terms for fundamental physical concepts, or more generally concepts corresponding to fundamental joints in reality, 
and we look for the shortest possible formula logically equivalent to the predicate, and say that the predicate is 
gerrymandered in proportion to the length of this formula. It is indeed likely that by that measure ``is grue'' is more 
than twice as complex ``is green''.??ref:Lewis

But now notice something odd. Say something is ``pogatively charged'' if it is positively charged and observed before
$5\times 10^{60}$ Planck times or never observed and negatively charged. All the protons we have seen are pogatively charged.
But we should not conclude that all protons are pogatively charged. It seems that ``is pogatively charged'' is just as
unprojectible as ``is grue''. However, notice that by the formula length account, ``is green'' is more gerrymandered than ``is 
pogatively charged''. Pogative charge is much closer to the fundamental than colors. It seems, thus, that our Mersenne question
about the boundary between the non-projectible and projectible is not merely defined by a single number---a threshold such that
predicates definable with a length below that number are projectible. 

Perhaps, however, what is going on here is this. The hypothesis that all emeralds are grue cannot overcome the hypothesis that 
all emeralds are green, even though both fit with observation. Similarly, the hypothesis that all protons are pogatively charged 
cannot overcome the hypothesis that all protons are positively. So perhaps rather than an absolute concept of projectibility, we
have a relation of relative projectibility: ``is green'' is projectible relative to ``is grue'' and ``is grue'' is 
non-projectible relative to ``is green''. 

We can once again try to account for this in terms of the complexity of formulae. But now we need to compare the complexity of
two formulae. And where previously we had a single numerical threshold as our parameter of projectibility, we now have a threshold
and a new non-numerical parameter that specifies the mathematical way in which the complexities of the two terms are to be compared. 
This parameter specifies how we test against the threshold: the ratio of complexities, the difference in complexities, or some other
mathematical function of the two complexities?

Furthermore, while the idea of a language all of whose terms reflect fundamental joints in reality can be defended, the grammar
of the language will make a difference to the precise complexity measurements. For instance, if we have the fundamental predicates 
$Cx$, $Dx$ and $Ex$, then the complex formula expressing the predicate ``is $C$ as well as either $D$ or $E$'' will be 
$$
    Cx \And (Dx \Or Ex)
$$    
    in infix notation, and hence five times longer than the formula $Cx$ represening ``is $C$'', but in Polish notation 
    will be 
$$
    KCxADxEx
$$    
and hence only four times longer than $Cx$.

For a relative projectibility relation defined in terms of linguistic complexity, we thus have at least three free parameters,
each a fit subject for a Mersenne question: a threshold, a comparison function, and a grammar for the basic language.

But in fact we probably should not think of a binary projectible / non-projectible distinction, whether
relational or absolute. As Goodman himself observed??ref-in-https://www.jstor.org/stable/pdf/686416.pdf, what we have instead is a range of predicates that are more or less projectible. We have ``is green'' and
``is grue''. But we can also say that $x$ is grue$^*$ provided that $x$ is green and observed by a French speaker before 2100 or by a 
non-speaker of French before 2107, or not observed, and ``grue$^*$'' will be less projectible than ``is grue''. On the basis of our observations, the
probability that all emeralds is green is very high, and the probability that they are all grue or grue$^*$ is very low. But
nonetheless, the probability that they are grue is somewhat higher than that they are grue$^*$. After all, an alien conspiracy to
recolor emeralds upon observation with a sharp cut-off in one year seems a little bit less unlikely than one where the cut-off 
depends on whether the observer speaks French. Similarly, it makes sense to think of ``is green'' as less projectible than ``is positively
charged'', and of ``is cute'' as even less projectible. 

Projectibility now becomes a matter of degree. An advantage of this is that perhaps we no longer need to make it relational.
The reason for the superiority of the green-hypothesis to the grue-hypothesis and for the positive-charge-hypothesis to the
pogative-charge-hypothesis can be given in terms of the relationship between the degrees of projectibility. However, the cost
is that now we need a function from predicates to degrees of projectibility, and the choice of that function will have infinitely
many degrees of freedom.

\section{$^*$Epistemic value}??hard
Plausibly, the more sure you are of a truth, the better off epistemically you are, and similarly the more sure you are
of a falsehood, the worse off you are. 

But what exactly is the dependence of value on the degree of certainty? Fix some hypothesis $H$ and let $T(p)$ be the epistemic value of 
having degree of belief or credence $p$ (where $0\le p\le 1$) in $H$ if $H$ is in fact true and let $F(p)$ be the value of credence
$p$ in $H$ if $H$ is in fact false. The pair $T$ and $F$ is called an accuracy scoring rule in the literature.??ref 

We can put some plausible constraints on $T$ and $F$. First, $T(p)$ cannot decrease if $p$ increases,
and $F(p)$ cannot increase if $p$ decreases.\footnote{We might more strongly specify that $T(p)$ always strictly increases with $p$, and $T(p)$
strictly decreases. That is
plausible, but one might also have a view on which there is a finite number of discrete thresholds at which increase/decrease happens.}
But that still leaves infinitely many degrees of freedom for the selection of $T$ and $F$.

We can, however, make some progress if we reflect on expected values. If your current credence in $H$ is $p$, then by your lights
there is a probability $p$ of your having epistemic score $T(p)$ and a probability $1-p$ of your epistemic score being $F(p)$, so
your expected score is:
$$
    p T(p) + (1-p) F(p).
$$
Suppose now you consider doing something odd: without any evidence, brainwashing yourself to switch your credence from $p$ to some 
other value $p'$. By your current lights, the expected epistemic value of this switch is:
$$
    p T(p') + (1-p) F(p').
$$
And this shouldn't be higher that the expected epistemic value of your actual credence $p$. For surely by the lights of your
assignment of $p$ to $H$, no other credence assignment should be expected to do better. Indeed, if another credence assignment
$p'$ were expected to do better by the lights of $p$, then $p$ would be some kind of a ``cursed probability'', one such that
if you assign it to $H$, then immediately expected value reasoning pushes you to replace it with $p'$. This is not rational.
So, it is very plausible indeed that:
$$
    p T(p) + (1-p) F(p) \ge
    p T(p') + (1-p) F(p').
$$
If $T$ and $F$ satisfy this inequality for all $p$ and $p'$, we say that the pair $T$ and $F$ is a \textit{proper} scoring rule.
And if by the lights of the assignment of $p$ to $H$, that assignment has better expectation than any other, i.e., if the
inequality above is strict whenever $p\ne p'$, we say that the rule is \textit{strictly proper}.

Propriety reduces the degrees of freedom in the choice of scoring rule. Given any non-decreasing function $T$, there is a
function $F$ that is unique up to an additive constant such that the parir $T$ and $F$ is a proper scoring rule, and conversely given
any non-increasing function $F$, there is a $T$ unique up to an additive constant such that $T$ and $F$ is a proper scoring rule.???
Hence, once we have one of the two functions, the other is almost determined. However, at the same time, this result shows what
a profusion of proper scoring rules there is: for every non-decreasing function, there is a proper scoring rule that has that as 
its $T$ component.

The question of epistemic value assignment may seem purely theoretical. However, it has real-world ramifications. 
Suppose a scientist has attained a credence $p$ in a hypothesis $H$, and is considering which of two experiments to
perform. One experiment will very likely have a minor but real effect on the credence in $H$ (think here of a case
where you've gathered $1000$ data points, and you now have a chance of gathering $100$ more). The other will most
likely be turn out to be irrelevant to $H$, but there is a small chance that it will nearly conclusively establish
$H$ or its negation. For each experiment, the scientist can use their present credence assignments to estimate the
probabilities of the various epistemic outcomes, and can then estimate expected epistemic values of the outcomes.

It is well-known??ref that if the scoring rule is strictly proper, for each experiment that has potential relevance 
to $H$ (i.e., there is at least one outcome that has non-zero probability by the scientist's current lights and
learning which would affect the credence in $H$), the expected epistemic value of performing the experiment is
higher than the expected epistemic value of the \textit{status quo}. Thus if the experiments are cost-free, it is
always worth performing more experiments, as long as we agree that the appropriate scoring rule is strictly proper,
and it does not matter which strictly proper scoring rule we choose. But if in addition to deciding whether to perform
another experiment, the decision to be made is \textit{which} experiment to perform, then the choice of scoring rule
will indeed be important, with different strictly proper scoring rules yielding different decisions.??ref:fill-in

There are a number of mathematically elegant strictly proper scoring rules, such as the Brier quadratic score, the spherical score 
and the logarithmic score. Of these, the logarithmic score is the only that is a serious candidate for being \textit{the} correct
scoring rule, in the light of information-theoretic and other arguments (??ref:phil of sci paper). In our setting where we are
evaluating the value of a credence in a single proposition $H$, the logarithmic score is $T(r) = \log r$ and $F(r) = \log (1-r)$. 

However, there are also reasons to doubt that the logarithmic score is the One True Score. 
First, there is an immediate intuitive problem. If you are certain of a falsehood, your logarithmic score is $\log 0 = -\infty$, while
if you are certain of a truth, your score is $\log 1 = 0$. Now, while there is good reason to think that the disvalue of being
sure of a falsehood exceeds the value of being sure of a truth, it is somewhat implausible that it infinitely exceeds it. 

For the next two problems, note that logarithmic scores and the arguments for them only really come into their own when we are 
dealing with more than two propositions (in our above setting, we had $H$ and $\sim H$ are the only relevant possibilities). Suppose 
we are dealing with $n$ primitive possibilities or ``cells'', $\omega_1,\dots,\omega_n$ (say, the sides of an $n$-sided die), and that our agent 
has assigned credence $p_i$ to $\omega_i$. If in fact $\omega_i$ eventuates, the logarithmic score yields epistemic value 
$\log p_i$. 

One of the merits touted for the logarithmic score is that ???for how many cells??? (up to multiplicative and additive constants) it is 
the only proper score where the epistemic value depends only on the credence assigned to the cell that eventuates.  But this is also
a serious demerit. Suppose that you and I are trying to figure out how many jelly beans there are in a jar. Let's say that our range of
possibilities is between 1 and 1000. I look very quickly and assign equal probability $1/1000$ to each number. You count very carefully
and arrive at 390. But then you think that although you are really good at counting, you might be off by one. So you assign $998/1000$
to 390, and $1/1000$ to each of 389 and 391. It turns out that the number is 391. We both have the same logarithmic score, $\log (1/1000)$,
since we both assigned the same probability $1/1000$ to cell 391. But intuitively your assignment is much better than mine: you are better
off epistemically than I.

Finally, observe that in real life, credences are not consistent---do not satisfy the axioms of probability. And the logarithmic score
allows one to have extremely inconsistent credences and still do well. If I assign credence $1$ to \textit{every} possible outcome, I am
guaranteed to max out the logarithmic score no matter what. Thus one of the least rational credence assignments results in the best
possible score.

We now have two different approaches to the Mersenne questions about epistemic value and scoring rules. First, we could suppose that there
is such a thing as \textit{the} One True Score. Since only the logarithmic score seems significantly mathematically privileged over all
the other scores, and the logarithmic score is not the One True Score, there will be an appearance of contingency about the
One True Score even if there is one.

Second, we might suppose that just as rational people can differ in prudential preferences, they can differ in epistemic preferences.
Some may, for instance, have a strong sharpish preference for gaining near-certainty in truths, while being fairly indifferent whether their
credence in a truth is $0.6$ or $0.8$, as neither is that close to certainty. Others, on the other hand, may value increased certainty in
a gradual way, like the logarithmic rule does.

However, it is important to note that while there may be room for rational people to differ in epistemic preferences, there is reason to
think that there are rational constraints on epistemic preferences that go beyond formal conditions such as strict propriety, continuity
or symmetry---where the last is the condition that $T(p)=F(1-p)$. 

Let $T_0(x)=1000$ if $x\ge 0.999$, $T_0(x)=-1000000$ if $x \le 0.001$, and $T_0(x)=0$ otherwise. Let $F_0(x)=T_0(1-x)$.
Then the pair $T_0$ and $F_0$ is a symmetric and proper scoring rule.??ref

Consider now a scientist who adopts this scoring rule for some hypothesis $H$ of minor importance about some chemicals in 
her lab that she initially assigns credence $1/2$ to. She has a choice between two methods. She can use clunky machine $A$
that she has in her lab, which is guaranteed to give an answer to the question of whether $H$ is true, but for either
answer there is a $0.11\%$ chance that the answer is wrong. Or she can use spiffy new machine $B$ which has the slightly lower $0.09\%$ chance 
of error either way. The only problem is that her lab doesn't own machine $B$ and her grant can't offer the price. Her only 
hope for using machine $B$ is to go and buy a scratch-off lottery ticket which has a one in a million chance of yielding a prize exactly
sufficient to purchase machine $B$. However, because some chemicals involved in the experiment are expiring exactly in a week, and 
machine $A$ is slower than machine $B$ and takes exactly a week to run, if she is to use machine $A$, she needs to start right now
and doesn't have time to buy the lottery ticket. And once she starts up machine $A$, she can't transfer the experiment to
machine $B$. 

In other words, her choice is between using machine $A$, and then learning whether $H$ is true with a credence of
$0.9989$, or buying a lottery ticket, which gives her a one in a million chance of learning whether $H$ is true with
a credence of $0.9991$ and a $999,999$ out of a million chance of being no further ahead. Going for the second option
seems irrational if all that is at stake is epistemic value: the difference between $0.9989$ and $0.9991$ is just not
worth the fact that most likely going with the lottery route one won't learn anything about $H$. (If what was at stake
wasn't epistemic value but something pragmatic, then things could be different. We could imagine a law where some 
life-saving medication can be administered to a patient only if we have $0.9991$ confidence that it'll work, and then
there will be no practical difference between $1/2$ and $0.9989$, but a big one between $0.9989$ and $0.9991$.) 

But a scoring rule like the one described above prefers the lottery option. For the epistemic value of using 
machine $A$ is guaranteed to be zero since after using machine $A$, the scientist will have credence $0.9989$ or $0.0011$, 
depending on whether the result favors $H$ or not.

On the lottery option, however, conditionally on winning the lottery, the expected epistemic value will be:
$$
    (1/2)(0.9991 \cdot T(0.9991)+0.0009\cdot F(0.9991))+(1/2)(0.9991 \cdot F(0.0009) + 0.0009\cdot T(0.0009))=
$$
since it is equally likely given the scientist's priors that the machine will return a verdict for or against $H$,
which will result in a credence of $0.9991$ or $0.0009$, respectively, and in either case there will be a $0.0009$
chance that the verdict is erroneous. Since $F(0.0009)=T(0.9991)=1000$ and $T(0.0009)=F(0.9991)=-1000000$, it follows 
that the expected epistemic value, conditionally on winning the lottery, will be:
$$
    (1/2)(0.9991 \cdot 1000+0.0009\cdot (-1000000)+0.9991 \cdot 1000+0.0009\cdot (-1000000)) = 99.1 > 0.
$$
And if we multiply this by the $1/1000000$ chance of winning the lottery, we still have something positive, so
the expected epistemic value of playing the lottery with the plan of using machine $B$ is positive, while that of
using machine $A$ is zero. 

Thus, by considerations of epistemic value, the scientist with this scoring rule will prefer a $1/1000000$ chance of 
gaining credence $0.9991$ as to whether $H$ is true to a certainty of gaining the slightly lower credence $0.9989$.
This is not rational. 

Now, in the above example, our scoring rule while proper, symmetric and finite, was neither continuous nor strictly proper. 
However, we will show in the Appendix??ref that there is a sequence of continuous, strictly proper, finite and symmetric
scoring rules $T_n$ and $F_n$ such that $T(x)=\lim_{n\to\infty} T_n(x)$ and $F(x)=\lim_{n\to\infty} F_n(x)$ for all $x$.
If $n$ is large enough, then the pair $T_n$ and $F_n$ will require exactly the same decision from our scientist as 
$T$ and $F$ did, since the expected value of the expected $(T_n,F_n)$-scores of the two courses of action will converge
to the expected vlue of the expected $(T,F)$-scores.

Hence not all epistemic valuations that satisfy the plausible formal axioms are rationally acceptable, then we 
will have Mersenne questions about what grounds the further constraints on the epistemic valuations. These constraints
are likely to include messy prohibitions, with multiple degrees of freedom, on the kinds of sharp jumps that our
pathological scoring rule above exhibited.

Furthermore, things become more complicated when we consider that the epistemic value of a credence in a truth will
differ depending on the importance of that truth. Getting right whether mathematical entities exist or whether we are 
material or how life on earth started has much more epistemic value than getting right Napoleon's shoe size. Epistemic
value will thus not only be a function of credence and truth, but also of subject matter. Moreover, we will have 
further degrees of freedom concerning the operation of combining epistemic values for different propositions---addition
may seem a plausible operation, but the logarithmic and spherical rules are not combined additively across propositions.

We thus have multiple indicators of a contingency about epistemic value assignments. And there is good reason to think
that different forms of life are more suited to different epistemic value assignments. The most obvious aspect of this
is that once we move away from the toy case of assigning a value to one's epistemic attitude to a single proposition
and consider that attitudes to a large number of propositions need to be considered, it is obvious that the subject
matter of the propositions will affect how great a weight we give credences about them in the overall evaluation. And
the importance of subject matter obviously depends on the form of life. It is plausible that for intelligent agents whose
natural environment is more hostile it would be more fitting to have a greater epistemic 
value assigned to practical matters, while agents that have few natural enemies and can get food easily might more fittingly
have a greater epistemic value assigned to theoretical matters. One imagines here that intelligent antelope might be
properly expected to be less philosophical than intelligent elephants.

\section{Bayesianism}
\subsection{Priors}
From a Bayesian point of view, how induction works is determined by the probabilities prior to all evidence, the ur-priors.
Suppose, for instance, that I assign equal prior probability to every logically possible color sequence of observed ravens. 
For simplicity, suppose that there are only two colors, white and black.  I find out that there are a million ravens, and 
I observe a thousand of them, and find them all black. I am about to observe another raven. The probability that the next 
raven will be black will be $1/2$. For the sequence $B,...,B,W$ (with $1000$ $B$s) is just as likely as the sequence
$B,...,B,B$ (with $1001$ $B$s), and both sequences fit equally well with our observations. 

On the other hand, suppose I assigned probability $1/3$ to the hypothesis that all ravens are white, $1/3$ to all black,
and split the remaining $1/3$ equally among the $2^{1000000}-2$ multicolor sequences. My observation of the first $1000$
ravens then rules out the all-white  hypothesis. And it rules out most of the multicolor sequences: there are $2^{999000}-1$
multicolor sequences that start with $1000$ black ravens, which is a tiny fraction of the original $2^{1000000}-2$. Since as a 
good Bayesian I keep the ratios between the probabilities unchanged, each of the remaining multicolor sequences has 
$1/(2^{1000000}-2)$ of the probability of the all-black sequence, and since there are only $2^{999000}-1$ multicolor
sequences remaining compatible with the evidence, the ratio between the multicolor probability and the all-black probability
is $(2^{999000}-1)/(2^{1000000}-2)$ to $1$, or approximately $1$ to $2^{1000}$. Thus, we have overwhelming confirmation of 
the all-black probability, and hence an even more overwhelming confirmation of the hypothesis that the next raven will be black.

Other ways of dividing the probabilities between the hypotheses yield other results. Carnap??ref, for instance, had a division
that worked as follows. For each number $n$ between zero and a million we have the hypothesis $H_n$ that there are exactly 
$n$ black ravens, and Carnap proposed that all million-and-one of these hypotheses should have equal probability, and then 
each hypothesis $H_n$ is divided into equally likely subhypotheses specifying all the subhypotheses that make there be $n$
ravens. Thus, $H_0$ and $H_{1000000}$ have only one subhypothesis: there is only one way to have no-black or all-black. But
$H_1$ and $H_{999999}$ have a million subhypotheses each: there are a million options for which is the raven with the outlying
color. Using the same constant-ratio technique as before, after observing $1000$ black ravens, the chance that the next one is black 
will turn out to be approximately $0.999$, but the chance that all million are black will only be $0.001$. More generally, if there
are $N$ ravens, and the first $m$ of them have been observed to be black, and $n\ge m$, then the probability that the first $n$
will be black will be $(1+m)/(1+n)$.\footnote{Let $B_m$ be the claim that the first $m$ ravens are black. Then
$P(B_m)=\sum_{n=0}^{N-m} {{N-m \choose n}}/((N+1){N\choose m+n}) = \frac{1}{1+m}$.??why:Mathematica The probability that the first
$n$ are black given that the first $m$ are black where $n\ge m$ will then $P(B_n\mid B_m)=(1+m)/(1+n)$.} 
  Hence we have very reason to think that the \textit{next} raven is black,
but unless we have observed the bulk of the ravens, we won't have reason to think that all the ravens are black. 

Intuitively, while Carnapian probabilities support induction, they result in induction being too slow---it is only when we have observed the bulk of the cases
being a certain way that we get to conclude that they are all like that. My $1/3$--$1/3$--$1/3$ division is too fast. Even with
$1000$ black ravens having been observed, the probability of a white raven shouldn't be \textit{astronomically} small in the way
that $1/2^{1000}$ is. Reasonable priors, thus, yield a speed of induction somewhere between these. 

We can presumably come up with a formula for the priors which will fit with our intuitions of how fast induction should work.
For instance, we could take Carnap's setup, but increase the prior probability of the all-white and all-black raven hypotheses. 
But such an increase would be apt to involve one or more parameters. If the specific assignment of priors were rationally
required of us, then we would have the Mersenne question of why it is these and not some other very similar priors that are required.
And if there is a range of priors rationally permitted to us, then we would have Mersenne questions about the boundaries of
this range. 

Further, imagine beings other than us that inhabit a more Carnapian world than we do. While in our world, we have a significant number of natural kinds
that exhibit or fail to exhibit some basic property exceptionlessly---for instance, every electron is charged, and no photon 
has mass--in that world there are few such natural kinds. Instead, if we were to tabulate the frequencies of basic binary properties
in various natural populations---say, tabulating the frequency of blackness among ravens, charge among electrons, mass among photons---we 
would find the frequencies to be distributed uniformly between $0$ and $1$. In that world, Carnapian priors would lead to the truth faster
than the more induction-friendly priors that we have. And let us imagine that in that world we have intelligent beings who reason according
to Carnapian priors. Even if we happily grant that Carnapian priors are irrational for us, it seems plausible to think that they could be
rational for those beings. To insist that these Carnapians are irrational, because it would be irrational for us to have these priors,
seems akin to saying that bigamy would immoral for aliens who need three individuals to reproduce, or that there is something wrong with fish
because they lack lungs. 

Consideration of the rationality of induction thus once again reveals an appearance of contingency in the normative realm, which
once again yield an argument for an Aristotelian picture of human nature, where the rationally required priors or ranges of priors
are those that we are impelled to by our human nature.

But before we embrace this conclusion fully, we should consider two Bayesian challenges, from two opposed points of view.
The algorithmic Bayesian thinks that considerations of coding can yield a reasonable set of priors, while the 
the subjective Bayesian says that there are no constraints on the priors, and 

\subsection{Algorithmic priors}
Suppose, first, we have a language $L$ of finite sequences of symbols chosen from some finite alphabet of basic symbols, 
with some of the sequences representing a member of some set $S$ of situations. 

For instance, $S$ could be arrangements of chess pieces on a board\footnote{The arrangements contain less information
than chess positions, since a chess position includes other information, such as whether a given king or rook has already
moved, whose turn it is, as well as historical information needed for adjudicating draws.}, and $L$ could be a declarative 
first-order language with no quantifiers, twelve piece predicates (specifying both the color and piece type) and sixty-four names of squares.
We could then say that a symbol sequence represents an arrangement $a$ provided that the sequence is a 
syntactically valid sentence that is true of $a$ and of no other arrangement. However, in general $L$ need not be a declarative
language. It could, for instance, be an imperative computer language for an abstract Turing machine or a physical computer,
and the situations could be possible outputs of that machine. Then we might say that a symbol sequence $s$ represents
a possible output $a$ just in case $s$ is a program that, when run, halts with the output being $a$. Or if we like we might 
add an additional layer of representation between the outputs of the machine and the situations---for instance, the outputs
of an abstract Turing machine might represent the physical arrangement of particles in a universe.  
We can even chain languages. For instance, we could have a computer language $L_1$, with the outputs being sequences of
symbols in some declarative language $L_2$, whose sentences in turn represent members of a set $S$ of situations.

Next, consider a natural way of choosing at random a finite sequence of symbols of $L$. Here is one. Add to $L$'s finite alphabet 
a new ``end'' symbol. Then randomly and independently, with each symbol being equally likely (i.e., having probability
$1/(n+1)$, where $n$ is the number of non-end symbols), choose a sequence of
symbols until you hit the end symbol. The sequence preceding the ``end'' symbol will then count as the randomly
selected sequence in $L$. Every sequence of length $k$ has probability $1/(n+1)^k$, so the probabilities decrease 
exponentially with the length of the sqeuence. We repeat the random selection process until we get a sequence that is both syntactically correct
and represents a situation in $S$.\footnote{If at least one finite sequence is syntactically correct and represents a 
situation in $S$, then with probability one, we will eventually get to a sequence that syntactically correctly represents some 
sequence.}  We now stipulate that the prior probability of a situation $s$ is equal to the probability that the
above process will generate a sequence that represents $s$.

Alternately, we can formulate this as follows. If $a$ is a sequence of symbols of $L$ and $s$ is a situation in $S$, write
$R(a,s)$ if $a$ is syntactically correct and represents $s$, and let $R(a)$ be shorthand for the claim that $a$ syntactically
correctly represents $S$, i.e., $\exists s(s\in a\And R(a,s))$. Then if $A$ is a randomly chosen sequence of symbols of $L$,
we can define the prior probability $Q(s)$ of $s$ as the conditional probability that $A$ syntactically correct represents $s$ on the supposition
it syntactically correctly represents something, i.e.,
$$
    Q(s) = P(R(A,s)\mid R(A)) = \frac{P(R(A,s))}{P(R(A))}.
$$    
We can call these $L$-Solomonoff priors.

These priors favor situations that can be can be more briefly represented in $L$ over ones whose representations are long.
The effect of these priors depends heavily on the choice of language $L$ and how well it can compress some situations over
others. For instance, in our chess case, if we have no quantifiers, it is easy to see that any two piece arrangements with 
the same number of pieces will have equal prior probability, because each square's contents have to be separately specified. 
Thus, if we know that every square contains a pawn, and we have observed the first 63 of these pawns and found them all to be black, the 
probability that the 64th square will be black is still $1/2$. On the other hand, if quantifiers and identity are allowed into our
language, then the all-black-pawn situation can be briefly represented by
\ditem{all-black}{$\forall x(\operatorname{BlackPawn}(x))$}
(where the domain is
squares on the board), while the situation where squares $1,...,63$ have black pawns and square 64 has a white pawn is harder
to represent. We might, for instance, use a sentence like:
\ditem{63-black}{$\forall x(\Not (x=64)\rightarrow \operatorname{BlackPawn}(x))\And \operatorname{WhitePawn}(x)$.}

Since the probability of generating a given sequence of symbols decreases exponentially with the number of symbols, \ditem{63-black}
is much less likely to be randomly generated than \ditem{all-black}, and it is intuitively very likely (though proving this rigorously would
be quite difficult???refs) that in general the probability of generating a sentence representing 64 black pawns is higher than that of 
generating a sentence representing 63 black pawns followed by one white pawn. We can thus expect that the conditional probability of
the 64th pawn being black on the first 63 being black to be very high. (Maybe even too high? It is very difficult to get good
estimates here, because there are many ways that a single situation can be represented. ??refs)

Just as in the case of using linguistic complexity to quantify projectibility, the choice of language here provides many Mersenne
questions. If we opt for the algorithmic version of the theory, we need to choose some computer language for a real or abstract
computer, and then we need to choose a representation map between outputs and situations in the external world, with infinitely
many possible candidates. And on the more
descriptive versions, we still need to choose a language, with many decision points as to syntax and vocabulary. It is very unlikely 
that there is a privileged language. And not
every will fit with our intuitions about induction. For instance, we can easily create a language, whether algorithmic or descriptive,
where 63 squares of black pawns followed by one square with a white pawn are much more briefly describable than 64 squares with
black pawns. For instance, on the descriptive side, we might use a $\operatorname{BlitePawn}(x)$ predicate, where something is a blite pawn 
provided it is a black pawn and on one of the first 63 squares or a white pawn and on the 64th, and an analogous $\operatorname{WhackPawn}(x)$.

In the unlikely case that there is a privileged language $L$ such that $L$-Solomonoff priors are rationally required for us,
we will have a vast number of Mersenne questions about the various parameters of the language and its representation relation.
In the more plausible case that there is a set of languages such that we are required to have $L$-Solomonoff priors for some $L$
in the set, we will have a vast number of Mersenne questions about the parameters that control the range of languages. All of this
gives rise to a significant degree of appearance of contingency.

Consider, too, the following observation. It is implausible that the languages defining rational priors for us
should be ones that are completely beyond our ken. But, on the other hand, it is plausible that there are possible
languages that to the smartest human are as incomprehensible as one of the less intuitive computer languages like
Haskell or Verilog or one of the creations of logicians further from natural language like lambda-calculus is to a typical
six-year-old. Imagine now beings to whom such these languages beyond human ken are easy. It \textit{could} be the case that
the norms for rational priors for them are formulated in terms of $L$-Solomonoff priors for one of the ``baby languages''
that humans can understand, but this does not seem a particularly plausible thesis. It seems more likely that for those
beings, the algorithmic rational priors would be different than for us. 

The standard way??ref to defend algorithmic measures of complexity from the problems presented by a plurality of languages
is to observe that sufficiently sophisticated languages have translational resources. Thus, one can write a Haskell interpreter
in Javascript, and so anything that can be expressed in Haskell can be expressed in Javascript by including the code for a Haskell interpreter, 
and then using a string constant that contains the Haskell code. The result is that the difference in the length of code needed to
generate a given output in different computer language will typically not be more than an additive constant: if one can
produce the output with Haskell code in $n$ bytes, then one can produce it in approximately\footnote{The approximation is due
to complications due to having to embed code in a string constant, which may involve various escape characters.} $n+k_{H,J}$ 
bytes in Javascript, where $k_{H,J}$ is the length of the Haskell interpreter in Javascript. For large enough $n$, the additive constant will be
unimportant. If we are to measure the complexity of a one-hour broadcast-quality video by the length of code needed to compress 
the  video, the addition of $k_{H,J}$ will likely be negligible: a Haskell interpreter is about half a megabyte, while 
an hour of video compressed losslessly can be reasonably expected to be in the gigabytes.

However, two points need to be made in our epistemological context. First, even if two different languages give very similar sets of priors, 
if they give even slightly different priors, we either have the Mersenne question of what makes one of these sets of priors be the 
objectively correct one, or we have the Mersenne question about the boundaries of the range of permissible languages. Second,
unlike in the case where we are measuring the complexity of a large set of data, such as a video file, in the inductive cases we 
need to look at ways of expressing relatively simple statements, such as ``All electrons are negatively charged'' or 
Schr\"odinger's equation. But for such statements, a translation manual is apt to dwarf the length of the translated text.
To say ``All electrons are negatively charged'' in French by first describing how English works, and then saying that this
description should be applied to the English sentence, will produce a French sentence that is many orders of magnitude longer
than the English one, and hence not a sentence that is relevant to measuring the prior probability that all electrons are negatively
charged.

Finally, while there is something elegant and natural about randomly choosing items in $L$ by randomly choosing within the
set of symbols with an end marker added, there are other ways to proceed. For instance, instead of making the end marker
equally likely as each of the ordinary symbols, one could at each step of generation flip a fair coin. On heads, one is done
generating. On tails, one then uniformly randomly chooses one of the $n$ symbols.\footnote{This will actually not make a 
difference in those languages where the syntax already determines where a syntactically valid sequence ends. This will be the
case with some Polish notation languages, where a valid sequence ends when the main operator is filled out with arguments.}
Or one might first randomly choose a positive integer specifying the length of the sequence of symbols, and then make all the
sequences of that specified length be equally likely. The particular method of random choice then yields more Mersenne questions,
though perhaps the reader will be convinced that the earlier given method is somehow natural and privileged.

\subsection{Subjective Bayesianism}



%% ?? ref: https://link.springer.com/article/10.1007/s11098-019-01367-0?utm_source=toc
%% ?? indifference



\section{Testimony}
\section{Infinity, self-indication and other limitations of Bayesianism}

\section*{Appendix: $^*$Approximating the pathological scoring rule with continuous ones}
We need to show that the stepwise scoring rule $(T,F)$ from ??backref can be written as a limit of
symmetric, strictly proper, finite and continuous scoring rules. 

First note that any symmetric continuous proper scoring rule $(t,f)$ can be written as the limit of symmetric continuous
strictly proper scoring rules by letting $t_n(x)=t(x)-(1-x)^2/n$ and $f_n(x)=f(x)-x^2/n$, since the Brier scoring rule defined
by the functions $-(1-x)^2$ an $-x^2$ is strictly proper, and the sum of a proper and a strictly proper scoring rules is
strictly proper. 

Thus, all we need to show is that is that we can approximate $(T,F)$ with symmetric, finite and continuous scoring rules.
Furthermore, we can drop the symmetry requirement. For write $f^*(x)=f(1-x)$. Then $(t,f)$ is a proper scoring rule if and
only if $(f^*,t^*)$ is a proper scoring rule. Now if $T(x)=\lim_n t_n(x)$ for all $x$ and $F(x)=\lim_n f_n(x)$, then
$$
    (T(x)+F^*(x))/2 = \lim_n (t_n(x)+f_n^*(x))/2
$$    
    and
$$    
    (F(x)+T^*(x))/2 = \lim_n (f_n(x)+t_n^*(x))/2. 
$$    
But $T=F^*$ and $F=T^*$, so the left-hand sides are just $T(x)$ and $F(x)$, respectively. Moreover, $(t_n+f_n^*,f_n+t_n^*)$ is 
will be a continuous symmetric finite proper scoring rule if $(t_n,f_n)$ is a continuous finite proper scoring rule.

Fix $\e>0$. Let $\phi_\e$ be a continuous non-negative finite function that is zero except on the set $U_\e=[0.999-\e,0.999)\cup (0.001,0.001+\e]$, 
and is such that $\int_{0.999-\e}^{0.999} (1-x) \phi_\e(x) = 1000$ and $\int_{0.001}^{0.001+\e} x \phi_\e(x) = 1000000$. 
Define 
$$
    T_n(x) = \int_{1/2}^x \, (1-u)phi_{1/n}(u) \, du
$$24
and 
$$
    F_n(x) = \int_{1/2}^x \, u \phi_{1/n}(u) \, du.
$$
By ??SchervishThm4.2, $(T_n,F_n)$ is proper. It is clearly continuous and finite. It is, further, easy to calculate that 
$T_n(x)$ and $F_n(x)$ equal $T(x)$ and $F(x)$ except perhaps on $U_{1/n}$.??check For any $x$ in $[0,1]$, there is an $N$
such that $x$ is not in $U_{1/n}$ for any $n\ge N$. It follows that $T(x)=T_n(x)$ and $F(x)=F_n(x)$ if $n\ge N$, and we have
the limiting condition we wanted.

\chaptertail 


