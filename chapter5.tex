\def\mychapter{V}
\input{chapterhead}
\chapter{Epistemology}\label{ch:epistemology}
\section{Balancing doxastic desiderata}
I observe one raven, and it's black. I observe another and it's black, too. The story goes on. Every
raven I observe is black. After a certain number of ravens, in a sufficiently broad number of settings,
it becomes reasonable to believe that all ravens are black. But when?\footnote{I am grateful to Sherif Girgis for raising the issue of incommensurable desiderata in connection with these issues.}

William James famously identified two incommensurable doxastic desiderata: attainment of truth and avoidance of falsehood.
The larger the number of black ravens that are needed for me to believe that all ravens are black, the more surely
I avoid falsehood, but the more slowly I attain truth. Intuitively, there is room for differences between reasonable
people: some tend to jump to conclusions more quickly, while others are more apt to suspend judgment. But on either
extreme, eventually we reach unreasonableness. Both someone who concludes that all ravens are black based on one 
observation and someone who continues to suspend judgment after a million broadly spread observations are unreasonable.

There is, thus, a range of reasonable levels of evidence for an inductive belief. And, as in the myriad of ethical
cases of Chapter~\ref{ch:ethics}, this raises the Mersenne question: What grounds facts about the minimum amount of 
evidence required for an inductive inference and the maximum amount at which suspending judgment is still rational? 
Of course, the ``minimum'' and ``maximum'' may may depend on the subject matter, on higher-order evidence such as about 
how well previous inductive generalizations have fared, and even on pragmatic factors(??ref). But that added complexity does
nothing to make the Mersenne question easier to answer. And, as we discussed in ??backref, invoking vagueness does not
solve the problem, but multiplies the complexity even further.

And, of course, my contention will be that conformity to the human form is what grounds the answers for us. The
rational way to reason is the way conforms to our form's specification of the proper functioning of our intellect.

It appears to be quite plausible that different answers to the rationality questions would be appropriate
for species of rational animals adapted to different environments. First, some possible worlds as a whole have 
laws of nature implying a greater uniformity than that found in other worlds, and hence make it appropriate to make 
inductive inferences more quickly. Second, the environments that the rational animals evolved in may have greater or 
lesser uniformity, despite the same laws of nature. Third, the ecological niche occupied by the rational animals may
punish falsehood more or may reward truth more. ??explain with examples  Because of this, the Aristotelian 
species-relative answer to the Mersenne questions is particularly appealing.

\section{Logics of induction}
Attempts have been made to give precise answers to the questions about the reasonableness of inductive 
inferences using a rigorously formulated logics of induction.??refs Let us suppose, first, that some such
logic, call it $L_{12}$, does indeed embody the correct answers. Nonetheless, we will have a Mersenne question
as to why $L_{12}$, rather than one of the many alternatives, is the logic by which we ought to reason 
inductively. 

In the truthfunctional deductive case, there is a system that appears to be both particularly natural and matches
our intuitions so well that it has gained a nearly universal following among philosophers, logicians, mathematicians
and computer scientists: two-valued boolean logic. It is a sociological fact that no logic of induction has anything
like this following, and a plausible explanation of this sociological fact is that no logic of induction has the kind
of naturalness and fit with intuition that would privilege it over the others to a degree where it would seem
non-arbitrary to say that it is \textit{the} logic we should reason with.

Further, observe that logics of induction can be divided into two categories: those with parameters (say, 
parameters controlling the speed of inductive inference--??refs) and those without. 

A logic of induction with parameters raises immediate Mersenne problems about what grounds the fact about which 
parameters, or ranges of parameters, are in fact rationally correct. 

A parameter-free logic of induction, however, is not likely to do justice to the fact that different ways of balancing
rational goods are appropriate in different epistemic and pragmatic contexts. Moreover, it is unlikely to do justice
to the intuition that the balancing should be different in different species of rational agents.

\section{Goodman's new riddle of induction}
All the emeralds we've observed are green, and it's reasonable to infer that all emeralds are green.
But Goodman's famous riddle notes that all the emeralds we've observed are also grue, but it's not reasonable infer 
that all emeralds are grue. Here, an emerald is grue if it is observed before the year 2100 and green, or if it is 
blue and unobserved. According to Goodman, the predicate ``is green'' is \textit{projectible}, i.e., amenable to 
inductive inference, while the predicate ``is grue'' is not. But how do the two differ?

As Goodman notes, the fact that ``grue'' is defined in terms of ``green'' and ``blue'' does not help answer the
question. For if we specify that something is bleen if it is observed before 2100 and blue, or it is never observed
and blue, then we can define something to be green provided it is observed before 2100 and grue or never observed
and yet bleen, and similarly for ``blue'' with ``grue'' and ``bleen'' swapped. 

Whatever the \textit{justification} may be, it is clear that induction with ``green'' is reasonable, but not so with
``grue''. Notwithstanding Goodman's symmetry observations, ``grue'' is a gerrymandered predicate, as can be seen in
accounting for it in terms of more fundamental physical vocabulary. But now observe that ``green'' is also gerrymandered. 
An object is green provided that the wavelength profile of its reflected, transmitted and/or emitted light is predominantly 
concentrated somewhere around $500$ to $570$~nm. The actual boundaries of that region are messy and appear vague, the measure of
predominant concentration is difficult to specify, and accounting for reflective, transmittive and emissive spectra
is a challenge. The full account in terms of more fundamental scientific terms will be complex and rather messy, though 
not as badly as in the case of ``grue'', which is more than twice as complex since it needs to account for blueness
and the rather messy date of ``2100'', which is quite a messy date in more fundamental physics units (perhaps 
Planck times since the beginning of the universe?). Where the boundary between non-projectible and projectible lies---what
counts as too gerrymandered for projectibility---is an excellent Mersenne question.

There is a very plausible way to measure the degree of gerrymandering of a predicate. We take a language the content of 
whose symbols are terms for fundamental physical concepts, or more generally concepts corresponding to fundamental joints in reality, 
and we look for the shortest possible formula logically equivalent to the predicate, and say that the predicate is 
gerrymandered in proportion to the length of this formula. It is indeed likely that by that measure ``is grue'' is more 
than twice as complex ``is green''.??ref:Lewis

But now notice something odd. Say something is ``pogatively charged'' if it is positively charged and observed before
$5\times 10^{60}$ Planck times or never observed and negatively charged. All the protons we have seen are pogatively charged.
But we should not conclude that all protons are pogatively charged. It seems that ``is pogatively charged'' is just as
unprojectible as ``is grue''. However, notice that by the formula length account, ``is green'' is more gerrymandered than ``is 
pogatively charged''. Pogative charge is much closer to the fundamental than colors. It seems, thus, that our Mersenne question
about the boundary between the non-projectible and projectible is not merely defined by a single number---a threshold such that
predicates definable with a length below that number are projectible. 

Perhaps, however, what is going on here is this. The hypothesis that all emeralds are grue cannot overcome the hypothesis that 
all emeralds are green, even though both fit with observation. Similarly, the hypothesis that all protons are pogatively charged 
cannot overcome the hypothesis that all protons are positively. So perhaps rather than an absolute concept of projectibility, we
have a relation of relative projectibility: ``is green'' is projectible relative to ``is grue'' and ``is grue'' is 
non-projectible relative to ``is green''. 

We can once again try to account for this in terms of the complexity of formulae. But now we need to compare the complexity of
two formulae. And where previously we had a single numerical threshold as our parameter of projectibility, we now have a threshold
and a new non-numerical parameter that specifies the mathematical way in which the complexities of the two terms are to be compared. 
This parameter specifies how we test against the threshold: the ratio of complexities, the difference in complexities, or some other
mathematical function of the two complexities?

Furthermore, while the idea of a language all of whose terms reflect fundamental joints in reality can be defended, the grammar
of the language will make a difference to the precise complexity measurements. For instance, if we have the fundamental predicates 
$Cx$, $Dx$ and $Ex$, then the complex formula expressing the predicate ``is $C$ as well as either $D$ or $E$'' will be 
$$
    Cx \And (Dx \Or Ex)
$$    
    in infix notation, and hence five times longer than the formula $Cx$ represening ``is $C$'', but in Polish notation 
    will be 
$$
    KCxADxEx
$$    
and hence only four times longer than $Cx$.

For a relative projectibility relation defined in terms of linguistic complexity, we thus have at least three free parameters,
each a fit subject for a Mersenne question: a threshold, a comparison function, and a grammar for the basic language.

But in fact we probably should not think of a binary projectible / non-projectible distinction, whether
relational or absolute. What we have instead is a range of predicates that are more or less projectible. We have ``is green'' and
``is grue''. But we can also say that $x$ is grue$^*$ provided that $x$ is green and observed by a French speaker before 2100 or by a 
non-speaker of French before 2107, or not observed, and ``grue$^*$'' will be less projectible than ``is grue''. On the basis of our observations, the
probability that all emeralds is green is very high, and the probability that they are all grue or grue$^*$ is very low. But
nonetheless, the probability that they are grue is somewhat higher than that they are grue$^*$. After all, an alien conspiracy to
recolor emeralds upon observation with a sharp cut-off in one year seems a little bit less unlikely than one where the cut-off 
depends on whether the observer speaks French. Similarly, it makes sense to think of ``is green'' as less projectible than ``is positively
charged'', and of ``is cute'' as even less projectible. 

Projectibility now becomes a matter of degree. An advantage of this is that perhaps we no longer need to make it relational.
The reason for the superiority of the green-hypothesis to the grue-hypothesis and for the positive-charge-hypothesis to the
pogative-charge-hypothesis can be given in terms of the relationship between the degrees of projectibility. However, the cost
is that now we need a function from predicates to degrees of projectibility, and the choice of that function will have infinitely
many degrees of freedom.

\section{$^*$Epistemic value}??hard
Plausibly, the more sure you are of a truth, the better off epistemically you are, and similarly the more sure you are
of a falsehood, the worse off you are. 

But what exactly is the dependence of value on the degree of certainty? Fix some hypothesis $H$ and let $T(p)$ be the epistemic value of 
having degree of belief or credence $p$ (where $0\le p\le 1$) in $H$ if $H$ is in fact true and let $F(p)$ be the value of credence
$p$ in $H$ if $H$ is in fact false. The pair $T$ and $F$ is called an accuracy scoring rule in the literature.??ref 

We can put some plausible constraints on $T$ and $F$. First, $T(p)$ cannot decrease if $p$ increases,
and $F(p)$ cannot increase if $p$ decreases.\footnote{We might more strongly specify that $T(p)$ always strictly increases with $p$, and $T(p)$
strictly decreases. That is
plausible, but one might also have a view on which there is a finite number of discrete thresholds at which increase/decrease happens.}
But that still leaves infinitely many degrees of freedom for the selection of $T$ and $F$.

We can, however, make some progress if we reflect on expected values. If your current credence in $H$ is $p$, then by your lights
there is a probability $p$ of your having epistemic score $T(p)$ and a probability $1-p$ of your epistemic score being $F(p)$, so
your expected score is:
$$
    p T(p) + (1-p) F(p).
$$
Suppose now you consider doing something odd: without any evidence, brainwashing yourself to switch your credence from $p$ to some 
other value $p'$. By your current lights, the expected epistemic value of this switch is:
$$
    p T(p') + (1-p) F(p').
$$
And this shouldn't be higher that the expected epistemic value of your actual credence $p$. For surely by the lights of your
assignment of $p$ to $H$, no other credence assignment should be expected to do better. Indeed, if another credence assignment
$p'$ were expected to do better by the lights of $p$, then $p$ would be some kind of a ``cursed probability'', one such that
if you assign it to $H$, then immediately expected value reasoning pushes you to replace it with $p'$. This is not rational.
So, it is very plausible indeed that:
$$
    p T(p) + (1-p) F(p) \ge
    p T(p') + (1-p) F(p').
$$
If $T$ and $F$ satisfy this inequality for all $p$ and $p'$, we say that the pair $T$ and $F$ is a \textit{proper} scoring rule.
And if by the lights of the assignment of $p$ to $H$, that assignment has better expectation than any other, i.e., if the
inequality above is strict whenever $p\ne p'$, we say that the rule is \textit{strictly proper}.

Propriety reduces the degrees of freedom in the choice of scoring rule. Given any non-decreasing function $T$, there is a
function $F$ that is unique up to an additive constant such that the parir $T$ and $F$ is a proper scoring rule, and conversely given
any non-increasing function $F$, there is a $T$ unique up to an additive constant such that $T$ and $F$ is a proper scoring rule.???
Hence, once we have one of the two functions, the other is almost determined. However, at the same time, this result shows what
a profusion of proper scoring rules there is: for every non-decreasing function, there is a proper scoring rule that has that as 
its $T$ component.

The question of epistemic value assignment may seem purely theoretical. However, it has real-world ramifications. 
Suppose a scientist has attained a credence $p$ in a hypothesis $H$, and is considering which of two experiments to
perform. One experiment will very likely have a minor but real effect on the credence in $H$ (think here of a case
where you've gathered $1000$ data points, and you now have a chance of gathering $100$ more). The other will most
likely be turn out to be irrelevant to $H$, but there is a small chance that it will nearly conclusively establish
$H$ or its negation. For each experiment, the scientist can use their present credence assignments to estimate the
probabilities of the various epistemic outcomes, and can then estimate expected epistemic values of the outcomes.

It is well-known??ref that if the scoring rule is strictly proper, for each experiment that has potential relevance 
to $H$ (i.e., there is at least one outcome that has non-zero probability by the scientist's current lights and
learning which would affect the credence in $H$), the expected epistemic value of performing the experiment is
higher than the expected epistemic value of the \textit{status quo}. Thus if the experiments are cost-free, it is
always worth performing more experiments, as long as we agree that the appropriate scoring rule is strictly proper,
and it does not matter which strictly proper scoring rule we choose. But if in addition to deciding whether to perform
another experiment, the decision to be made is \textit{which} experiment to perform, then the choice of scoring rule
will indeed be important, with different strictly proper scoring rules yielding different decisions.??ref:fill-in

There are a number of mathematically elegant strictly proper scoring rules, such as the Brier quadratic score, the spherical score 
and the logarithmic score. Of these, the logarithmic score is the only that is a serious candidate for being \textit{the} correct
scoring rule, in the light of information-theoretic and other arguments (??ref:phil of sci paper). In our setting where we are
evaluating the value of a credence in a single proposition $H$, the logarithmic score is $T(r) = \log r$ and $F(r) = \log (1-r)$. 

However, there are also reasons to doubt that the logarithmic score is the One True Score. 
First, there is an immediate intuitive problem. If you are certain of a falsehood, your logarithmic score is $\log 0 = -\infty$, while
if you are certain of a truth, your score is $\log 1 = 0$. Now, while there is good reason to think that the disvalue of being
sure of a falsehood exceeds the value of being sure of a truth, it is somewhat implausible that it infinitely exceeds it. 

For the next two problems, note that logarithmic scores and the arguments for them only really come into their own when we are 
dealing with more than two propositions (in our above setting, we had $H$ and $\sim H$ are the only relevant possibilities). Suppose 
we are dealing with $n$ primitive possibilities or ``cells'', $\omega_1,\dots,\omega_n$ (say, the sides of an $n$-sided die), and that our agent 
has assigned credence $p_i$ to $\omega_i$. If in fact $\omega_i$ eventuates, the logarithmic score yields epistemic value 
$\log p_i$. 

One of the merits touted for the logarithmic score is that ???for how many cells??? (up to multiplicative and additive constants) it is 
the only proper score where the epistemic value depends only on the credence assigned to the cell that eventuates.  But this is also
a serious demerit. Suppose that you and I are trying to figure out how many jelly beans there are in a jar. Let's say that our range of
possibilities is between 1 and 1000. I look very quickly and assign equal probability $1/1000$ to each number. You count very carefully
and arrive at 390. But then you think that although you are really good at counting, you might be off by one. So you assign $998/1000$
to 390, and $1/1000$ to each of 389 and 391. It turns out that the number is 391. We both have the same logarithmic score, $\log (1/1000)$,
since we both assigned the same probability $1/1000$ to cell 391. But intuitively your assignment is much better than mine: you are better
off epistemically than I.

Finally, observe that in real life, credences are not consistent---do not satisfy the axioms of probability. And the logarithmic score
allows one to have extremely inconsistent credences and still do well. If I assign credence $1$ to \textit{every} possible outcome, I am
guaranteed to max out the logarithmic score no matter what. Thus one of the least rational credence assignments results in the best
possible score.

We now have two different approaches to the Mersenne questions about epistemic value and scoring rules. First, we could suppose that there
is such a thing as \textit{the} One True Score. Since only the logarithmic score seems significantly mathematically privileged over all
the other scores, and the logarithmic score is not the One True Score, there will be an appearance of contingency about the
One True Score even if there is one.

Second, we might suppose that just as rational people can differ in prudential preferences, they can differ in epistemic preferences.
Some may, for instance, have a strong sharpish preference for gaining near-certainty in truths, while being fairly indifferent whether their
credence in a truth is $0.6$ or $0.8$, as neither is that close to certainty. Others, on the other hand, may value increased certainty in
a gradual way, like the logarithmic rule does.

However, it is important to note that while there may be room for rational people to differ in epistemic preferences, there is reason to
think that there are rational constraints on epistemic preferences that go beyond formal conditions such as strict propriety, continuity
or symmetry---where the last is the condition that $T(p)=F(1-p)$. One way to see this is to give an example of a scoring rule that is
strictly proper, continuous and symmetrical, but nonetheless we would judge someone who adopted it as not rational. For concreteness,
let $n$ be a positive even integer. Let:
$$
    T_n(p) = (2p-1)^n + 2n (1-p) (2p-1)^{n-1} = (2p-1)^{n-1}(2n-1+2(1-n)p)
$$
and
$$  
    F_n(p) = T(1-p).
$$
This is a strictly proper, continuous and symmetrical scoring rule.\footnote{This scoring rule
is derived using a standard method from the strictly convex function $(x-y)^n$??ref and hence is strictly convex.}
Now suppose that $n$ is large, say $1000$. The function $T_n$ is increasing (this follows from strict propriety??ref). 
We can calculate some values:
$$
    T_{1000}(0) = -1999 = F_{1000}(1)
$$
$$
    T_{1000}(0.001) \approx -270 \approx F_{1000}(0.999)
$$    
$$
    T_{1000}(0.01) \approx -0.000003 \approx F_{1000}(0.99)
$$
$$
    T_{1000}(0.5) = 0 = F_{1000}(0.5)
$$
$$    
    T_{1000}(1) = 1 = F_{1000}(1).
$$
This is a very implausible set of numbers. For instance, according to this scoring rule, being $0.99$ sure of a falsehood 
is barely worse than assigning a credence of $1/2$ to something (whether true or false), and relative to the disvalue of 
certainty in a falsehood there is hardly any value in certainty in a truth. Such a scoring rule makes one not care much 
whether one suspends judgment about a proposition or assigns a credence of $0.001$ or a credence of $0.999$, and that does
not appear to be a rational set of epistemic preferences.

But if some but not all epistemic valuations that satisfy the plausible formal axioms are rationally acceptable, then we 
will have Mersenne questions about what grounds the further constraints on the epistemic valuations.

Furthermore, things become more complicated when we consider that the epistemic value of a credence in a truth



\section{Priors}
\subsection{Indifference}
%% ref: https://link.springer.com/article/10.1007/s11098-019-01367-0?utm_source=toc
%% indifference


\section{Testimony}
\section{Infinity, self-indication and other limitations of Bayesianism}

\chaptertail 


