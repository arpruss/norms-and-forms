\def\mychapter{V}
\input{chapterhead}
\chapter{Epistemology}\label{ch:epistemology}
\section{Balancing doxastic desiderata}
I observe one raven, and it's black. I observe another and it's black, too. The story goes on. Every
raven I observe is black. After a certain number of ravens, in a sufficiently broad number of settings,
it becomes reasonable to believe that all ravens are black. But when?\footnote{I am grateful to Sherif Girgis for raising the issue of incommensurable desiderata in connection with these issues.}

William James famously identified two incommensurable doxastic desiderata: attainment of truth and avoidance of falsehood.
The larger the number of black ravens that are needed for me to believe that all ravens are black, the more surely
I avoid falsehood, but the more slowly I attain truth. Intuitively, there is room for differences between reasonable
people: some tend to jump to conclusions more quickly, while others are more apt to suspend judgment. But on either
extreme, eventually we reach unreasonableness. Both someone who concludes that all ravens are black based on one 
observation and someone who continues to suspend judgment after a million broadly spread observations are unreasonable.

There is, thus, a range of reasonable levels of evidence for an inductive belief. And, as in the myriad of ethical
cases of Chapter~\ref{ch:ethics}, this raises the Mersenne question: What grounds facts about the minimum amount of 
evidence required for an inductive inference and the maximum amount at which suspending judgment is still rational? 
Of course, the ``minimum'' and ``maximum'' may may depend on the subject matter, on higher-order evidence such as about 
how well previous inductive generalizations have fared, and even on pragmatic factors(??ref). But that added complexity does
nothing to make the Mersenne question easier to answer. And, as we discussed in ??backref, invoking vagueness does not
solve the problem, but multiplies the complexity even further.

And, of course, my contention will be that conformity to the human form is what grounds the answers for us. The
rational way to reason is the way conforms to our form's specification of the proper functioning of our intellect.

It appears to be quite plausible that different answers to the rationality questions would be appropriate
for species of rational animals adapted to different environments. First, some possible worlds as a whole have 
laws of nature implying a greater uniformity than that found in other worlds, and hence make it appropriate to make 
inductive inferences more quickly. Second, the environments that the rational animals evolved in may have greater or 
lesser uniformity, despite the same laws of nature. Third, the ecological niche occupied by the rational animals may
punish falsehood more or may reward truth more. ??explain with examples  Because of this, the Aristotelian 
species-relative answer to the Mersenne questions is particularly appealing.

\section{Logics of induction}
Attempts have been made to give precise answers to the questions about the reasonableness of inductive 
inferences using a rigorously formulated logics of induction.??refs Let us suppose, first, that some such
logic, call it $L_{12}$, does indeed embody the correct answers. Nonetheless, we will have a Mersenne question
as to why $L_{12}$, rather than one of the many alternatives, is the logic by which we ought to reason 
inductively. 

In the truthfunctional deductive case, there is a system that appears to be both particularly natural and matches
our intuitions so well that it has gained a nearly universal following among philosophers, logicians, mathematicians
and computer scientists: two-valued boolean logic. It is a sociological fact that no logic of induction has anything
like this following, and a plausible explanation of this sociological fact is that no logic of induction has the kind
of naturalness and fit with intuition that would privilege it over the others to a degree where it would seem
non-arbitrary to say that it is \textit{the} logic we should reason with.

Further, observe that logics of induction can be divided into two categories: those with parameters (say, 
parameters controlling the speed of inductive inference--??refs) and those without. 

A logic of induction with parameters raises immediate Mersenne problems about what grounds the fact about which 
parameters, or ranges of parameters, are in fact rationally correct. 

A parameter-free logic of induction, however, is not likely to do justice to the fact that different ways of balancing
rational goods are appropriate in different epistemic and pragmatic contexts. Moreover, it is unlikely to do justice
to the intuition that the balancing should be different in different species of rational agents.

\section{Goodman's new riddle of induction}
All the emeralds we've observed are green, and it's reasonable to infer that all emeralds are green.
But Goodman's famous riddle notes that all the emeralds we've observed are also grue, but it's not reasonable infer 
that all emeralds are grue. Here, an emerald is grue if it is observed before the year 2100 and green, or if it is 
blue and unobserved. According to Goodman, the predicate ``is green'' is \textit{projectible}, i.e., amenable to 
inductive inference, while the predicate ``is grue'' is not. But how do the two differ?

As Goodman notes, the fact that ``grue'' is defined in terms of ``green'' and ``blue'' does not help answer the
question. For if we specify that something is bleen if it is observed before 2100 and blue, or it is never observed
and blue, then we can define something to be green provided it is observed before 2100 and grue or never observed
and yet bleen, and similarly for ``blue'' with ``grue'' and ``bleen'' swapped. 

Whatever the \textit{justification} may be, it is clear that induction with ``green'' is reasonable, but not so with
``grue''. Notwithstanding Goodman's symmetry observations, ``grue'' is a gerrymandered predicate, as can be seen in
accounting for it in terms of more fundamental physical vocabulary. But now observe that ``green'' is also gerrymandered. 
An object is green provided that the wavelength profile of its reflected, transmitted and/or emitted light is predominantly 
concentrated somewhere around $500$ to $570$~nm. The actual boundaries of that region are messy and appear vague, the measure of
predominant concentration is difficult to specify, and accounting for reflective, transmittive and emissive spectra
is a challenge. The full account in terms of more fundamental scientific terms will be complex and rather messy, though 
not as badly as in the case of ``grue'', which is more than twice as complex since it needs to account for blueness
and the rather messy date of ``2100'', which is quite a messy date in more fundamental physics units (perhaps 
Planck times since the beginning of the universe?). Where the boundary between non-projectible and projectible lies---what
counts as too gerrymandered for projectibility---is an excellent Mersenne question.

There is a very plausible way to measure the degree of gerrymandering of a predicate. We take a language the content of 
whose symbols are terms for fundamental physical concepts, or more generally concepts corresponding to fundamental joints in reality, 
and we look for the shortest possible formula logically equivalent to the predicate, and say that the predicate is 
gerrymandered in proportion to the length of this formula. It is indeed likely that by that measure ``is grue'' is more 
than twice as complex ``is green''.??ref:Lewis

But now notice something odd. Say something is ``pogatively charged'' if it is positively charged and observed before
$5\times 10^{60}$ Planck times or never observed and negatively charged. All the protons we have seen are pogatively charged.
But we should not conclude that all protons are pogatively charged. It seems that ``is pogatively charged'' is just as
unprojectible as ``is grue''. However, notice that by the formula length account, ``is green'' is more gerrymandered than ``is 
pogatively charged''. Pogative charge is much closer to the fundamental than colors. It seems, thus, that our Mersenne question
about the boundary between the non-projectible and projectible is not merely defined by a single number---a threshold such that
predicates definable with a length below that number are projectible. 

Perhaps, however, what is going on here is this. The hypothesis that all emeralds are grue cannot overcome the hypothesis that 
all emeralds are green, even though both fit with observation. Similarly, the hypothesis that all protons are pogatively charged 
cannot overcome the hypothesis that all protons are positively. So perhaps rather than an absolute concept of projectibility, we
have a relation of relative projectibility: ``is green'' is projectible relative to ``is grue'' and ``is grue'' is 
non-projectible relative to ``is green''. 

We can once again try to account for this in terms of the complexity of formulae. But now we need to compare the complexity of
two formulae. And where previously we had a single numerical threshold as our parameter of projectibility, we now have a threshold
and a new non-numerical parameter that specifies the mathematical way in which the complexities of the two terms are to be compared. 
This parameter specifies how we test against the threshold: the ratio of complexities, the difference in complexities, or some other
mathematical function of the two complexities?

Furthermore, while the idea of a language all of whose terms reflect fundamental joints in reality can be defended, the grammar
of the language will make a difference to the precise complexity measurements. For instance, if we have the fundamental predicates 
$Cx$, $Dx$ and $Ex$, then the complex formula expressing the predicate ``is $C$ as well as either $D$ or $E$'' will be 
$$
    Cx \And (Dx \Or Ex)
$$    
    in infix notation, and hence five times longer than the formula $Cx$ represening ``is $C$'', but in Polish notation 
    will be 
$$
    KCxADxEx
$$    
and hence only four times longer than $Cx$.

For a relative projectibility relation defined in terms of linguistic complexity, we thus have at least three free parameters,
each a fit subject for a Mersenne question: a threshold, a comparison function, and a grammar for the basic language.

But in fact we probably should not think of a binary projectible / non-projectible distinction, whether
relational or absolute. As Goodman himself observed??ref-in-https://www.jstor.org/stable/pdf/686416.pdf, what we have instead is a range of predicates that are more or less projectible. We have ``is green'' and
``is grue''. But we can also say that $x$ is grue$^*$ provided that $x$ is green and observed by a French speaker before 2100 or by a 
non-speaker of French before 2107, or not observed, and ``grue$^*$'' will be less projectible than ``is grue''. On the basis of our observations, the
probability that all emeralds is green is very high, and the probability that they are all grue or grue$^*$ is very low. But
nonetheless, the probability that they are grue is somewhat higher than that they are grue$^*$. After all, an alien conspiracy to
recolor emeralds upon observation with a sharp cut-off in one year seems a little bit less unlikely than one where the cut-off 
depends on whether the observer speaks French. Similarly, it makes sense to think of ``is green'' as less projectible than ``is positively
charged'', and of ``is cute'' as even less projectible. 

Projectibility now becomes a matter of degree. An advantage of this is that perhaps we no longer need to make it relational.
The reason for the superiority of the green-hypothesis to the grue-hypothesis and for the positive-charge-hypothesis to the
pogative-charge-hypothesis can be given in terms of the relationship between the degrees of projectibility. However, the cost
is that now we need a function from predicates to degrees of projectibility, and the choice of that function will have infinitely
many degrees of freedom.

\section{Epistemic value}
\subsection{Epistemic value on its own}
Plausibly, the more sure you are of a truth, the better off epistemically you are, and similarly the more sure you are
of a falsehood, the worse off you are. 

But what exactly is the dependence of value on the degree of certainty? Fix some hypothesis $H$ and let $T(p)$ be the epistemic value of 
having degree of belief or credence $p$ (where $0\le p\le 1$) in $H$ if $H$ is in fact true and let $F(p)$ be the value of credence
$p$ in $H$ if $H$ is in fact false. The pair $T$ and $F$ is called an accuracy scoring rule in the literature.??ref 

We can put some plausible constraints on $T$ and $F$. First, $T(p)$ cannot decrease if $p$ increases,
and $F(p)$ cannot increase if $p$ decreases.\footnote{We might more strongly specify that $T(p)$ always strictly increases with $p$, and $T(p)$
strictly decreases. That is
plausible, but one might also have a view on which there is a finite number of discrete thresholds at which increase/decrease happens.}
But that still leaves infinitely many degrees of freedom for the selection of $T$ and $F$.

We can, however, make some progress if we reflect on expected values. If your current credence in $H$ is $p$, then by your lights
there is a probability $p$ of your having epistemic score $T(p)$ and a probability $1-p$ of your epistemic score being $F(p)$, so
your expected score is:
$$
    p T(p) + (1-p) F(p).
$$
Suppose now you consider doing something odd: without any evidence, brainwashing yourself to switch your credence from $p$ to some 
other value $p'$. By your current lights, the expected epistemic value of this switch is:
$$
    p T(p') + (1-p) F(p').
$$
And this shouldn't be higher that the expected epistemic value of your actual credence $p$. For surely by the lights of your
assignment of $p$ to $H$, no other credence assignment should be expected to do better. Indeed, if another credence assignment
$p'$ were expected to do better by the lights of $p$, then $p$ would be some kind of a ``cursed probability'', one such that
if you assign it to $H$, then immediately expected value reasoning pushes you to replace it with $p'$. This is not rational.
So, it is very plausible indeed that:
$$
    p T(p) + (1-p) F(p) \ge
    p T(p') + (1-p) F(p').
$$
If $T$ and $F$ satisfy this inequality for all $p$ and $p'$, we say that the pair $T$ and $F$ is a \textit{proper} scoring rule.
And if by the lights of the assignment of $p$ to $H$, that assignment has better expectation than any other, i.e., if the
inequality above is strict whenever $p\ne p'$, we say that the rule is \textit{strictly proper}.

Propriety reduces the degrees of freedom in the choice of scoring rule. Given any non-decreasing function $T$, there is a
function $F$ that is unique up to an additive constant such that the parir $T$ and $F$ is a proper scoring rule, and conversely given
any non-increasing function $F$, there is a $T$ unique up to an additive constant such that $T$ and $F$ is a proper scoring rule.???
Hence, once we have one of the two functions, the other is almost determined. However, at the same time, this result shows what
a profusion of proper scoring rules there is: for every non-decreasing function, there is a proper scoring rule that has that as 
its $T$ component.

The question of epistemic value assignment may seem purely theoretical. However, it has real-world ramifications. 
Suppose a scientist has attained a credence $p$ in a hypothesis $H$, and is considering which of two experiments to
perform. One experiment will very likely have a minor but real effect on the credence in $H$ (think here of a case
where you've gathered $1000$ data points, and you now have a chance of gathering $100$ more). The other will most
likely be turn out to be irrelevant to $H$, but there is a small chance that it will nearly conclusively establish
$H$ or its negation. For each experiment, the scientist can use their present credence assignments to estimate the
probabilities of the various epistemic outcomes, and can then estimate expected epistemic values of the outcomes.

It is well-known??ref that if the scoring rule is strictly proper, for each experiment that has potential relevance 
to $H$ (i.e., there is at least one outcome that has non-zero probability by the scientist's current lights and
learning which would affect the credence in $H$), the expected epistemic value of performing the experiment is
higher than the expected epistemic value of the \textit{status quo}. Thus if the experiments are cost-free, it is
always worth performing more experiments, as long as we agree that the appropriate scoring rule is strictly proper,
and it does not matter which strictly proper scoring rule we choose. But if in addition to deciding whether to perform
another experiment, the decision to be made is \textit{which} experiment to perform, then the choice of scoring rule
will indeed be important, with different strictly proper scoring rules yielding different decisions.??ref:fill-in

There are a number of mathematically elegant strictly proper scoring rules, such as the Brier quadratic score, the spherical score 
and the logarithmic score. Of these, the logarithmic score is the only that is a serious candidate for being \textit{the} correct
scoring rule, in the light of information-theoretic and other arguments (??ref:phil of sci paper). In our setting where we are
evaluating the value of a credence in a single proposition $H$, the logarithmic score is $T(r) = \log r$ and $F(r) = \log (1-r)$. 

However, there are also reasons to doubt that the logarithmic score is the One True Score. 
First, there is an immediate intuitive problem. If you are certain of a falsehood, your logarithmic score is $\log 0 = -\infty$, while
if you are certain of a truth, your score is $\log 1 = 0$. Now, while there is good reason to think that the disvalue of being
sure of a falsehood exceeds the value of being sure of a truth, it is somewhat implausible that it infinitely exceeds it. 

For the next two problems, note that logarithmic scores and the arguments for them only really come into their own when we are 
dealing with more than two propositions (in our above setting, we had $H$ and $\sim H$ are the only relevant possibilities). Suppose 
we are dealing with $n$ primitive possibilities or ``cells'', $\omega_1,\dots,\omega_n$ (say, the sides of an $n$-sided die), and that our agent 
has assigned credence $p_i$ to $\omega_i$. If in fact $\omega_i$ eventuates, the logarithmic score yields epistemic value 
$\log p_i$. 

One of the merits touted for the logarithmic score is that ???for how many cells??? (up to multiplicative and additive constants) it is 
the only proper score where the epistemic value depends only on the credence assigned to the cell that eventuates.  But this is also
a serious demerit. Suppose that you and I are trying to figure out how many jelly beans there are in a jar. Let's say that our range of
possibilities is between 1 and 1000. I look very quickly and assign equal probability $1/1000$ to each number. You count very carefully
and arrive at 390. But then you think that although you are really good at counting, you might be off by one. So you assign $998/1000$
to 390, and $1/1000$ to each of 389 and 391. It turns out that the number is 391. We both have the same logarithmic score, $\log (1/1000)$,
since we both assigned the same probability $1/1000$ to cell 391. But intuitively your assignment is much better than mine: you are better
off epistemically than I.

Finally, observe that in real life, credences are not consistent---do not satisfy the axioms of probability. And the logarithmic score
allows one to have extremely inconsistent credences and still do well. If I assign credence $1$ to \textit{every} possible outcome, I am
guaranteed to max out the logarithmic score no matter what. Thus one of the least rational credence assignments results in the best
possible score.

We now have two different approaches to the Mersenne questions about epistemic value and scoring rules. First, we could suppose that there
is such a thing as \textit{the} One True Score. Since only the logarithmic score seems significantly mathematically privileged over all
the other scores, and the logarithmic score is not the One True Score, there will be an appearance of contingency about the
One True Score even if there is one.

Second, we might suppose that just as rational people can differ in prudential preferences, they can differ in epistemic preferences.
Some may, for instance, have a strong sharpish preference for gaining near-certainty in truths, while being fairly indifferent whether their
credence in a truth is $0.6$ or $0.8$, as neither is that close to certainty. Others, on the other hand, may value increased certainty in
a gradual way, like the logarithmic rule does.

However, it is important to note that while there may be room for rational people to differ in epistemic preferences, there is reason to
think that there are rational constraints on epistemic preferences that go beyond formal conditions such as strict propriety, continuity
or symmetry---where the last is the condition that $T(p)=F(1-p)$. 

Let $T_0(x)=1000$ if $x\ge 0.999$, $T_0(x)=-1000000$ if $x \le 0.001$, and $T_0(x)=0$ otherwise. Let $F_0(x)=T_0(1-x)$.
Then the pair $T_0$ and $F_0$ is a symmetric and proper scoring rule.??ref

Consider now a scientist who adopts this scoring rule for some hypothesis $H$ of minor importance about some chemicals in 
her lab that she initially assigns credence $1/2$ to. She has a choice between two methods. She can use clunky machine $A$
that she has in her lab, which is guaranteed to give an answer to the question of whether $H$ is true, but for either
answer there is a $0.11\%$ chance that the answer is wrong. Or she can use spiffy new machine $B$ which has the slightly lower $0.09\%$ chance 
of error either way. The only problem is that her lab doesn't own machine $B$ and her grant can't offer the price. Her only 
hope for using machine $B$ is to go and buy a scratch-off lottery ticket which has a one in a million chance of yielding a prize exactly
sufficient to purchase machine $B$. However, because some chemicals involved in the experiment are expiring exactly in a week, and 
machine $A$ is slower than machine $B$ and takes exactly a week to run, if she is to use machine $A$, she needs to start right now
and doesn't have time to buy the lottery ticket. And once she starts up machine $A$, she can't transfer the experiment to
machine $B$. 

In other words, her choice is between using machine $A$, and then learning whether $H$ is true with a credence of
$0.9989$, or buying a lottery ticket, which gives her a one in a million chance of learning whether $H$ is true with
a credence of $0.9991$ and a $999,999$ out of a million chance of being no further ahead. Going for the second option
seems irrational if all that is at stake is epistemic value: the difference between $0.9989$ and $0.9991$ is just not
worth the fact that most likely going with the lottery route one won't learn anything about $H$. (If what was at stake
wasn't epistemic value but something pragmatic, then things could be different. We could imagine a law where some 
life-saving medication can be administered to a patient only if we have $0.9991$ confidence that it'll work, and then
there will be no practical difference between $1/2$ and $0.9989$, but a big one between $0.9989$ and $0.9991$.) 

But a scoring rule like the one described above prefers the lottery option. For the epistemic value of using 
machine $A$ is guaranteed to be zero since after using machine $A$, the scientist will have credence $0.9989$ or $0.0011$, 
depending on whether the result favors $H$ or not.

On the lottery option, however, conditionally on winning the lottery, the expected epistemic value will be:
$$
    (1/2)(0.9991 \cdot T(0.9991)+0.0009\cdot F(0.9991))+(1/2)(0.9991 \cdot F(0.0009) + 0.0009\cdot T(0.0009))=
$$
since it is equally likely given the scientist's priors that the machine will return a verdict for or against $H$,
which will result in a credence of $0.9991$ or $0.0009$, respectively, and in either case there will be a $0.0009$
chance that the verdict is erroneous. Since $F(0.0009)=T(0.9991)=1000$ and $T(0.0009)=F(0.9991)=-1000000$, it follows 
that the expected epistemic value, conditionally on winning the lottery, will be:
$$
    (1/2)(0.9991 \cdot 1000+0.0009\cdot (-1000000)+0.9991 \cdot 1000+0.0009\cdot (-1000000)) = 99.1 > 0.
$$
And if we multiply this by the $1/1000000$ chance of winning the lottery, we still have something positive, so
the expected epistemic value of playing the lottery with the plan of using machine $B$ is positive, while that of
using machine $A$ is zero. 

Thus, by considerations of epistemic value, the scientist with this scoring rule will prefer a $1/1000000$ chance of 
gaining credence $0.9991$ as to whether $H$ is true to a certainty of gaining the slightly lower credence $0.9989$.
This is not rational. 

Now, in the above example, our scoring rule while proper, symmetric and finite, was neither continuous nor strictly proper. 
However, we will show in the Appendix??ref that there is a sequence of continuous, strictly proper, finite and symmetric
scoring rules $T_n$ and $F_n$ such that $T(x)=\lim_{n\to\infty} T_n(x)$ and $F(x)=\lim_{n\to\infty} F_n(x)$ for all $x$.
If $n$ is large enough, then the pair $T_n$ and $F_n$ will require exactly the same decision from our scientist as 
$T$ and $F$ did, since the expected value of the expected $(T_n,F_n)$-scores of the two courses of action will converge
to the expected vlue of the expected $(T,F)$-scores.

Hence not all epistemic valuations that satisfy the plausible formal axioms are rationally acceptable, then we 
will have Mersenne questions about what grounds the further constraints on the epistemic valuations. These constraints
are likely to include messy prohibitions, with multiple degrees of freedom, on the kinds of sharp jumps that our
pathological scoring rule above exhibited.

Furthermore, things become more complicated when we consider that the epistemic value of a credence in a truth will
differ depending on the importance of that truth. Getting right whether mathematical entities exist or whether we are 
material or how life on earth started has much more epistemic value than getting right Napoleon's shoe size. Epistemic
value will thus not only be a function of credence and truth, but also of subject matter. Moreover, we will have 
further degrees of freedom concerning the operation of combining epistemic values for different propositions---addition
may seem a plausible operation, but the logarithmic and spherical rules are not combined additively across propositions.

We thus have multiple indicators of a contingency about epistemic value assignments. And there is good reason to think
that different forms of life are more suited to different epistemic value assignments. The most obvious aspect of this
is that once we move away from the toy case of assigning a value to one's epistemic attitude to a single proposition
and consider that attitudes to a large number of propositions need to be considered, it is obvious that the subject
matter of the propositions will affect how great a weight we give credences about them in the overall evaluation. And
the importance of subject matter obviously depends on the form of life. It is plausible that for intelligent agents whose
natural environment is more hostile it would be more fitting to have a greater epistemic 
value assigned to practical matters, while agents that have few natural enemies and can get food easily might more fittingly
have a greater epistemic value assigned to theoretical matters. One imagines here that intelligent antelope might be
properly expected to be less philosophical than intelligent elephants.

\subsection{Connection with other values}
????????????

\section{Bayesianism}
\subsection{Introduction}
Bayesianism is the best developed picture of what a precise and rigorous account of epistemic rationality would be like.
It is thus worth looking carefully at what kind of answers the Bayesian could give to the questions we have been asking.

??introduce Bayesianism

\subsection{Priors}
From a Bayesian point of view, how induction works is determined by the probabilities prior to all evidence, the ur-priors.
Suppose, for instance, that I assign equal prior probability to every logically possible color sequence of observed ravens. 
For simplicity, suppose that there are only two colors, white and black.  I find out that there are a million ravens, and 
I observe a thousand of them, and find them all black. I am about to observe another raven. The probability that the next 
raven will be black will be $1/2$. For the sequence $B,...,B,W$ (with $1000$ $B$s) is just as likely as the sequence
$B,...,B,B$ (with $1001$ $B$s), and both sequences fit equally well with our observations. 

On the other hand, suppose I assigned probability $1/3$ to the hypothesis that all ravens are white, $1/3$ to all black,
and split the remaining $1/3$ equally among the $2^{1000000}-2$ multicolor sequences. My observation of the first $1000$
ravens then rules out the all-white  hypothesis. And it rules out most of the multicolor sequences: there are $2^{999000}-1$
multicolor sequences that start with $1000$ black ravens, which is a tiny fraction of the original $2^{1000000}-2$. Since as a 
good Bayesian I keep the ratios between the probabilities unchanged, each of the remaining multicolor sequences has 
$1/(2^{1000000}-2)$ of the probability of the all-black sequence, and since there are only $2^{999000}-1$ multicolor
sequences remaining compatible with the evidence, the ratio between the multicolor probability and the all-black probability
is $(2^{999000}-1)/(2^{1000000}-2)$ to $1$, or approximately $1$ to $2^{1000}$. Thus, we have overwhelming confirmation of 
the all-black probability, and hence an even more overwhelming confirmation of the hypothesis that the next raven will be black.

Other ways of dividing the probabilities between the hypotheses yield other results. Carnap??ref, for instance, had a division
that worked as follows. For each number $n$ between zero and a million we have the hypothesis $H_n$ that there are exactly 
$n$ black ravens, and Carnap proposed that all million-and-one of these hypotheses should have equal probability, and then 
each hypothesis $H_n$ is divided into equally likely subhypotheses specifying all the subhypotheses that make there be $n$
ravens. Thus, $H_0$ and $H_{1000000}$ have only one subhypothesis: there is only one way to have no-black or all-black. But
$H_1$ and $H_{999999}$ have a million subhypotheses each: there are a million options for which is the raven with the outlying
color. Using the same constant-ratio technique as before, after observing $1000$ black ravens, the chance that the next one is black 
will turn out to be approximately $0.999$, but the chance that all million are black will only be $0.001$. More generally, if there
are $N$ ravens, and the first $m$ of them have been observed to be black, and $n\ge m$, then the probability that the first $n$
will be black will be $(1+m)/(1+n)$.\footnote{Let $B_m$ be the claim that the first $m$ ravens are black. Then
$P(B_m)=\sum_{n=0}^{N-m} {{N-m \choose n}}/((N+1){N\choose m+n}) = \frac{1}{1+m}$.??why:Mathematica The probability that the first
$n$ are black given that the first $m$ are black where $n\ge m$ will then $P(B_n\mid B_m)=(1+m)/(1+n)$.} 
  Hence we have very reason to think that the \textit{next} raven is black,
but unless we have observed the bulk of the ravens, we won't have reason to think that all the ravens are black. 

Intuitively, while Carnapian probabilities support induction, they result in induction being too slow---it is only when we have observed the bulk of the cases
being a certain way that we get to conclude that they are all like that. My $1/3$--$1/3$--$1/3$ division is too fast. Even with
$1000$ black ravens having been observed, the probability of a white raven shouldn't be \textit{astronomically} small in the way
that $1/2^{1000}$ is. Reasonable priors, thus, yield a speed of induction somewhere between these. 

We can presumably come up with a formula for the priors which will fit with our intuitions of how fast induction should work.
For instance, we could take Carnap's setup, but increase the prior probability of the all-white and all-black raven hypotheses. 
But such an increase would be apt to involve one or more parameters. If the specific assignment of priors were rationally
required of us, then we would have the Mersenne question of why it is these and not some other very similar priors that are required.
And if there is a range of priors rationally permitted to us, then we would have Mersenne questions about the boundaries of
this range. 

Further, imagine beings other than us that inhabit a more Carnapian world than we do. While in our world, we have a significant number of natural kinds
that exhibit or fail to exhibit some basic property exceptionlessly---for instance, every electron is charged, and no photon 
has mass--in that world there are few such natural kinds. Instead, if we were to tabulate the frequencies of basic binary properties
in various natural populations---say, tabulating the frequency of blackness among ravens, charge among electrons, mass among photons---we 
would find the frequencies to be distributed uniformly between $0$ and $1$. In that world, Carnapian priors would lead to the truth faster
than the more induction-friendly priors that we have. And let us imagine that in that world we have intelligent beings who reason according
to Carnapian priors. Even if we happily grant that Carnapian priors are irrational for us, it seems plausible to think that they could be
rational for those beings. To insist that these Carnapians are irrational, because it would be irrational for us to have these priors,
seems akin to saying that bigamy would immoral for aliens who need three individuals to reproduce, or that there is something wrong with fish
because they lack lungs. 

Consideration of the rationality of induction thus once again reveals an appearance of contingency in the normative realm, which
once again yield an argument for an Aristotelian picture of human nature, where the rationally required priors or ranges of priors
are those that we are impelled to by our human nature.

But before we embrace this conclusion fully, we should consider two Bayesian challenges, from two opposed points of view.
The algorithmic Bayesian thinks that considerations of coding can yield a reasonable set of priors, while the 
the subjective Bayesian says that there are no constraints on the priors.

\subsection{Algorithmic priors}
Suppose, first, we have a language $L$ of finite sequences of symbols chosen from some finite alphabet of basic symbols, 
with some of the sequences representing a member of some set $S$ of situations. 

For instance, $S$ could be arrangements of chess pieces on a board\footnote{The arrangements contain less information
than chess positions, since a chess position includes other information, such as whether a given king or rook has already
moved, whose turn it is, as well as historical information needed for adjudicating draws.}, and $L$ could be a declarative 
first-order language with no quantifiers, twelve piece predicates (specifying both the color and piece type) and sixty-four names of squares.
We could then say that a symbol sequence represents an arrangement $a$ provided that the sequence is a 
syntactically valid sentence that is true of $a$ and of no other arrangement. However, in general $L$ need not be a declarative
language. It could, for instance, be an imperative computer language for an abstract Turing machine or a physical computer,
and the situations could be possible outputs of that machine. Then we might say that a symbol sequence $s$ represents
a possible output $a$ just in case $s$ is a program that, when run, halts with the output being $a$. Or if we like we might 
add an additional layer of representation between the outputs of the machine and the situations---for instance, the outputs
of an abstract Turing machine might represent the physical arrangement of particles in a universe.  
We can even chain languages. For instance, we could have a computer language $L_1$, with the outputs being sequences of
symbols in some declarative language $L_2$, whose sentences in turn represent members of a set $S$ of situations.

Next, consider a natural way of choosing at random a finite sequence of symbols of $L$. Here is one. Add to $L$'s finite alphabet 
a new ``end'' symbol. Then randomly and independently, with each symbol being equally likely (i.e., having probability
$1/(n+1)$, where $n$ is the number of non-end symbols), choose a sequence of
symbols until you hit the end symbol. The sequence preceding the ``end'' symbol will then count as the randomly
selected sequence in $L$. Every sequence of length $k$ has probability $1/(n+1)^k$, so the probabilities decrease 
exponentially with the length of the sqeuence. We repeat the random selection process until we get a sequence that is both syntactically correct
and represents a situation in $S$.\footnote{If at least one finite sequence is syntactically correct and represents a 
situation in $S$, then with probability one, we will eventually get to a sequence that syntactically correctly represents some 
sequence.}  We now stipulate that the prior probability of a situation $s$ is equal to the probability that the
above process will generate a sequence that represents $s$.

Alternately, we can formulate this as follows. If $a$ is a sequence of symbols of $L$ and $s$ is a situation in $S$, write
$R(a,s)$ if $a$ is syntactically correct and represents $s$, and let $R(a)$ be shorthand for the claim that $a$ syntactically
correctly represents $S$, i.e., $\exists s(s\in a\And R(a,s))$. Then if $A$ is a randomly chosen sequence of symbols of $L$,
we can define the prior probability $Q(s)$ of $s$ as the conditional probability that $A$ syntactically correct represents $s$ on the supposition
it syntactically correctly represents something, i.e.,
$$
    Q(s) = P(R(A,s)\mid R(A)) = \frac{P(R(A,s))}{P(R(A))}.
$$    
We can call these $L$-Solomonoff priors.

These priors favor situations that can be can be more briefly represented in $L$ over ones whose representations are long.
The effect of these priors depends heavily on the choice of language $L$ and how well it can compress some situations over
others. For instance, in our chess case, if we have no quantifiers, it is easy to see that any two piece arrangements with 
the same number of pieces will have equal prior probability, because each square's contents have to be separately specified. 
Thus, if we know that every square contains a pawn, and we have observed the first 63 of these pawns and found them all to be black, the 
probability that the 64th square will be black is still $1/2$. On the other hand, if quantifiers and identity are allowed into our
language, then the all-black-pawn situation can be briefly represented by
\ditem{all-black}{$\forall x(\operatorname{BlackPawn}(x))$}
(where the domain is
squares on the board), while the situation where squares $1,...,63$ have black pawns and square 64 has a white pawn is harder
to represent. We might, for instance, use a sentence like:
\ditem{63-black}{$\forall x(\Not (x=64)\rightarrow \operatorname{BlackPawn}(x))\And \operatorname{WhitePawn}(x)$.}

Since the probability of generating a given sequence of symbols decreases exponentially with the number of symbols, \ditem{63-black}
is much less likely to be randomly generated than \ditem{all-black}, and it is intuitively very likely (though proving this rigorously would
be quite difficult???refs) that in general the probability of generating a sentence representing 64 black pawns is higher than that of 
generating a sentence representing 63 black pawns followed by one white pawn. We can thus expect that the conditional probability of
the 64th pawn being black on the first 63 being black to be very high. (Maybe even too high? It is very difficult to get good
estimates here, because there are many ways that a single situation can be represented. ??refs)

Just as in the case of using linguistic complexity to quantify projectibility, the choice of language here provides many Mersenne
questions. If we opt for the algorithmic version of the theory, we need to choose some computer language for a real or abstract
computer, and then we need to choose a representation map between outputs and situations in the external world, with infinitely
many possible candidates. And on the more
descriptive versions, we still need to choose a language, with many decision points as to syntax and vocabulary. It is very unlikely 
that there is a privileged language. And not
every will fit with our intuitions about induction. For instance, we can easily create a language, whether algorithmic or descriptive,
where 63 squares of black pawns followed by one square with a white pawn are much more briefly describable than 64 squares with
black pawns. For instance, on the descriptive side, we might use a $\operatorname{BlitePawn}(x)$ predicate, where something is a blite pawn 
provided it is a black pawn and on one of the first 63 squares or a white pawn and on the 64th, and an analogous $\operatorname{WhackPawn}(x)$.

In the unlikely case that there is a privileged language $L$ such that $L$-Solomonoff priors are rationally required for us,
we will have a vast number of Mersenne questions about the various parameters of the language and its representation relation.
In the more plausible case that there is a set of languages such that we are required to have $L$-Solomonoff priors for some $L$
in the set, we will have a vast number of Mersenne questions about the parameters that control the range of languages. All of this
gives rise to a significant degree of appearance of contingency.

Consider, too, the following observation. It is implausible that the languages defining rational priors for us
should be ones that are completely beyond our ken. But, on the other hand, it is plausible that there are possible
languages that to the smartest human are as incomprehensible as one of the less intuitive computer languages like
Haskell or Verilog or one of the creations of logicians further from natural language like lambda-calculus is to a typical
six-year-old. Imagine now beings to whom such these languages beyond human ken are easy. It \textit{could} be the case that
the norms for rational priors for them are formulated in terms of $L$-Solomonoff priors for one of the ``baby languages''
that humans can understand, but this does not seem a particularly plausible thesis. It seems more likely that for those
beings, the algorithmic rational priors would be different than for us. 

The standard way??ref to defend algorithmic measures of complexity from the problems presented by a plurality of languages
is to observe that sufficiently sophisticated languages have translational resources. Thus, one can write a Haskell interpreter
in Javascript, and so anything that can be expressed in Haskell can be expressed in Javascript by including the code for a Haskell interpreter, 
and then using a string constant that contains the Haskell code. The result is that the difference in the length of code needed to
generate a given output in different computer language will typically not be more than an additive constant: if one can
produce the output with Haskell code in $n$ bytes, then one can produce it in approximately\footnote{The approximation is due
to complications due to having to embed code in a string constant, which may involve various escape characters.} $n+k_{H,J}$ 
bytes in Javascript, where $k_{H,J}$ is the length of the Haskell interpreter in Javascript. For large enough $n$, the additive constant will be
unimportant. If we are to measure the complexity of a one-hour broadcast-quality video by the length of code needed to compress 
the  video, the addition of $k_{H,J}$ will likely be negligible: a Haskell interpreter is about half a megabyte, while 
an hour of video compressed losslessly can be reasonably expected to be in the gigabytes.

However, two points need to be made in our epistemological context. First, even if two different languages give very similar sets of priors, 
if they give even slightly different priors, we either have the Mersenne question of what makes one of these sets of priors be the 
objectively correct one, or we have the Mersenne question about the boundaries of the range of permissible languages. Second,
unlike in the case where we are measuring the complexity of a large set of data, such as a video file, in the inductive cases we 
need to look at ways of expressing relatively simple statements, such as ``All electrons are negatively charged'' or 
Schr\"odinger's equation or ``The first 63 squares have a black pawn and the last square has a white pawn.'' But for such statements, 
a translation manual will dwarf the length of the translated text. To say ``All electrons are negatively charged'' in French by first describing how English works, and then saying that this
description should be applied to the English sentence, will produce a French sentence that is many orders of magnitude longer
than the English one, and hence not a sentence that is relevant to measuring the prior probability that all electrons are negatively
charged.

Finally, while there is something elegant and natural about randomly choosing items in $L$ by randomly choosing within the
set of symbols with an end marker added, there are other ways to proceed. For instance, instead of making the end marker
equally likely as each of the ordinary symbols, one could at each step of generation flip a fair coin. On heads, one is done
generating. On tails, one then uniformly randomly chooses one of the $n$ symbols.\footnote{This will actually not make a 
difference in those languages where the syntax already determines where a syntactically valid sequence ends. This will be the
case with some Polish notation languages, where a valid sequence ends when the main operator is filled out with arguments.}
Or one might first randomly choose a positive integer specifying the length of the sequence of symbols according to some
probability distribution on the positive integers, and then make all the sequences of that specified length be equally likely. 
Or one might randomly choose a positive integer, and then choose the $n$th sequence of symbols in some ordering (e.g., alphabetical).
While the initial symbol-by-symbol method with an end-symbol may seem more elegant, it is hard to say that it is rationally
privileged to the point that the priors generated with it are rationally required. But if it's not thus privileged, then the
range of random choice methods will provide more Mersenne questions. For not every random choice method yields priors that
are plausible candidates for rational permissibility. There will be random choice methods where the sentence ``Birds are a government-run 
drones'' is many orders of magnitude more likely than all other sentences taken together, and so a boundary would need to be
posited between the admissible and inadmissible random choice methods.

On a final note, ne might think that the intuitively most natural way of choosing a linguistic item at random is to make them
all be equally likely. Unfortunately, this presents serious mathematical and philosophical difficulties. For a language
based on finite sequences taken from a finite (or countable) alphabet, there are countably infinitely many sentences:
we can enumerate them $s_1,s_2,s_3,...$ in some arbitrary way. But if each one is equally likely, with probability some
real number $\alpha$, then we have a problem. In classical probability, we will have to have:
$$
    1=P(\{s_1\})+P(\{s_2\})+P(\{s_3\})\dots = \alpha+\alpha+\alpha+\dots.
$$
But if $\alpha>0$, then the right-hand-side is infinite, while if $\alpha=0$, it is zero, and in neither case is it $1$.
There are technical ways of escaping this by departing from classical probability. They all require restricting the 
additivity axiom of probability that says that if $A_1,A_2,\dots$ are countably many disjoint events then the probability 
of the union of the events is equal to the sum of the probabilities of the events to the case where there are only finitely
many events. After that, one either takes $\alpha=0$ or takes $\alpha$ to be a positive infinitesimal---something that is bigger 
than zero and smaller than any positive real number. 

But whatever one does on the technical side, there will be philosophical
difficulties. Emblematic of them is this paradox. Suppose you and I play a game where we each randomly pick a sentence with
all sentences equally likely, and without seeing the other's sentence. When I see my sentence, whatever it turns out to be, I inevitably become nearly sure that your 
sentence comes after mine in the sequence, because there are only finitely many sentences that come before mine and infinitely
many that come after. And you come to be convinced of the same thing. This leads to paradoxical decision-theoretic conclusions.
For instance if we are playing a game where one wins if one has a sentence further down in the sequence, you will then be willing to pay
me any amount short of the prize to swap sentences with me, no matter what sentence you got. ?????

In any case, the equal probability approach itself does not appear to be a good way to generate induction-friendly priors, 
because it lacks the preference for shorter descriptions that is central to the functioning of algorithmic or linguistic
priors. 

Once we abandon, as we should, the equal probability approach to choosing a linguistic item, anything else is a matter 
of choosing from among infinitely many ways to be biased in favor of shorter expressions. Some of these are mathematically
more elegant than others, but none is decisively so.

\subsection{Subjective Bayesianism}
Subjective Bayesians avoid all the difficulties of specifying permissible priors by merely
requiring the priors to satisfy some formal properties. These are taken to include the
axioms of probability and, sometimes, the regularity constraint that all contingent 
propositions have non-zero probability. Rationality then constrains transitions from one
set of probabilities to another: these must follow the Bayesian update rule that upon
receiving evidence $E$, one's probability in a hypothesis $H$ goes from $P(H)$ to 
$P(H\mid E)$. But the initial choice of priors is up to the individual, subject to the
formal constraints.

The resulting picture of rationality does not match common sense. Take the most
ridiculous set of conspiracy theories that we would all agree is unsupported by our evidence, 
but where nonetheless the conjunction of the theories is logically consistent with the evidence.
Then there is a possible assignment of priors such that updating in the Bayesian way on our actual 
evidence strongly confirms the conjunction of these theories, both in the incremental sense of 
greatly increasing that probability and in the absolute sense of making that probability high. (All 
we need is that the prior probability of the conjunction of theories be low, but the conditional
probability of that conjunction on the conjunction of our evidence be high.) Yet to reason that
our evidence strongly supports these theories is paradigmatic of irrationality. It shouldn't be the
case that a rational person could come to exactly the same conclusions on exactly the same evidence
as are paradigmatic of irrationality.

Or consider the implausible asymmetry between the freedom in choosing initial priors and the
rigid constraint in updating. Suppose I don't like my current set $C$ of posteriors, and I would 
feel better if  I had some cheerier alternative set $A$ of posteriors. There is some set of priors $Q$ that, 
given the evidence $E$ that I had received over my lifetime, would have yielded $A$. According to 
the subjective Bayesian there would have been nothing irrational in having adopted those priors in 
the first place, and thus having ended up at the cheerier posteriors. Why should I be tied to the
priors that I actually had? 

The picture of priors here is like a rationally unbreakable vow to live one's life as an evolution
of these priors under Bayesian application of evidence. But it is a vow made without any rational 
ground, indeed without any choice, and likely in childhood. We would think it unsupportable that
someone be held committed for life to a promise made early in childhood. Why then should we be bound to
our priors and the posteriors coming from them?

%% ?? ref: https://link.springer.com/article/10.1007/s11098-019-01367-0?utm_source=toc
%% ?? indifference

\subsection{Non-Bayesian update}
Bayesianism is committed to updating by conditionalization being the only permissible way to update credences. 
If my current credence in a hypothesis $H$ is $P(H)$, and I receive evidence $E$, my credence should move 
to $P(H\mid E)=P(H\And E)/P(E)$. No other changes of credence are permitted. There is an elegance and non-arbitrariness to this,
of course modulo the above-discussed issues about the choice of the priors $P(H)$ and $P(H\mid E)$.

But as is the case for many other formally simple philosophical theories, this is too
simple. Consider several cases. 

\textsc{Pill:} A trustworthy oracle offers you a pill which will shift your credences closer to truth and does not introduce 
any incoherence. 

\textsc{Mistake:} An hour ago, I made an arithmetical evidence when updating on evidence. I moved from credence $0.4$ in $H$ to
credence $0.6$, even though in fact $P(H\And E)/P(E)$ equalled $0.7$. I have just discovered my mistake. Surely it would be good
for me to go back and correct my credences, essentially rewinding my epistemic life from the mistake and re-conditionalizing on all
the subsequent evidence. 

\textsc{Stupidity:} My original priors had extremely high probabilities for some conspiracy theory involving members of a certain 
minority group, so high that despite the fact that all my life I have been receiving strong evidence against the theory, nonetheless I 
was quite convinced of the theory. I reflect on my original priors, and note that my priors of the conspiracy theory in question were 
quite out of proportion to my priors for similar theories involving other minority groups. I conclude that my original priors were
stupid and racist. I go back and change them to treat the various groups more equally, and then as best I can I fix up my posteriors
to match the evidence I recall having had. 

In all three cases, my update of credences seems quite rational, but is not a case of conditionalization. We should thus
suppose that while it may be a good general rule that we should update by conditionalization, we should at times depart from it.
But the question of how we should depart from conditionalization now introduces complexity in the theory that raises significant
Mersenne questions. We need more than just simple exceptions for each case: there will be parameters to set.

The pill case as given is straightforward. We can say that you should depart from Bayesian update when doing so would move
some of your credences closer to truth and none further away from it, where a credence's ``distance from truth'' is the 
distance of the credence from $0$ or $1$ depending respectively on whether the proposition the credence is in is false or
true. However, there are variants of the pill case. Suppose that the pill has a $0.9$ chance of moving you significantly
closer to truth and a $0.1$ chance of moving you slightly away from truth. It seems like it could be worth it. Or suppose
that the pill moves your credences in important propositions closer to truth at the expense of moving your credences in some
unimportant propositions further from the truth.

This suggests that we will need to quantify epistemic value, much as we did in Section??backref, and then say
something like this: You are epistemically required (respectively, permitted) to opt for a pragmatically costless change of 
credence when doing so increases (does not decrease) expected epistemic value. If we quantify epistemic value in terms of
strictly proper scoring rules, a delightful result of this move is that we do not need to handle the ordinary case of 
Bayesian update any differently. For it can be proved that in the kinds of cases where one simply receives
evidence and sets one's credences according to it, conditionalization is the unique best policy for maximizing expected
epistemic value.??ref:(Also Isaacs and Russell??) 

However, as we also saw in Section??backref, the choice of a scoring rule or measure of epistemic value involves infinitely
many free parameters, and at the same time not every scoring rule that satisfies the formal constraints is rationally
plausible. In our present context of evaluating non-conditionalizing updates the point is particularly clear. For when
evaluating credence-changing pills, we need to take into account the \textit{importance} of the affected credences, and
that is not a formal criterion. 

Note that pill-type cases are not as outlandish as they may initially seem. We do in fact modify people's thinking with
psychiatric medication as well as with psychotherapy.

Next, consider the case of the mistake in updating. We don't in fact remember all our evidence:
we update our credences on the more important things and forget the less important, which often includes some or
all of the evidence. I know that Beijing is the capital of China, but I don't know where I learned it from. I know that 
there was a cat in the yard earlier this morning, but much of the rich sensory evidence is now gone from my mind. 
An attempt to go back and correct a past update error is bound to be a messy affair. Sometimes when the mistake is minor
it might be better to let it go rather than miss out on some of the evidence gathered since. And the situation is often
too messy and too complex for any sort of an expected epistemic utility calculation. 

Yet the fact that a situation is messy does not get us off the hook. There will still
be a distinction between right and wrong ways to proceed, even if they cannot be formalized. We have here something very
similar to the kinds of messy everyday ethics cases that are grist for the situationist's mill.??backref-or-add 
It is unlikely that the answer is given by any elegant principle without the kinds of parameters that raise Mersenne
questions, but at the same time there is an answer, and there
must be something to ground it.

And what goes for the messiness in correcting a calculational mistake applies even more to the messiness involved in
the kind of intellectual conversion that occurs when one realizes that all of one's thinking for years has been based
on irrational prejudice, and one attempts to dig oneself out of the resulting heap of epistemic defects. Again, there
are right and wrong ways to proceed, but the likelihood of a clear and elegant principle that solves the problem is
nearly nil. 

There is a final set of issues with update. Presumably, there is something to the maxim that ought implies can. And typically 
we can't do Bayesian update. We don't have the time, don't have the mathematical skills, or perhaps most importantly aren't 
able to quantify our priors in such a way as to make them amenable to precise mathematics. We need an epistemology that 
works in this all too human predicament, a \textit{human} epistemology, and we need an account of what grounds that 
epistemology. 

One may insist that under these imperfect conditions, we should simply say that we are stuck with irrationality.
But nonetheless there are cases where it is clear that something should be done under the unhappy circumstances.
Suppose that the completely right credence on my priors in some proposition $p$ is $0.9874$, 
but I can only calculate to two significant figures. Then I should take the credence to be $0.99$, not $0.98$ or 
$0.01$. But things will be less clear if I need to form credences in both $p$ and $q$, which a perfect Bayesian
with my priors and evidence would take to be $0.9874$ and $0.0332$, but due to limitations of
time or energy level, I can only get a total of four significant figures. Are the right credences for me $0.99$
and $0.03$, or $0.987$ and $0.0$, or $1$ and $0.033$? It depends, surely, on the relative epistemic importance of $p$ and
$q$. And once importance needs to be taken into account, it is very unlikely that we will have a precise and elegant
account with no free parameters. Instead, we have human messiness.

\section{Intellectual limitations}
Limitations in recognizing logical implications and contradictions are a clear case where rational norms are at
least species-relative. It is epistemically irrational for one to conclude that it's raining from the fact that it's neither 
raining nor snowing. But it is not irrational for one to take $22828 \times 2219 = 50645332$ (which is in fact
false) to follow from the axioms of arithmetic due to an arithmetical slip. Of course, someone with 
exceptional calculational skills may immediately that see the latter is mistaken, but the ordinary person's failure to see it is 
not an instance of irrationality. 

Ethics also contains intellectual limitation cases. Analogically to the multiplication
case, we would not consider a person to be less than virtuous because they are unable to see an extremely complex medical 
ethics case rightly. But if someone doesn't realize that it's wrong to ambush strangers in order to sell their organs,
there seems to be something morally wrong.

But the ethical cases may also be different. Consider an adult who strives to follow their conscience as best they can,
but nonetheless fails to see that it's wrong to kill strangers for their organs. This person presumably has
a severe intellectual and/or emotional disability. We might judge them to be a good person, or at least not a vicious one. 
However, the logical case seems different. The person who, due to intellectual disability, concludes that it's raining 
from the claim that it's neither raining nor snowing \textit{is} failing at rationality, though of course they are 
inculpable. One can defensibly define moral worth in terms of the seriousness of their attempt to do what seems right, but
it is difficult to define someone's degree of rationality in terms of the seriousness of their attempt to think as seems
right. For it is paradigmatic of irrational people that they take themselves to be acting quite rationally---if one
derives that it's raining from its neither raining nor snowing, this is precisely because the conclusion seems to follow.
It is, indeed, unclear whether it is even possible to conclude $p$ from $q$ without its seeming that $p$ follows from $q$.
But it is paradigmatic of immoral people that they lack integrity and violate their own conscience. 

The line to be drawn in the epistemic rationality cases (and in the ethical ones if they turn out to be similar), is a line that we 
are unlikely to be able to draw in a species-independent way. It seems plausible that for beings that as a species are 
more adept at logic than we are, a failure to see the truth of Fermat's Last Theorem as following from the axioms of
arithmetic is a failure of rationality, but it is not so for us. 

Recently, Jeffrey??ref has argued that ethical norms may be relative to a stage in life. Whether this is so, it is very
plausible that some epistemic norms are so. There may be logical facts failure to notice which constitutes a failure of 
an adult's rationality, but would not constitute a failure of a child's rationality. If this is right, then we have all
the more reason to think the norms of epistemic rationality to be species-relative, since surely what the stages in 
life are, and when they occur, is something that is species-relative.

\subsection{Innate beliefs and testimony}
Humans are said to have much less in the way of instinct than other earth animals. Furthermore, we do not seem to
have any innate beliefs. But we can imagine a species of intelligent animals which do more by instinct than we do,
and which have evolved to be born with some unshakeable and true beliefs, such as that purple winged things are to 
be avoided and electrically charged spiky fruit is good to eat. For these beings, there is nothing irrational about 
having such beliefs without any evidence. For us, there is. In a Bayesian mode, we might say that for these
beings very high priors in these empirical claims are appropriate, but not so for us. 

We can also imagine a species where memories are inherited. A member of this species could, then, be born with a 
large number of beliefs that they had no evidence for. For, like we often do, the members of this species could forget the
evidence that led to a conclusion. But while in our case, we once had the evidence, in this species it was some ancestor
of theirs that had the evidence. It is plausible that for us it is generally irrational to have beliefs without ever having had
evidence. But this is just a normative fact about our species, not a fact about species-independent rationality.

Now, let us return to our species. Perhaps we should not think the species where memories are inherited to be all that
different from us. For do not our parents bequeath much knowledge to us? It is true that this is mediated by vibrations
of the air rather than by gametes, but does that make a significant difference. We should, thus, take seriously the idea
that just as for members of a species where memories are inherited it is fundamentally rational to believe these inherited
memories, and to question them without special reason is irrational, for us it may be fundamentally rational to believe
at least some testimony, and to question it without special reason is irrational. If so, then we have another example
where the Bayesian way of looking at update may not be correct.
???fill out

\section{Going beyond the applicability of human epistemology}
Suppose that you are certain you are one of infinitely many people have each rolled a fair die, none of whom have seen the 
result. It is then announced by a perfectly reliable angel that all but finitely many of the dice show six. 
What should be your credence that your die shows six? 

On the one hand, it seems very likely that your die shows six. After all, the vast majority of the dice show
six, and you have no reason to think yourself exceptional. 

On the other hand, prior to the announcement, your credence in six was $1/6$. And the announcement told you nothing 
about your die. For the following two statements are logically equivalent:
\ditem{die-announce-1}{All but finitely many of the dice show six.}
\ditem{die-announce-2}{All but finitely many of other people's dice show six.}
For your die's state makes no difference to whether there are finitely or infinitely many non-sixes.
But then given your certainty that the dice are fair, \dref{die-announce-2} gives no information about 
your die's result. And since \dref{die-announce-1} is logically equivalent, it too gives no information.
Having received no information, you should stick to $1/6$. 

This reasoning seems convincing. Yet if everyone sticks to $1/6$, then all but finitely many people are quite far 
from the truth. Furthermore, if everyone is playing a game where you are asked to guess if you have a six or not,
and you are rewarded for getting it right and penalized for getting it wrong, then if everyone sticks to $1/6$
for having a six, everyone will guess that they don't have six, and all but finitely many people will be penalized,
which is surely not the right result.

In ??ref, I argued that a good solution to epistemological paradoxes like this is to reject the metaphysical possibility
of an event depending causally on infinitely many events, in the way that the angel's accouncement depends on the infinite
number of die rolls. But there is another possible solution: we can deny that our epistemic norms extend to far-fetched
situations, just as in ??ref it was suggested that our ethical norms do not apply to far-fetched situations. Whether this is 
a completely satisfactory resolution to the paradox is not clear. For there is some plausibility in thinking that if 
predicaments like the above were metaphysically possible, there could be rational beings who could reason
in them. But it seems there couldn't be any. However, no matter what we say about the metaphysical possibility of such beings,
there is something right about the thought that \textit{we} are not made to reason about such things.

There are many other examples where epistemology seems to break down in radical cases. What should you think if you came to be
convinced that an evil demon is trying to get you to believe as many falsehoods as possible? Any thought you might have ends
up undercut. Should your credences in everything be at $1/2$, then? But that is incoherent: for then you have credence $1/2$
that the last die you rolled is $1$, and credence $1/2$ that it was $2$, and credence $1/2$ that it was $3$, and credence $1/2$
that you never tossed a die, and so on. Perhaps suspension of judgment is something other than assigning probability $1/2$. But 
if so, should you suspend judgment about everything, including about the norm of fitting your beliefs to you evidence? However,
if you suspend your judgment about that, then why bother with suspending your judgment about other things, given that you don't 
think you need to fit your beliefs to your evidence? Or what should you think if you come to be convinced that you are a computer 
simulated non-player character in a sophisticated video game? What kind of a world should you think you are in? 

Note now that there is such a thing as realizing that when you were using certain words, you had no concept behind them, and
the words were mere meaningless words. For instance, take a word that we as laypeople defer to experts on the meaning of, such
as ``gluon''. But now imagine that physicists have been pulling our collective legs about gluons all this time: it was just a 
made-up word without any meaning behind it. We can imagine discovering that. And words whose meaning we get by deference are not
the only words like that. The phenomenon of a person who doesn't know what they are talking about is not uncommon. My metaphysics
includes accidents. But I could imagine finding myself in a position where I come to be convinced that I never had a concept of
an accident---that ``accident'' is a meaningless word. Now imagine a radical hypothesis: I come to be convinced that I have no
concepts at all. What should I think now? 

There is something plausible about the thought that in certain extreme situations there is no right way for us to think.
??other beings*

\section{Facts about species-independent rationality?}
??:also, species-independent ethics 

rationality-and-ethics constitutive stuff?? God??

\section{Metaepistemology}
The primary question of metaethics is whether ethically normative claims have truth value, and if so, what grounds their truth value.
The Natural Law answer that I have been defending is that ethically normative claims have truth value grounded in facts about the 
flourishing of the will, which facts in turn are grounded in our form (??add this!). Analogically, we would expect epistemologically 
normative facts to have truth value grounded in facts about the flourishing of the intellect, which facts in turn would be grounded
in our form.

Natural Law metaethics as it stands is compatible with a wide variety of normative ethical theories, though some combinations are
less appealing philosophically. Thus, while one could suppose that the proper function of the will is to maximize utility, 
normative utilitarianism fits more elegantly with metaethical utilitarianism according to which facts about the right
and wrong just are facts about utility maximization. 

Similarly, Natural Law metaepistemology fits with a variety of normative epistemological theories. Reflection on ethical cases
shows that flourishing can have internal and external components (e.g., one's friends' well-being is partly constitutive of one's 
well-being), and so Natural Law metaepistemology is compatible with internalist and externalist norms. It is compatible with 
full-blown Bayesianism, though just as normative utilitarianism fits more neatly with metaethical utilitarianism, so too those
versions of Bayesianism on which the norms all have simple formal statements---say, subjective Bayesianism or Carnapian objective
Bayesianism---fit more simply with an epistemology on which the norms are grounded in formal constraints. On the other hand, an
objective Bayesianism with non-formal constraints on priors fits very well with a Natural Law metaepistemology.

However, over the last century or so there has been a disciplinary difference between ethics and epistemology. While the ethicists
have had a primary focus on the question of how one should act, the epistemologists' primary focus has not been the question of 
how we should think, but what knowledge is. One might take this question of the nature of knowledge to be akin to many ethicists' 
interest in the nature of virtue. In both cases, there is a sense in which the question does not carry normativity on its sleeve,
in that there are possible answers to the question that do not involve any normative components. Thus, Descartes identified knowledge
with the clear and distinct, and a naive Aristotelian (though of course not Aristotle himself) might identify virtue with the 
mathematical center between the humanly possible extremes. Of course, in both cases it is highly plausible that the item is worth
having, but that does not make the item essentially normative---water is also worth having, but is not essentially normative.
However, likewise, in both cases more plausible theories of the item have a significant normative component. Thus, the real 
Aristotle would identify virtue with a \textit{reasonable} mean, while many have tried for accounts of knowledge where one of the
components is \textit{justification}. 

Natural Law metaepistemology more easily yields an account of the grounding of more obviously normative concepts such as 
justification. For instance, a justified inference could be an inference performing which constitutes intellectual flourishing 
(or the intellect \textit{qua} engine of inference). Again, this is compatible with many theories as to what kinds of inferences
are in fact justified. But it is more difficult to see exactly how to ground knowledge. Nonetheless, the Natural Law metaepistemologist
can simply say that facts about knowledge are grounded in facts about intellectual flourishing with specifying how the grounding
goes.  Or they might punt and say that the concept of knowledge is not essentially normative, and hence explaining how knowledge is
grounded is beyond the scope of the normative parts of metaepistemology---and perhaps it is only to the normative parts that the
Natural Lawyer has a distinctive contribution.

I think there is a plausible story about something in the vicinity of knowledge, however. We can think of two aspects of the
intellect, namely process and accomplishment, and both aspects have their distinctive flourishing. Justification will be a matter of
process flourishing. But knowledge seems more like accomplishment. Thus, we might try to identify an accomplishment of the intellect as 
knowledge provided that this output constitutes intellectual flourishing. However, this is not exactly right. For flourishing
intellectually in an accomplishment will include not just the aspects of knowledge acknowledged in the analytic tradition, like true 
belief and justification, but also \textit{understanding}, structural connections between different things found in the intellect that
enlightens us about the things that are important. Aquinas famously distinguishes the vice of \textit{curiositas} from the virtue of
studiousness, and the distinction lies in curiositas' trivial pursuit of mere bits of knowledge while the virtue pursues a holistic
structured understanding of the world's explanatory conenctions. Thus flourishing with respect to intellectual accomplisment is more
like what the medievals called \textit{scientia} than what we call knowledge.

So a Natural Law metaepistemology elegantly gives is a theory about the grounding of \textit{scientia} rather than of knowledge. 
Is this a disadvantage? Or is it a hint that in fact many of the blind alleys of post-Gettier epistemology have been due to the
fact that the more natural concept in the vicinity is the intellectual accomplishment of \textit{scientia} rather than mere 
knowledge?  ?????

\section*{Appendix: $^*$Approximating the pathological scoring rule with continuous ones}
We need to show that the stepwise scoring rule $(T,F)$ from ??backref can be written as a limit of
symmetric, strictly proper, finite and continuous scoring rules. 

First note that any symmetric continuous proper scoring rule $(t,f)$ can be written as the limit of symmetric continuous
strictly proper scoring rules by letting $t_n(x)=t(x)-(1-x)^2/n$ and $f_n(x)=f(x)-x^2/n$, since the Brier scoring rule defined
by the functions $-(1-x)^2$ an $-x^2$ is strictly proper, and the sum of a proper and a strictly proper scoring rules is
strictly proper. 

Thus, all we need to show is that is that we can approximate $(T,F)$ with symmetric, finite and continuous scoring rules.
Furthermore, we can drop the symmetry requirement. For write $f^*(x)=f(1-x)$. Then $(t,f)$ is a proper scoring rule if and
only if $(f^*,t^*)$ is a proper scoring rule. Now if $T(x)=\lim_n t_n(x)$ for all $x$ and $F(x)=\lim_n f_n(x)$, then
$$
    (T(x)+F^*(x))/2 = \lim_n (t_n(x)+f_n^*(x))/2
$$    
    and
$$    
    (F(x)+T^*(x))/2 = \lim_n (f_n(x)+t_n^*(x))/2. 
$$    
But $T=F^*$ and $F=T^*$, so the left-hand sides are just $T(x)$ and $F(x)$, respectively. Moreover, $(t_n+f_n^*,f_n+t_n^*)$ is 
will be a continuous symmetric finite proper scoring rule if $(t_n,f_n)$ is a continuous finite proper scoring rule.

Fix $\e>0$. Let $\phi_\e$ be a continuous non-negative finite function that is zero except on the set $U_\e=[0.999-\e,0.999)\cup (0.001,0.001+\e]$, 
and is such that $\int_{0.999-\e}^{0.999} (1-x) \phi_\e(x) = 1000$ and $\int_{0.001}^{0.001+\e} x \phi_\e(x) = 1000000$. 
Define 
$$
    T_n(x) = \int_{1/2}^x \, (1-u)phi_{1/n}(u) \, du
$$24
and 
$$
    F_n(x) = \int_{1/2}^x \, u \phi_{1/n}(u) \, du.
$$
By ??SchervishThm4.2, $(T_n,F_n)$ is proper. It is clearly continuous and finite. It is, further, easy to calculate that 
$T_n(x)$ and $F_n(x)$ equal $T(x)$ and $F(x)$ except perhaps on $U_{1/n}$.??check For any $x$ in $[0,1]$, there is an $N$
such that $x$ is not in $U_{1/n}$ for any $n\ge N$. It follows that $T(x)=T_n(x)$ and $F(x)=F_n(x)$ if $n\ge N$, and we have
the limiting condition we wanted.

\chaptertail 


