\def\mychapter{V}
\input{chapterhead}
\chapter{Epistemology}\label{ch:epistemology}
\section{Balancing doxastic desiderata}
I observe one raven, and it's black. I observe another and it's black, too. The story goes on. Every
raven I observe is black. After a certain number of ravens, in a sufficiently broad number of settings,
it becomes reasonable to believe that all ravens are black. But when?\footnote{I am grateful to Sherif Girgis for raising the issue of incommensurable desiderata in connection with these issues.}

William James famously identified two incommensurable doxastic desiderata: attainment of truth and avoidance of falsehood.
The larger the number of black ravens that are needed for me to believe that all ravens are black, the more surely
I avoid falsehood, but the more slowly I attain truth. Intuitively, there is room for differences between reasonable
people: some tend to jump to conclusions more quickly, while others are more apt to suspend judgment. But on either
extreme, eventually we reach unreasonableness. Both someone who concludes that all ravens are black based on one 
observation and someone who continues to suspend judgment after a million broadly spread observations are unreasonable.

There is, thus, a range of reasonable levels of evidence for an inductive belief. And, as in the myriad of ethical
cases of Chapter~\ref{ch:ethics}, this raises the Mersenne question: What grounds facts about the minimum amount of 
evidence required for an inductive inference and the maximum amount at which suspending judgment is still rational? 
Of course, the ``minimum'' and ``maximum'' may may depend on the subject matter, on higher-order evidence such as about 
how well previous inductive generalizations have fared, and even on pragmatic factors(??ref). But that added complexity does
nothing to make the Mersenne question easier to answer. And, as we discussed in ??backref, invoking vagueness does not
solve the problem, but multiplies the complexity even further.

And, of course, my contention will be that conformity to the human form is what grounds the answers for us. The
rational way to reason is the way that conforms to our form's specification of the proper functioning of our intellect.

It appears to be quite plausible that different answers to the rationality questions would be appropriate
for species of rational animals adapted to different environments. First, some possible worlds as a whole have 
laws of nature implying a greater uniformity and simplicity than that found in other worlds, and hence make it appropriate to make 
inductive inferences more quickly. Second, the specific environments that the rational animals evolved in may have greater or 
lesser uniformity, despite the same laws of nature---an environment where natural kinds are found has greater uniformity,
for instance. Third, the ecological niche occupied by the rational animals may
punish falsehood more or may reward truth more, either in general or in specific cases: perhaps for prey animals 
an oversensitivity to danger is not a problem. Thus the Aristotelian 
species-relative answer to the Mersenne questions is particularly appealing.??ref:Hawthorne

\section{Logics of induction}
Attempts have been made to give precise answers to questions about the reasonableness of inductive 
inferences using a rigorously formulated logics of induction.??refs Let us suppose, first, that some such
logic, call it $L_{12}$, does indeed embody the correct answers. Nonetheless, we will have a Mersenne question
as to why $L_{12}$, rather than one of the many alternatives like $L_{11}$ or $L_{13}$, is the logic by which we ought to reason 
inductively. 

In the case of truth-functional deductive logic, there is a system that appears to be both particularly natural and matches
our intuitions so well that it has gained a nearly universal following among philosophers, logicians, mathematicians
and computer scientists: two-valued boolean logic. No logic of induction has anything
like this following, which is unsurprising given that no logic of induction has the kind
of naturalness and fit with intuition that would privilege it over the others to a degree where it would seem
non-arbitrary to say that it is \textit{the} logic we should reason with.\footnote{Once we get to quantification
and modality, things become less clear.}

Further, observe that logics of induction can be divided into two categories: those with parameters (say, 
parameters controlling the speed of inductive inference--??refs) and those without. 
A logic of induction with parameters raises immediate Mersenne problems about what grounds the fact about which 
parameters, or ranges of parameters, are in fact rationally correct. 
A parameter-free logic of induction, however, is not likely to do justice to the fact that different ways of balancing
rational goods are appropriate in different epistemic and pragmatic contexts. Moreover, it is unlikely to do justice
to the intuition that the balancing should be different in different species of rational agents.

\section{Goodman's new riddle of induction}
All the emeralds we've observed are green, and it's reasonable to infer that all emeralds are green.
But Goodman's famous riddle notes that all the emeralds we've observed are also grue, but it's not reasonable infer 
that all emeralds are grue. Here, an emerald is grue if it is observed before the year 2100 and green, or if it is 
blue and not observed before 2100. According to Goodman, the predicate ``is green'' is \textit{projectible}, i.e., amenable to 
inductive inference, while the predicate ``is grue'' is not. But how do the two differ?

As Goodman notes, the fact that ``grue'' is defined in terms of ``green'' and ``blue'' does not help answer the
question. For if we specify that something is bleen if it is observed before 2100 and blue, or it is blue and not observed
then, then we can define something to be green provided it is observed before 2100 and grue or bleen and not observed in that time period, and similarly for ``blue'' with ``grue'' and ``bleen'' swapped. 

Whatever the \textit{justification} of inductive reasoning may be, it is clear that induction with ``green'' is reasonable, but not so with
``grue''. Notwithstanding Goodman's symmetry observations, ``grue'' is a gerrymandered predicate, as can be seen in
accounting for it in terms of more fundamental physical vocabulary. But now observe that ``green'' is also gerrymandered. 
Giving an account of colors is a difficult philosophical project, but to a first approximation an object is green provided that the wavelength profile of its reflected, transmitted and/or emitted light is predominantly 
concentrated somewhere in the range of $500$ to $570$~nm. The actual boundaries of that region are messy and appear vague, the measure of
predominant concentration is difficult to specify, and accounting for reflective, transmittive and emissive spectra
is a challenge. The full account in terms of more fundamental scientific terms will be complex and rather messy, though 
not as badly as in the case of ``grue'', which is more than twice as complex since it needs to account for blueness
and the date of ``2100'', which is a very messy date in more fundamental physics units (perhaps 
Planck times since the beginning of the universe?). Where the boundary between non-projectible and projectible lies---what
counts as too gerrymandered for projectibility---is an excellent Mersenne question.

There is a very plausible way to measure the degree of gerrymandering of a predicate. We take a language the content of 
whose symbols are terms for perfectly natural concepts, ones corresponding to fundamental joints in reality??ref:Lewis,
and we look for the shortest possible formula logically equivalent to the predicate, and say that the predicate is 
gerrymandered in proportion to the length of this formula. Now, to account for ``is grue'' in terms of fundamental
terms, we need an account of greenness, an account of blueness and, worst of all, an account of observation, so
``is grue'' is much more gerrymandered than ``is green''.

But this approach does not yield the correct judgments about inductive reasoning. Let $t_1$ abbreviate ``$10^{61}$ Planck times from the beginning of
the universe''. Since Planck times are a rather natural unit, the specification of $t_1$ is pretty natural. Note
that $t_1$ is about three billion years in the future.  Say that something is ``pogatively charged'' provided that
it is positively charged and came into existence earlier than at $t_1$ or it is negatively charged and came into existence 
no earlier than at $t_1$. All the protons we have seen are pogatively charged. But we should not conclude that
all protons across time are pogatively charged, and hence that in about three billion years whenever new protons are formed,
they will be negatively charged. However, notice that by the formula length account, ``is green'' is significantly
more gerrymandered than ``is pogatively charged''. Pogative charge is much closer to the fundamental than specific
human-named colors like green. It seems, thus, that our Mersenne question
about the boundary between the non-projectible and projectible is not merely defined by a single number---a threshold such that
predicates definable with a length below that number are projectible. 

Perhaps, however, what is going on here is this. The hypothesis that all emeralds are grue is beaten by the hypothesis that 
all emeralds are green, even though both fit with observation. Similarly, the hypothesis that all protons are pogatively charged 
is beaten by the hypothesis that all protons are positively charged. So perhaps rather than an absolute concept of projectibility, we
have a relation of relative projectibility: ``is green'' is projectible relative to ``is grue'' and ``is grue'' is 
non-projectible relative to ``is green''. 

We can once again try to account for this in terms of the complexity of formulae. But now we need to compare the complexity of
two formulae. The most natural comparison is that $P$ is projectible relative to $Q$ provided that in perfectly
natural terms $Q$ is more complex than $P$. But a small difference in complexity does not make for a difference
in whether a term is projectible or not. We shouldn't say that ``is blackish'' is projectible relative to 
``is brownish'', just because ``blackish'' is slightly easier (I assume) to define in fundamental terms. 
Thus our account will say something like: $P$ is projectible relative to $Q$ provided that 
$F(\ell(P),\ell(Q))<\lambda$ where $\ell(R)$ is the length of the shortest expression of $R$ in perfectly
natural terms, $F$ is some sort of a comparison function, say $F(n,m)=n-m$ or $F(n,m)=n/m$, and $\lambda$ is
a threshold. Thus, while previously we had a single numerical threshold 
as our parameter of projectibility---predicates with complexity lower than the threshold are projectible---we now have 
both the threshold $\lambda$ and the function $F$ to be specified.

Furthermore, while the idea of a language all of whose terms reflect fundamental joints in reality can be defended, the grammar
of the language will make a difference to the precise complexity measurements. For instance, if we have the fundamental predicates 
$Cx$, $Dx$ and $Ex$, then the complex formula expressing the predicate ``is $C$ as well as either $D$ or $E$'' will be 
$$
    Cx \And (Dx \Or Ex)
$$    
    in infix notation, and hence five times longer than the formula $Cx$ represening ``is $C$'', but in Polish notation 
    will be 
$$
    KCxADxEx
$$    
and hence only four times longer than $Cx$.

For a relative projectibility relation defined in terms of linguistic complexity, we thus have at least three free parameters,
each a fit subject for a Mersenne question: a threshold, a comparison function, and a grammar for the basic language.

But in fact we probably should not think of a binary projectible / non-projectible distinction, whether
relational or absolute. As Goodman himself observed??ref-in-https://www.jstor.org/stable/pdf/686416.pdf, what we have instead is a range of predicates that are more or less projectible. We have ``is green'' and
``is grue''. But we can also say that $x$ is grue$^*$ provided that $x$ is green and observed by a French speaker before 2100 or by a 
non-speaker of French before 2107, or never observed, and ``grue$^*$'' will be less projectible than ``is grue''. On the basis of our observations, the
probability that all emeralds is green is very high, and the probability that they are all grue or grue$^*$ is very low. But
nonetheless, the probability that they are grue is somewhat higher than that they are grue$^*$. After all, an alien conspiracy to
re-color emeralds upon observation with a sharp cut-off in one year seems a little bit less unlikely than one where the cut-off 
depends on whether the observer speaks French. Similarly, it makes sense to think of ``is green'' as less projectible than ``is positively
charged'', and of ``is cute'' as even less projectible. 

Projectibility now becomes a matter of degree. An advantage of this is that perhaps we no longer need to make it relational.
The reason for the superiority of the green-hypothesis to the grue-hypothesis and for the positive-charge-hypothesis to the
pogative-charge-hypothesis can be given in terms of the relationship between the degrees of projectibility. However, 
we will need a function from predicates to degrees of projectibility, and the choice of that function will have infinitely
many degrees of freedom.

\section{Epistemic value}
\subsection{Epistemic value on its own}
\subsubsection{Scoring rules}
Plausibly, the more sure you are of a truth, the better off epistemically you are, and similarly the more sure you are
of a falsehood, the worse off you are. 

But what exactly is the dependence of value on the degree of certainty? Fix some hypothesis $H$ and let $T(p)$ be the epistemic value of 
having degree of belief or credence $p$ (where $0\le p\le 1$) in $H$ if $H$ is in fact true and let $F(p)$ be the value of credence
$p$ in $H$ if $H$ is in fact false. The pair $T$ and $F$ is called an accuracy scoring rule in the literature.??ref 

We can put some plausible constraints on $T$ and $F$. First, $T(p)$ cannot decrease if $p$ increases,
and $F(p)$ cannot increase if $p$ decreases.\footnote{We might more strongly specify that $T(p)$ always strictly increases with $p$, and $T(p)$
strictly decreases. That is
plausible, but one might also have a view on which there is a finite number of discrete thresholds at which increase/decrease happens.}
But that still leaves infinitely many degrees of freedom for the selection of $T$ and $F$.

We can, however, make some progress if we reflect on expected values. If your current credence in $H$ is $p$, then by your lights
there is a probability $p$ of your having epistemic score $T(p)$ and a probability $1-p$ of your epistemic score being $F(p)$, so
your expected score is:
$$
    p T(p) + (1-p) F(p).
$$
Suppose now you consider doing something odd: without any evidence, you might brainwash yourself to switch your credence from $p$ to some 
other value $p'$. By your current lights, the expected epistemic value of this switch is:
$$
    p T(p') + (1-p) F(p').
$$
And this shouldn't be higher that the expected epistemic value of your actual credence $p$. For surely by the lights of your
assignment of $p$ to $H$, no other credence assignment should be expected to do better. Indeed, if another credence assignment
$p'$ were expected to do better by the lights of $p$, then $p$ would be some kind of a ``cursed probability'', one such that
if you assign it to $H$, then immediately expected value reasoning pushes you to replace it with $p'$. This is not rational.
So, it is very plausible indeed that:
$$
    p T(p) + (1-p) F(p) \ge
    p T(p') + (1-p) F(p').
$$
If $T$ and $F$ satisfy this inequality for all $p$ and $p'$, we say that the pair $T$ and $F$ is a \textit{proper} scoring rule.
And if by the lights of the assignment of $p$ to $H$, that assignment has better expectation than any other, i.e., if the
inequality above is strict whenever $p\ne p'$, we say that the rule is \textit{strictly proper}.

Propriety mutually constrains the choice of $T$ and $F$. Given any non-decreasing function $T$, there is a
function $F$ that is unique up to an additive constant such that the parir $T$ and $F$ is a proper scoring rule, and conversely given
any non-increasing function $F$, there is a $T$ unique up to an additive constant such that $T$ and $F$ is a proper scoring rule.???
Hence, once we have one of the two functions, the other is almost determined. However, at the same time, this result shows what
a profusion of proper scoring rules there is: for every non-decreasing function, there is a proper scoring rule that has that as 
its $T$ component.

The question of epistemic value assignment may seem purely theoretical. However, it has real-world ramifications. 
Suppose a scientist has attained a credence $p$ in a hypothesis $H$, and is considering which of two experiments to
perform. One experiment will very likely have a minor but real effect on the credence in $H$ (think here of a case
where you've gathered $1000$ data points, and you now have a chance of gathering $100$ more). The other will most
likely be turn out to be irrelevant to $H$, but there is a small chance that it will nearly conclusively establish
$H$ or its negation. For each experiment, the scientist can use their present credence assignments to estimate the
probabilities of the various epistemic outcomes, and can then estimate expected epistemic values of the outcomes.

It is well-known??ref that if the scoring rule is strictly proper, for each experiment that has potential relevance 
to $H$, the expected epistemic value of performing the experiment is
higher than the expected epistemic value of the \textit{status quo}. Thus if the experiments are cost-free, it is
always worth performing more experiments, as long as the appropriate scoring rule is strictly proper,
and it does not matter which strictly proper scoring rule we choose. But if in addition to deciding whether to perform
another experiment, the decision to be made is \textit{which} experiment to perform, then the choice of scoring rule
will indeed be important, with different strictly proper scoring rules yielding different decisions.??ref:fill-in

There are a number of mathematically elegant strictly proper scoring rules, such as the Brier quadratic score, the spherical score 
and the logarithmic score. Of these, the logarithmic score is the only that is a serious candidate for being \textit{the} correct
scoring rule, in the light of information-theoretic and other arguments (??ref:phil of sci paper). In our setting where we are
evaluating the value of a credence in a single proposition $H$, the logarithmic score is $T(r) = \log r$ and $F(r) = \log (1-r)$. 
However, there are also reasons to doubt that the logarithmic score is the One True Score. 

\subsubsection{The logarithmic score is not the One True Score}
There is an immediate intuitive problem for the logarithmic score. If you are certain of a falsehood, your logarithmic score is $\log 0 = -\infty$, while
if you are certain of a truth, your score is $\log 1 = 0$. Now, while there is good reason to think that the disvalue of being
sure of a falsehood exceeds the value of being sure of a truth, it is somewhat implausible that it infinitely exceeds it. 

For the next two problems, note that logarithmic scores and the arguments for them only really come into their own when we are 
dealing with more than two propositions (in our above setting, we had $H$ and $\sim H$ are the only relevant possibilities). Suppose 
we are dealing with $n$ primitive possibilities or ``cells'', $\omega_1,\dots,\omega_n$ (say, the sides of an $n$-sided die), and that our agent 
has assigned credence $p_i$ to $\omega_i$. If in fact $\omega_i$ eventuates, the logarithmic score yields epistemic value 
$\log p_i$. 

One of the merits touted for the logarithmic score is that the epistemic value depends only on the credence assigned 
to the cell that eventuates.  But this is also
a serious demerit. Suppose that you and I are trying to figure out how many jelly beans there are in a jar. Let's say that our range of
possibilities is between 1 and 1000. I look very quickly and assign equal probability $1/1000$ to each number. You count very carefully
and arrive at 390. But then you think that although you are really good at counting, you might be off by one. So you assign $998/1000$
to 390, and $1/1000$ to each of 389 and 391. It turns out that the number is 391. We both have the same logarithmic score, $\log (1/1000)$,
since we both assigned the same probability $1/1000$ to cell 391. But intuitively your assignment is much better than mine: you are better
off epistemically than I.

Finally, observe that in real life, credences are not consistent---do not satisfy the axioms of probability. And the logarithmic score
allows one to have extremely inconsistent credences and still do well. If I assign credence $1$ to \textit{every} possible outcome, I am
guaranteed to max out the logarithmic score no matter what. Thus one of the least rational credence assignments results in the best
possible score.

\subsubsection{Two approaches}
We now have two different approaches to the Mersenne questions about epistemic value and scoring rules. First, we could suppose that there
is such a thing as \textit{the} One True Score. Since only the logarithmic score seems significantly mathematically privileged over all
the other scores, and the logarithmic score is not the One True Score, there will be a significant appearance of contingency about the
One True Score even if there is one.

Second, and this seems the better approach, we might suppose that just as rational people can differ in prudential preferences, they can differ in epistemic preferences. 
Some may, for instance, have a strong sharpish preference for gaining near-certainty in truths, while being fairly indifferent whether their
credence in a truth is $0.6$ or $0.8$, as neither is that close to certainty. Others, on the other hand, may value increased certainty in
a gradual way, somewhat like the logarithmic rule does. 

However, it is important to note that while there may be room for rational people to differ in epistemic preferences, there is reason to
think that there are rational constraints on epistemic preferences that go beyond formal conditions such as strict propriety, continuity
or symmetry---where the last is the condition that $T(p)=F(1-p)$. 

Let $T_0(x)=1000$ if $x\ge 0.999$, $T_0(x)=-1000000$ if $x \le 0.001$, and $T_0(x)=0$ otherwise. Let $F_0(x)=T_0(1-x)$.
Then the pair $T_0$ and $F_0$ is a symmetric and proper scoring rule (see this chapter's Appendix).

Consider now a scientist who adopts this scoring rule for some hypothesis $H$ of minor importance about some chemicals in 
her lab that she initially assigns credence $1/2$ to. She has a choice between two methods. She can use beat-up machine $A$
that she has in her lab, which displays an answer to the question of whether $H$ is true, but for either
answer there is a $0.11\%$ chance that the answer is wrong. Or she can use spiffy new machine $B$ which has the slightly lower $0.09\%$ chance 
of error either way. The only problem is that her lab doesn't own machine $B$ and her grant can't afford the price. Her only 
hope for using machine $B$ is to go and buy a scratch-off lottery ticket which has a one in a million chance of yielding a prize exactly
sufficient to purchase machine $B$.

Suppose her only options are using machine $A$, and then learning whether $H$ is true with a credence of
$0.9989$, or buying a lottery ticket, which gives her a one in a million chance of learning whether $H$ is true with
a credence of $0.9991$ and a $999,999$ out of a million chance of being no further ahead. Going for the second option
seems irrational if all that is at stake is epistemic value: the difference between $0.9989$ and $0.9991$ is just not
worth it given that most likely the lottery route won't yield anything.\footnote{If what was at stake
wasn't epistemic value but something pragmatic, then things could be different. We could imagine a law where some 
life-saving medication can be administered to a patient only if we have confidence greater than $0.999$ that it'll work, and then
there will be no practical difference between $1/2$ and $0.9989$, but a big one between $0.9989$ and $0.9991$.}

But a scoring rule like the one described above prefers the lottery option. For the epistemic value of using 
machine $A$ is guaranteed to be zero since after using machine $A$, the scientist will have credence $0.9989$ or $0.0011$, 
depending on whether the result favors $H$ or not, and a credence of $0.9989$ or of $0.0011$ gets assigned a value
of zero by both $T_0$ and $F_0$.

On the lottery option, however, conditionally on getting a winning ticket, it is equally likely given the 
scientist's priors that the machine will return a verdict for or against $H$, which will result in a credence 
of $0.9991$ or $0.0009$, respectively, and in either case there will be a $0.0009$
chance that the verdict is erroneous. Since $F_0(0.0009)=T_0(0.9991)=1000$ and $T_0(0.0009)=F_0(0.9991)=-1000000$, it follows 
that the expected epistemic value, conditionally on winning the lottery, will be:
$$
    \frac{0.9991 \cdot 1000+0.0009\cdot (-1000000)}2+\frac{0.9991 \cdot 1000+0.0009\cdot (-1000000)}2 = 99.1 > 0.
$$
And if we multiply this by the $1/1000000$ chance of winning the lottery, we still have something positive, so
the expected epistemic value of playing the lottery with the plan of using machine $B$ is positive, while that of
using machine $A$ is zero. 

Thus, by considerations of epistemic value, the scientist with this scoring rule will prefer a $1/1000000$ chance of 
gaining credence $0.9991$ as to whether $H$ is true to a certainty of gaining the slightly lower credence $0.9989$.
This is not rational. 

Now, in the above example, our scoring rule while proper, symmetric and finite, was neither continuous nor strictly proper. 
However, we will show in the Appendix??ref that there is a sequence of continuous, strictly proper, finite and symmetric
scoring rules $T_n$ and $F_n$ such that $T_0(x)=\lim_{n\to\infty} T_n(x)$ and $F_0(x)=\lim_{n\to\infty} F_n(x)$ for all $x$.
If $n$ is large enough, then the pair $T_n$ and $F_n$ will require exactly the same decision from our scientist as 
$T$ and $F$ did, since the expected values of the expected $(T_n,F_n)$-scores of the two courses of action will converge
to the expected values of the expected $(T,F)$-scores.

Hence not all epistemic valuations that satisfy the plausible formal axioms are rationally acceptable, and so we 
will have Mersenne questions about what grounds the further constraints on the epistemic valuations. These constraints
are likely to include messy prohibitions, with multiple degrees of freedom, on the kinds of sharp jumps that our
pathological scoring rule above exhibited as well as on the less sharp jumps that the approximating continuous
scoring rules had.

Furthermore, things become more complicated when we consider that the epistemic value of a credence in a truth will
differ depending on the importance of that truth. Getting right whether mathematical entities exist or whether humans are 
purely physical or how life on earth started has much more epistemic value than getting right Napoleon's shoe size at age 20. Epistemic
value will thus not only be a function of credence and truth, but also of subject matter. Moreover, we will have 
further degrees of freedom concerning the operation of combining epistemic values for different propositions---addition
may seem a plausible operation, but the logarithmic and spherical rules are not combined additively across propositions.

We thus have multiple indicators of a contingency about epistemic value assignments. And there is good reason to think
that different forms of life are more suited to different epistemic value assignments. The most obvious aspect of this
is that once we move away from the toy case of assigning a value to one's epistemic attitude to a single proposition
and consider that attitudes to a large number of propositions need to be considered, it is obvious that the subject
matter of the propositions will affect how great a weight we give credences about them in the overall evaluation. And
the importance of subject matter obviously depends on the form of life. It is plausible that for intelligent agents whose
natural environment is more hostile it would be more fitting to have a greater epistemic 
value assigned to practical matters, while agents that have fewer natural enemies and can get food easily might more fittingly
have a greater epistemic value assigned to theoretical matters. One imagines here that intelligent antelope might be
properly expected to be less philosophical than intelligent elephants.


\subsection{Connection of epistemic values with other values}
Suppose a scientist is deciding whether to go home to be with family or perform an experiment at the end of a long day, an experiment that 
cannot be done on another date. 
The scientist weighs the epistemic goods arising from the experiment against the value of interacting
with family. What is the right thing to do depends here on many unspecified factors. How much of a delay
in returning home would the experiment create? What is the current state of the scientist's relationship 
with the family and what is their current level of need? What
practical benefits, if any, could accrue to the scientist, the scientist's family, or humankind from the
experiment? And, finally, how important is the hypothesis being tested and 
how much can the experiment be expected to contribute to confirming or disconfirming the hypothesis? 

Only the last question adverts directly to epistemic value. It could be that the answers to the preceding questions suffice
to determine whether scientist should go home. Perhaps the scientist's progress towards tenure is crucial to the 
family's livelihood. Or perhaps there is a sufficiently high chance that the experiment's answer will contribute
to a cure for cancer that apart from any epistemic goods it is worth staying. Or, alternately, perhaps the scientist
has a commitment to family to go home on time this evening, and regardless of the epistemic goods involved, that
commitment needs to be honored. 

But of particular present interest is what should be done if the non-epistemic
goods do not answer the question of what the scientist should do. In that case, one needs to weigh the
epistemic goods against the non-epistemic ones. There is no automatic priority of one over the other.
Some purely epistemic goods are great enough that significant sacrifice of non-epistemic goods is worthwhile.
Thus, much sacrifice would make sense if the outcome would be to know whether there is life in other galaxies,
even if that knowledge has no practical value to us. On the other hand, very little sacrifice
would be worthwhile to find out whether the lab's largest rat has an even or odd number of freckles.

If we could find the right scoring rule, that would let us compare different epistemic values. But we would
still need a way to weigh the outputs of the scoring rule with non-epistemic values. And then we will have
our old familiar appearance of contingency. For whatever is the right way of comparing these values, we can
imagine beings slightly different from us for whom a different comparison is appropriate: beings that ought
to be more contemplative or more practical. And we have the familiar Mersenne question about why it is that 
for us the weighing goes as it does, rather than in some other way.

\section{Bayesianism}
\subsection{Introduction}
Bayesianism is the best developed picture of what a precise and rigorous account of epistemic rationality would be like.
It is thus worth looking carefully at what kind of answers the Bayesian could give to the questions we have been asking
as a model of the kind of answers we can hope for in a well-developed theory.

On Bayesianism, as was already the case for  scoring rules, the main interest is not in beliefs as such but in the agent's credences, which come in degrees. 
I will assume the most familiar model where credences are quantified by probabilities ranging from $0$ to $1$. Other models include qualitative probabilities where instead of a specific value being assigned one has fundamental
comparisons between probabilities, interval-valued probabilities where instead of a specific numerical
probability, a range of numerical probabilities are assigned, and accounts on which conditional probabilities
are fundamental.??refs Most of the issues raised will apply 
\textit{mutatis mutandis} to the other models.

On our model, if a perfectly rational agent assigns credence $C(p)$ to a proposition $p$, then these credences will 
satisfy plausible axioms for probabilities, such as:
\begin{enumerate}
\item[(i)] Non-negativity: $0 \le C(p)$
\item[(ii)] Normalization: $C(p)=1$ if $p$ is necessarily true
\item[(iii)] Finite Additivity: $C(p\Or q)=C(p)+C(q)$ if $p$ and $q$ are mutually exclusive.
\end{enumerate}

Furthermore, perfectly rational agents update their credences on receipt of evidence by conditionalization.
In other words, upon receipt of evidence $E$, the ``prior'' credence of $p$ goes from $C(p)$ to the 
``posterior'' conditional credence $C(p\mid E)$. If $C(E)>0$, then we define $C(p\mid E)=C(p\And E)/C(E)$.\footnote{There 
are some technical issues with handling cases where $C(E)=0$, say if one could find out that a spinner
has landed in a precise location. However, such cases may not occur in practice.} This conditionalization is 
designed to ensure the plausible rule that if $p$ and $q$ each entail the evidence $E$, then the ratio of the 
credences of $p$ and $q$ does not change upon learning $E$. 

\subsection{Induction and priors}
From a Bayesian point of view, how induction works is determined by the probabilities prior to all evidence, the ur-priors.
Suppose, for instance, that I assign equal prior probability to every logically possible color sequence of observed ravens. 
For simplicity, suppose that there are only two colors, white and black.  I find out that there are a million ravens, and 
I observe a thousand of them, and find them all black. I am about to observe another raven. The probability that the next 
raven will be black will be $1/2$. For the sequence $B,...,B,W$ (with $1000$ $B$s) is just as likely as the sequence
$B,...,B,B$ (with $1001$ $B$s), and both sequences fit equally well with the observations. 

On the other hand, suppose I assigned probability $1/3$ to the hypothesis that all ravens are white, $1/3$ to all black,
and split the remaining $1/3$ equally among the $2^{1000000}-2$ multicolor sequences. My observation of the first $1000$
ravens then rules out the all-white  hypothesis. And it rules out most of the multicolor sequences: there are $2^{999000}-1$
remaining multicolor sequences that start with $1000$ black ravens, which is a tiny fraction of the original $2^{1000000}-2$. Since as a 
good Bayesian I keep the ratios between the probabilities unchanged when the events entail the observations, each of the remaining multicolor sequences has 
$1/(2^{1000000}-2)$ of the probability of the all-black sequence, and since there are $2^{999000}-1$ multicolor
sequences remaining that yield the observation of the first thousand ravens being black, the ratio between the multicolor probability and the all-black probability
is $(2^{999000}-1)/(2^{1000000}-2)$ to $1$, or approximately $1$ to $2^{1000}$, while the all-white hypothesis
is completely ruled out. Thus, we have overwhelming confirmation of 
the all-black probability, and hence an even more overwhelming probability of the hypothesis that the next raven will be black.

Other ways of dividing the probabilities between the hypotheses yield other results. Carnap??ref, for instance, had a division
that worked as follows. For each number $n$ between zero and a million we have the hypothesis $H_n$ that there are exactly 
$n$ black ravens, and Carnap proposed that the million-and-one such hypotheses should have equal probability, and then 
each hypothesis $H_n$ is divided into equally likely subhypotheses specifying all the subhypotheses that make there be $n$
ravens. Thus, $H_0$ and $H_{1000000}$ have only one subhypothesis: there is only one way to have no-black or all-black. But
$H_1$ and $H_{999999}$ have a million subhypotheses each: there are a million options for which raven has the outlying
color. 

Using the same constant-ratio Bayesian technique as before, after observing $1000$ black ravens, the probability that the next one is black 
turns out to be approximately $0.999$, but the probability that all million are black will only be about $0.001$. More generally, if there
are $N$ ravens, and the first $m$ of them have been observed to be black, and $n\ge m$, then the probability that the first $n$
will be black will be $(1+m)/(1+n)$.\footnote{$^*$Let $B_m$ be the claim that the first $m$ ravens are black. Then
for $n\ge m$ we have $P(B_m\mid H_n)={N-m \choose n-m}/{N\choose n}$ since there are $N\choose n$ maximally specific subhypotheses of $H_n$, and there are ${N-m\choose n-m}$ of these that have the first $m$ ravens be black. 
Thus $P(B_m)=(N+1)^{-1}\sum_{n=m}^{N} {N-m \choose n-m}/{N\choose n} = \frac{1}{1+m}$, where the sum was
calculated with Mathematica software and is a sum of partial factorials. The probability that the first
$n$ are black given that the first $m$ are black where $n\ge m$ will then be $P(B_n\mid B_m)=(1+m)/(1+n)$.} 
  Hence we have very good reason to think that the \textit{next} raven is black,
but unless we have observed the bulk of the ravens, we won't have reason to think that all the ravens are black. 

Intuitively, while Carnapian probabilities do support induction, they make induction too slow---it is only when we have observed the bulk of the cases
being a certain way that we get to conclude that they are all like that. On the other hand, my $1/3$--$1/3$--$1/3$ division is too fast. Even with
$1000$ black ravens having been observed, the probability of a white raven shouldn't be \textit{astronomically} small in the way
that $1/2^{1000}$ is. Reasonable priors, thus, yield a speed of induction somewhere between these. 

We can presumably gerrymander a formula for the priors which will fit with our intuitions of how fast induction should work.
For instance, we could take Carnap's setup, but increase the prior probability of the all-white and all-black raven hypotheses. 
But such an increase would be apt to involve one or more parameters. If the specific assignment of priors were rationally
required of us, then we would have the Mersenne question of why it is these and not some other very similar priors that are required.
And if there is a range of priors rationally permitted to us, then we would have Mersenne questions about the boundaries of
this range. 

Further, imagine beings other than us that inhabit a more Carnapian world than we do. While in our world, we have a significant number of natural kinds
that exhibit or fail to exhibit some basic property exceptionlessly---for instance, every electron is charged, and no photon 
has mass---in that world there are few such natural kinds. Instead, if we were to tabulate the frequencies of basic binary properties
in various natural populations---say, tabulating the frequency of blackness among ravens, charge among electrons, mass among photons---we 
would find the frequencies to be spread out pretty uniformly between $0$ and $1$. In that world, Carnapian priors would lead to the truth faster
than the more induction-friendly priors that we have. And let us imagine that in that world we have intelligent beings who reason according
to Carnapian priors. Even if we happily grant that Carnapian priors are irrational for us, it seems plausible to think that they could be
rational for those beings. To insist that these Carnapians are irrational, because it would be irrational for us to have these priors,
seems akin to saying that bigamy would immoral for aliens who need three individuals to reproduce, or that there is something wrong with sharks
because they lack lungs. 

Consideration of the rationality of induction thus once again reveals an appearance of contingency in the normative realm, which
once again yields an argument for an Aristotelian picture of human nature, where the rationally required priors or ranges of priors
are those that we are impelled to by our human nature.

\subsection{Anti-skepticism}
There are skeptical hypotheses that predict the same observations as our best non-skeptical theories
about the world. Indeed, for any reasonable theory $T$ of the world, there is a variety of skeptical hypotheses about
how the world merely looks as if $T$, say due to an evil demon, or us being in a simulation, or random quantum 
fluctuations in a Boltzmann brain. If an as-if-$T$ skeptical theory $T'$ starts off with probabilities of the same
order of magnitude as the probabilities of $T$, because all of the evidence fitting with $T$ fits equally well with $T'$, 
no matter how much evidence we gather, the probabilities of $T'$ and $T$ will remain within an order of magnitude---indeed will have the same
ratio. For instance, if
$T'$ starts off at half of $T$'s probability, then it will remain at half of $T$'s probability, and in particular, 
it follows that $T$ can never have more than $2/3$ probability, no matter how much evidence we gather for it,
since we will always have $1 \ge P(T')+P(T) = (1/2)P(T)+P(T) = (3/2)P(T)$. 

A sane epistemology that lets us confirm reasonable theories to a degree bigger than, say, $2/3$, thus requires low 
priors for skeptical hypotheses. How is it going to achieve this?
Perhaps the best initial candidate would be a strong preference for simplicity. We might then expect to assign a lower probability to the hypothesis $T'$ 
on which an evil demon makes things seems as if theory $T$ were true on the grounds that the evil demon complicates 
the story. But on reflection, an evil demon need not \textit{greatly} complicate the story, and may even simplify it 
in some respects.

For instance, if $T$ is the true philosophical and scientific story about the world (assuming that in fact
the skeptical hypotheses are false), then $T$ will need to have a complex account of the mind-body problem---either 
of how consciousness and intentionality can be grounded in a physical system or of how non-physical mental states 
and physical states interact. But a skeptical hypothesis can be entirely
Berkeleian, supposing a non-physical evil ``demon'' to impose phenomenal states on a purely non-physical being, and supposing
the non-physical evil ``demon'' to have precisely the causal powers needed for that. Likewise, 
a real physical world that fits with our best theories has vast amounts of complexity beyond our observational
abilities: details of the behavior of the world too small or too far away for us to observe them. But an 
evil demon world need not include any of that complexity. Only what's empirically relevant to us needs to be included 
in the evil demon's deception. If the evil demon is just deceiving \textit{me}, the evil demon's deception at most 
needs to encompass a small pretend-universe of radius of about a hundred light-years, since anything outside that radius is
irrelevant to my observations. Nor need the evil demon think about the exact positions of all the particles in that sphere:
the evil demon need only work with an approximate physics good enough to fool me. 

Similar points apply to a number of other skeptical hypotheses. A theory on which I am a Boltzmann brain that's been 
floating for five minutes in a bubble of oxygen in an otherwise empty universe, about to die once the oxygen
dissipates, involves much less in the way of informational complexity than the world of our best theories which posit
vastly many more degrees of freedom due to there being many orders of magnitude more particles.

One might think a simulation hypothesis---that I inhabit a computer simulation---would involve significant complexity,
because not only would I need to be simulated, but we would have all the complexity of the physical cosmos in which the
simulating computer lives. But that physical cosmos need not be nearly as complex as the universe of our best theories.
It could, for instance, be a cosmos optimized for computing. Whereas in our world, the logic gates, memory cells, 
oscillators and wires of computers are built from many atoms each, and the atoms themselves
are made of multiple particles with very significant informational complexity due to their apparently analogue (or at least
very high resolution discrete) nature, we could suppose a cosmos whose basic particles \textit{are} logic gates, memory
cells, oscillators and connectors, and which have only the degrees of freedom relevant to their computational role. Such a 
cosmos could be vastly simpler than the cosmos of our best physics. 

Complexity considerations thus do not appear to let us escape from skeptical hypotheses. What else can we do? Well, one option is to just
go with common sense: assign low priors to skeptical hypotheses because they are ``crazy''.
But it seems like an anthropocentric cheat to 
build such an anti-skeptical bias into the conditions for our priors. It would be odd to think that our priors should
be such as to please our intuitions.

But this is not at all odd on Aristotelian optimism. If our world is as Aristotelian optimism has it, we would expect
the structure of our priors to match our world to some degree, and hence for the priors to reject skeptical hypotheses. Assuming
that in fact skeptical hypotheses are false, and that without presupposing their falsity it is difficult to engage
in any sort of reasoning, it is unsurprising that our human nature simply tells us to assign them low priors. 
It's both epistemically and holistically good for us to do that, common-sense agrees with this, and on optimistic Aristotelianism that some practice is common and good for us is evidence that the practice is normative.

The above discussion was formulated in Bayesian terms. But similar points apply to any reasonable epistemological framework.
We need some way to choose a non-skeptical over a skeptical hypothesis, even if the skeptical one fits our observations just 
as well. And an Aristotelian story on which our anthropocentric intuitions of what is and is not a ``crazy'' hypothesis 
are normative for us is a good account of why this kind of anthropocentric intuition is appropriate.

Outside of an Aristotelian story, however, it is difficult to see how to explain why
skeptical hypotheses consistently should have very low priors. Moreover, this kind of anthropocentric approach 
allows for the possibility that beings of a different sort, whose natural niche is a different kind of 
environment---say, non-material beings whose proper environment is a Berkeleian world of spirits---would have
different norms of reasoning, and what to us is a skeptical hypothesis might to them be the most common-sense thing.

But before we embrace the Aristotelian account, we should consider some Bayesian responses.

\subsection{Subjective Bayesianism}
Subjective Bayesians avoid all the difficulties of specifying permissible priors by merely
requiring the (initial, ultimate or ur-) priors to satisfy some formal properties. These are taken to include the
axioms of probability and, sometimes, the regularity constraint that all contingent 
propositions have non-zero probability. Rationality then constrains transitions from one
set of probabilities to another: these must follow the Bayesian update rule that upon
receiving evidence $E$, one's probability in a hypothesis $H$ goes from $P(H)$ to 
$P(H\mid E)$. But the initial choice of priors is up to the individual, subject to the
formal constraints.

The resulting picture of rationality does not match common sense. Take the most
ridiculous set of conspiracy theories that we would all agree is unsupported by our evidence, 
but where nonetheless the conjunction of the theories is logically consistent with the evidence.
Then there is a possible assignment of priors such that updating in the Bayesian way on our actual 
evidence strongly confirms the conjunction of these theories, both in the incremental sense of 
greatly increasing that probability and in the absolute sense of making that probability high. (All 
we need is that the prior probability of the conjunction of the theories be low, but that the conditional
probability of that conjunction on the conjunction of our evidence be high.) Yet to reason that
our evidence strongly supports these theories is paradigmatic of irrationality. It shouldn't be the
case that a rational person could come to exactly the same conclusions on exactly the same evidence
as a paradigm of irrationality would.

Or consider the implausible asymmetry between the freedom in choosing initial priors and the
rigid constraint in updating. Suppose I don't like my current rather high evidentially-based posterior for 
the hypothesis that I will die or lose my mind by age 105, and I would feel better if I had a cheery high posterior for 
the hypothesis that I will live with full mental faculties past 105.  Then there will be a set of priors 
such that, given the evidence $E$ that I had received over my lifetime, would have yielded the cheery 
posterior.\footnote{I just need to assume priors $P$ where $P(E\And L_{105})$ is much higher than 
$P(E\And \sim L_{105})$ and $P(L_{105})=1/2$, where $L_{105}$ is the hypothesis that I will live past 105 with full mental faculties.} According to 
the subjective Bayesian there would have been nothing irrational in having adopted those priors in 
the first place, and thus having ended up at the cheerier posteriors. I didn't adopt those priors, 
but why should I be tied to the priors that I actually had? 

The picture of priors here is like a rationally unbreakable vow to live one's life as an evolution
of these priors under Bayesian application of evidence. But it is a vow made without any rational 
ground, indeed without any choice, and likely in childhood. We would think it unsupportable that
someone be held committed for life to a promise made early in childhood. If we think with the subjective
Bayesian that the initial priors are rationally arbitrary, why then should we be bound to
them and hence to the posteriors coming from them? Yet if we are not bound to them, then subjective
Bayesianism just turns into nearly total subjectivism (maybe subject to some consistency constraints).

Let us now consider some non-subjective responses.

\subsection{Indifference}
An intuitive way to solve the problem of priors is to place a constraint on one's priors that 
epistemically equivalent situations get equal credences. This is essentially a version of the 
principle of indifference: For each side of a perfect die, the proposition that the die will land 
with that side up is epistemically equivalent, so our prior credences should be indifferent between
the sides. Assuming that the logical features of the situation necessitate that exactly one of the six 
sides eventuates, the finite additivity of probabilities ensures that each side has probability $1/6$.

There are many well-known problems with this approach. Some of these are technical??refs, but I will
focus here on two non-technical ones.

First, in many cases it is
difficult to see how we should carve up the space of possibilities into equivalent options. Should we,
for instance, divide grand metaphysical views  into naturalism and non-naturalism, with the two getting
equal probability $1/2$, or should we have a finer-grained division into naturalism, anti-naturalism
(nothing is natural) and dualism (there are both natural and non-natural things), with each getting 
probability $1/3$? Or should we have a longer list, with dualism being split into Platonism, monotheism,
polytheism, etc., but naturalism left as one item, now of rather low prior probability? 

Second, as we basically already saw in ??backref, if we are not careful, we will undercut induction.
If every assignment of black and white coloration to a set of ravens has equal probability, then until
they have observed \textit{all} the ravens, the Bayesian's credence that all the ravens are black will
be no more than $1/2$.\footnote{Specifically, if there are $N$ ravens in total, and $n$ have been observed 
to be black, there are $2^{N-n}$ sequences of raven coloration compatible with the observation, only
one of which has all the ravens black, the probability that they are all black will be $1/2^{N-n}$, which
is less than $1/2$ if $n<N$.} Now, perhaps we should not consider every sequence of ravens on par with 
every other one. But now the question of which sequences are on par with which does not seem to have any
canonical answer. As we discussed in ??backref, Carnap thought that lumping together sequences 
according to the \textit{frequency} of blackness was the way to go. But if not all sequences are on par,
why should we think all \textit{sequences} are on par? Intuitively having exactly one out of $N$ being black 
is equivalent to having exactly one out of $N$ being white, but why should either of these be equivalent 
to having exactly half of them be black, or all of them be white, say? But if we have no good way to 
divide the raven color assignments into equivalent events, indifferentism does not help us.

\subsection{Algorithmic priors}
Suppose we have a language $L$ of finite sequences of symbols chosen from some finite alphabet of basic symbols, 
with some of the sequences representing a member of some set $S$ of situations. 

For instance, $S$ could be arrangements of chess pieces on a board\footnote{The arrangements contain less information
than chess positions, since a chess position includes other information, such as whether a given king or rook has already
moved, whose turn it is, as well as historical information needed for adjudicating draws.}, and $L$ could be a declarative 
first-order language with no quantifiers, twelve piece predicates (specifying both the color and piece type) and sixty-four names of squares.
We could then say that a symbol sequence $\phi$ represents an arrangement $s$ provided that $\phi$ is a 
syntactically valid sentence that is true of $s$ and of no other arrangement. However, in general $L$ need not be a declarative
language. It could, for instance, be an imperative computer language for an abstract Turing machine or a physical computer,
and the situations could be possible outputs of that machine. Then we might say that a symbol sequence $\phi$ represents
a situation $s$ just in case $\phi$ is a program that, when run, halts with the output being $s$. Or if we like we might 
add an additional layer of representation between the outputs of the machine and the situations---for instance, the outputs
of an abstract Turing machine might represent the physical arrangement of particles in a universe, with the chess 
piece arrangements supervening on these.  
We can even chain languages. For instance, we could have a hardware  computer language $L_1$, with the outputs being sequences of
symbols in some declarative language $L_2$, whose sentences in turn represent members of a set $S$ of situations.

Next, consider a natural way of choosing at random a finite sequence of symbols of $L$. Here is one. Add to $L$'s finite alphabet 
a new ``end'' symbol. Then randomly and independently, with each symbol being equally likely (i.e., having probability
$1/(n+1)$, where $n$ is the number of non-end symbols), choose a sequence of
symbols until you hit the end symbol. The sequence preceding the ``end'' symbol will then count as the randomly
selected sequence in $L$. Every sequence of length $k$ has probability $1/(n+1)^k$, so the probabilities decrease 
exponentially with the length of the sequence. We repeat the random selection process until we get a sequence that is both syntactically correct
and represents a situation in $S$.\footnote{If at least one finite sequence is syntactically correct and represents a 
situation in $S$, then with probability one, we will eventually get to a sequence that syntactically correctly represents some 
sequence.}  We now stipulate that the prior probability of a situation $s$ is equal to the probability that the
above process will generate a sequence $\phi$ that represents $s$.

We can be a little bit more precise. If $\phi$ is a sequence of symbols of $L$ and $s$ is a situation in $S$, write
$R(\phi,s)$ if $\phi$ is syntactically correct and represents $s$, and let $R(\phi)$ be shorthand for the claim that $\phi$ syntactically
correctly represents something in $S$, i.e., $\exists x(x\in S\And R(\phi,x))$. Then if $A$ is a randomly chosen sequence of symbols of $L$,
we can define the prior probability $Q(s)$ of $s$ as the conditional probability that $A$ syntactically correctly represents $s$ on the supposition
it syntactically correctly represents something, i.e.,
$$
    Q(s) = P(R(A,s)\mid R(A)) = \frac{P(R(A,s))}{P(R(A))}.
$$    

We can call these priors $L$-Solomonoff priors.
Such priors favor situations that can be more briefly represented in $L$ over ones whose representations are long.
The effect of this choice of priors depends heavily on the choice of language $L$ and how well it can compress some situations over
others. For instance, in our chess case, if we have no quantifiers, it is easy to see that any two piece arrangements with 
the same number of pieces will have equal prior probability, because each square's contents have to be separately specified. 
Thus, if we know that every square contains a pawn, and we have observed the first 63 of these pawns and found them all to be black, the 
probability that the 64th square will be black is still $1/2$. On the other hand, if quantifiers and identity are allowed into our
language, then the all-black-pawn situation can be briefly represented by
\ditem{all-black}{$\forall x\, \operatorname{BlackPawn}(x)$}
(where the domain is
squares on the board), while the situation where squares $1,...,63$ have black pawns and square 64 has a white pawn is harder
to represent. We might, for instance, use a sentence like:
\ditem{63-black}{$\forall x(\Not (x=64)\rightarrow \operatorname{BlackPawn}(x))\And \operatorname{WhitePawn}(63)$.}

Since the probability of generating a given sequence of symbols decreases exponentially with the number of symbols, \dref{63-black}
is much less likely to be randomly generated than \dref{all-black}, and it is intuitively very likely (though proving this rigorously would
be quite difficult???refs) that in general the probability of generating a sentence representing 64 black pawns is higher than that of 
generating a sentence representing 63 black pawns followed by one white pawn. We can thus expect that the conditional probability of
the 64th pawn being black on the first 63 being black to be very high. (Maybe even too high? It is very difficult to get good
estimates here, because there are many ways that a single situation can be represented. ??refs)

Just as in the case of using linguistic complexity to quantify projectibility, the choice of language here provides many Mersenne
questions. If we opt for the algorithmic version of the theory, we need to choose some computer language for a real or abstract
computer, and then we need to choose a representation map between outputs and situations in the external world, with infinitely
many possible candidates. And on the more
descriptive versions, we still need to choose a language, with many decision points as to syntax and vocabulary. It is very unlikely 
that there is a privileged language. And not
every language will yield priors that fit with our intuitions about induction. For instance, we can easily create a language, whether algorithmic or descriptive,
where 63 squares of black pawns followed by one square with a white pawn are much more briefly describable than 64 squares with
black pawns. For instance, on the descriptive side, we might use a $\operatorname{BlitePawn}(x)$ predicate, where something is a blite pawn 
provided it is a black pawn and on one of the first 63 squares or a white pawn and on the 64th, and an analogous $\operatorname{WhackPawn}(x)$, and then we can say $\forall x\, \operatorname{BlitePawn}(x)$.

In the unlikely case that there is a single language $L$ such that $L$-Solomonoff priors are rationally required for us,
we will have a vast number of Mersenne questions about the various parameters of the language and its representation relation.
In the more plausible case that there is a set of admissible languages such that we are required to have $L$-Solomonoff priors for some admissible language, we will have a vast number of Mersenne questions about the parameters that control the range of admissible languages. All of this
gives rise to a significant degree of appearance of contingency.

Consider, too, the following observation. It is implausible that the languages defining rational priors for us
should be ones that are completely beyond our ken. But, on the other hand, it is plausible that there are possible
languages that to the smartest human are as incomprehensible as one of the less intuitive computer languages like
Haskell or Verilog or one of the creations of logicians further from natural language like lambda-calculus is to a typical
six-year-old. Imagine now beings to whom such these languages beyond human ken are easy. It \textit{could} be the case that
the norms for rational priors for them are formulated in terms of $L$-Solomonoff priors for one of the ``baby languages''
that humans can understand, but this does not seem a particularly plausible thesis. It seems more likely that for those
beings, the algorithmic rational priors would be different than for us. 

The standard way??ref to defend algorithmic measures of complexity from the problems presented by a plurality of languages
is to observe that sufficiently sophisticated languages have translational resources. Thus, one can write a Haskell interpreter
in Javascript, and so anything that can be expressed in Haskell can be expressed in Javascript by including the code for a Haskell interpreter, 
and then using a string constant that contains the Haskell code. The result is that the difference in the length of code needed to
generate a given output in different computer language will typically be just an additive constant: if one can
produce the output with Haskell code in $n$ bytes, then one can produce it in approximately\footnote{The approximation is due
to complications due to having to embed code in a string constant, which may involve various escape characters.} $n+k_{J,H}$ 
bytes in Javascript, where $k_{J,H}$ is the length of the Haskell interpreter in Javascript. For large enough $n$, the additive constant will be
unimportant. If we are to measure the complexity of a one-hour broadcast-quality video by the length of code needed to compress 
the  video, the addition of a constant like $k_{J,H}$ will likely be negligible: a Haskell interpreter is about half a megabyte, while 
an hour of video compressed losslessly can be reasonably expected to be in the gigabytes.

However, two points need to be made in our epistemological context. First, even if two different languages give very similar sets of priors, 
if they give even slightly different priors, we either have the Mersenne question of what makes one of these sets of priors be the 
objectively correct one, or we have the Mersenne question about the boundaries of the range of permissible languages. Second,
unlike in the case where we are measuring the complexity of a large set of data, such as a video file, in the inductive cases we 
need to look at ways of expressing relatively simple statements, such as ``All electrons are negatively charged'' or 
Schr\"odinger's equation or ``The first 63 squares have a black pawn and the last square has a white pawn.'' But for such statements, 
a translation manual will dwarf the length of the translated text. To say ``All electrons are negatively charged'' in French by first describing how English works, and then saying that this
description should be applied to the English sentence, will produce a French sentence that is many orders of magnitude longer
than the English one, and hence not a sentence that is relevant to measuring the prior probability that all electrons are negatively
charged.

Finally, while there is something elegant and natural about randomly choosing items in $L$ by randomly choosing within the
set of symbols with an end marker added, there are other ways to proceed. For instance, instead of making the end marker
equally likely as each of the ordinary symbols, one could at each step of generation flip a fair coin. On heads, one is done
generating. On tails, one then uniformly randomly chooses one of the $n$ symbols.\footnote{This will actually not make a 
difference in those languages where the syntax already determines where a syntactically valid sequence ends. This will be the
case with some Polish notation languages, where a valid sequence ends when the main operator is filled out with arguments.}
Or one might first randomly choose a positive integer specifying the length of the sequence of symbols according to some
probability distribution on the positive integers, and then make all the sequences of that specified length be equally likely. 
Or one might randomly choose a positive integer, and then choose the $n$th sequence of symbols in some ordering (e.g., alphabetical).
While the initial symbol-by-symbol method with an end-symbol may seem more elegant, it is hard to say that it is rationally
privileged to the point that the priors generated with it are rationally required. But if it's not thus privileged, then the
range of random choice methods will provide more Mersenne questions. For not every random choice method yields priors that
are plausible candidates for rational permissibility. There will be random choice methods where the sentence ``Birds are a government-run 
drones'' has a probability $1-(1/10^{100})$ of being generated (with all the other sentences having probabilities adding
up to $1/10^{100}$), and so a boundary would need to be
posited between the admissible and inadmissible random choice methods.\footnote{$^*$One might think that there is a 
privileged way to randomly generate sequences: one just makes each sequence has equal probability, regardless of the
sentence's length. Since there are countably infinitely many finite sequences, however, this option leads to a countably infinite fair
lottery which is known to have many paradoxes.??ref A similar difficulty arises if we first choose a positive integer
as the length, with all positive integers being equally likely, and then make all sequences of that length equally likely.}

\subsection{Basic probabilities, norms and explanationism}
Recently, Climenhaga??ref:\url{https://link.springer.com/article/10.1007/s11098-019-01367-0?utm_source=toc} introduced the 
very helpful notion 
of \textit{basic} epistemic probabilities, which determine other epistemic probabilities. For instance, the 
algorithmic approach tends to take unconditional probabilities as the basic ones, and then conditional ones are 
defined by the rule $P(A\mid B)=P(A\And B)/P(B)$. Or on an indifferentist approach to a die roll, the basic 
probabilities are the equal probability $1/6$ of each of the six sides, and the other probabilities are determined
by these and by logical relations. Thus, the probability of the die showing an even number will be $P(2)+P(4)+P(6)=3/6$,
because the three options are logically mutually exclusive. 

An Aristotelian meta-epistemology can accommodate a picture of the grounding of epistemic probabilities that 
takes some of these probabilities to be more fundamental than others. For it is open to the Aristotelian to have
a distinction between basic and derived norms. Thus, the prohibition of torturing blue-eyed persons is 
derived from the prohibition of torturing persons. The latter norm might be basic, or might derive from 
more basic ones. The question of the grounding structure of norms is a substantive question worthy of 
investigation, and indeed comprises a significant amount of the work in normative ethics. 

It is worth noting that the grounding structure of basic norms is not exhausted by norms of the form:
``Assign $x$ as the epistemic probability of $A$ (or of $A$ given $B$)'', nor does Climenhaga claim it is.
There surely are basic norms like: ``Make your epistemic probabilities satisfy the axioms of probability.''
But there could also be substantive constraining norms, like: ``Make your epistemic probabilities for $A$-type 
events independent of your epistemic probabilities for $B$-type events.'' Furthermore, it is quite possible
that some of the basic norms will be range-based, such as ``Assign $A$ an epistemic probability between $x$ 
and $y$'', or qualitatively or quantitatively comparative: ``Assign $A$ a higher epistemic probability than
$B$'' or ``Assign $A$ half the epistemic probability of $B$.''

Bracketing some important technical detail, Climenhaga's own approach is that the basic probabilities are conditional 
ones like $P(A\mid H)$, where $H$ is a hypothesis putatively immediately explanatory of $A$, and the basic 
probability values are determined by the explanatory relation between $H$ and $A$. We could, for instance, 
suppose that $H$ is a thorough description of the chemistry and physics of a match being struck and $A$ 
is the event of the match igniting. The probability value then would be determined by the chemical and physical facts 
in $H$.

There is reason to be pessimistic about this as a complete solution, however. One set of difficulties comes
from the thought that such a system of basic probabilities might work for an ideal agent, but humans are not
ideal. The explanatory hypotheses we formulate, especially in the pre-scientific era or in ordinary life, are not formulated
with the kind of precision that allows for precise probabilities to be read off. If Alice feels grave resentment against
Bob, that could explain her insulting Bob, but the hypothesis of grave resentment does not yield a specific 
probability. We might think that the hypothesis of grave resentment is a disjunction of a large
number of more scientifically precise hypotheses, ones that most people would not even be able to formulate. 
But if our basic probabilities only involve conditioning on these hypotheses, we will need norms for what probabilities 
a real human should assign in light of a very vague sense of what the range of the more precise hypotheses is,
what their probabilities are, and so on. We will need norms, coming along with many seemingly arbitrary parameters
and consequent Mersenne questions, on how a real human being needs to approximate the ideal agent.

A second set of difficulties comes from the difficulty of applying the explanationist approach to explanatorily 
fundamental hypotheses, such as that God exists, or that the beginning of the world is entirely a natural event 
satisfying some precise unexplained physical theory. Here it seems we either need one fundamental hypothesis to be 
\textit{a priori} certain or we need basic unconditional probabilities for the various fundamental hypotheses. 
The option of basic unconditional probabilities raises Mersenne questions. As for the possibility of 
an \textit{a priori} certain fundamental explanatory theory, there is perhaps only one candidate for such a 
theory in the philosophical literature: theism, either as supported by classical deductive arguments from
premises that are themselves certain (such as one's own existence, and various metaphysical principles), or 
supported as a basic belief required of all human beings by their nature.??cf.Plantinga 

Such an \textit{a 
priori} theism is worth considering, but it raises its own difficulties for the explanationist project. For 
while the hypothesis that God exists would yield immediate explanations for various claims about the world,
it does not seem theologically plausible to think that God is bound by precise probabilities. Thus even with 
an \textit{a priori} theism, we would have Mersenne questions about priors for conditional probabilities about
what God would do if God existed.

\subsection{Non-Bayesian update}
We have thus seen that our Aristotelian metaepistemology provides a grounding for priors that seems superior to main
Bayesian alternatives on offer. But there is at least one more area where Aristotelianism does better. 
Bayesianism is committed to updating by conditionalization being the only permissible way to update credences. 
If my current credence in a hypothesis $H$ is $P(H)$, and I receive evidence $E$, my credence should move 
to $P(H\mid E)$, which is equal to $P(H\And E)/P(E)$ if $P(E)>0$. No other changes of credence are permitted. There is an elegance and non-arbitrariness to this,
of course modulo the above-discussed issues about the choice of the priors $P(H)$ and $P(H\mid E)$.

But as is the case for many other formally simple philosophical theories, this is too
simple.??refs Consider several cases. 

\nitem{Pill}{A trustworthy oracle offers you a pill which will shift your credences closer to truth and does not introduce 
any incoherence.}

\nitem{Mistake}{An hour ago, I made an arithmetical evidence when updating on evidence. I moved from credence $0.4$ in $H$ to
credence $0.6$, even though in fact $P(H\And E)/P(E)$ equalled $0.7$. I have just discovered my mistake. It would be good
for me to go back and correct my credences, essentially rewinding my epistemic life from the mistake and re-conditionalizing on all
the subsequent evidence.}

\nitem{Prejudice}{My original priors had extremely high probabilities for some conspiracy theory involving members of a certain 
minority group, so high that despite the fact that all my life I have been receiving strong evidence against the theory, nonetheless I 
am still quite convinced of the theory. I reflect on my original priors, and note that my priors of the conspiracy theory in question were 
quite out of proportion to my priors for similar theories involving other minority groups. I conclude that my original priors were
stupid and racist. I go back and change them to treat the various groups more equally, and then as best I can I fix up my posteriors
to match the evidence I recall having had.}

In all three cases, my update of credences seems quite rational, but is not a case of conditionalization. We should thus
suppose that while it may be a good general rule that we should update by conditionalization, we should at times depart from it.
But the question of how we should depart from conditionalization now introduces complexity in the theory that raises significant
Mersenne questions. We need more than just simple exceptions for each case: there will be parameters to set.

The case of \textsc{Pill} as given is straightforward. We can say that you should depart from Bayesian update when doing so would move
some of your credences closer to truth and none further away from it, where a credence's ``distance from truth'' is the 
distance of the credence from $0$ or $1$ depending respectively on whether the proposition the credence is in is false or
true. However, there are variants of \textsc{Pill}. Suppose that the pill has a $0.9$ chance of moving you significantly
closer to truth and a $0.1$ chance of moving you slightly away from truth. It seems like it could be worth it. Or suppose
that the pill moves your credences in important propositions closer to truth at the expense of moving your credences in some
unimportant propositions further from the truth.

This suggests that we will need to quantify epistemic value, much as we did in Section??backref, and then say
something like this: You are epistemically required (respectively, permitted) to opt for a pragmatically costless change of 
credence when doing so increases (does not decrease) expected epistemic value. If we quantify epistemic value in terms of
strictly proper scoring rules, a delightful result of this move is that we do not need to handle the ordinary case of 
Bayesian update any differently. For it can be proved that in the kinds of cases where one simply receives
evidence and sets one's credences according to it, conditionalization is the unique best policy for maximizing expected
epistemic value.??ref:(Also Isaacs and Russell??) 

However, as we also saw in Section??backref, the choice of a scoring rule or measure of epistemic value involves infinitely
many free parameters, and at the same time not every scoring rule that satisfies the formal constraints is rationally
plausible. In our present context of evaluating non-conditionalizing updates the point is particularly clear. For when
evaluating credence-changing pills, we need to take into account the \textit{importance} of the affected credences, and
that is not a formal criterion. 

Note that \textsc{Pill} is not as outlandish as it initially seems. We do in fact modify people's thinking with
psychiatric medication as well as with psychotherapy.

Next, consider \textsc{Mistake}. We don't in fact remember all our evidence:
we update our credences on the more important things and forget the less important, which often includes some or
all of the evidence. I know that Beijing is the capital of China, but I don't know where I learned it from. I know that 
there was a cat in the yard earlier this morning, but much of the rich sensory evidence is now gone from my mind. 
An attempt to go back and correct a past update error is bound to be a messy affair. Sometimes when the mistake is minor
it might be better to let it go rather than miss out on some of the evidence gathered since. And the situation is often
too messy and too complex for any sort of an expected epistemic utility calculation. 

Yet the fact that a situation is messy does not get us off the hook. There will still
be a distinction between right and wrong ways to proceed, even if the rule cannot be formalized. We have here something very
similar to the kinds of messy everyday ethics cases that are grist for the situationist's mill.??backref 
It is unlikely that the answer is given by any elegant principle without the kinds of parameters that raise Mersenne
questions, but at the same time there \textit{is} an answer, and we expect there to be something to ground it.

And what goes for the messiness in correcting a calculational mistake applies even more to the messiness involved in
the kind of intellectual conversion that occurs when one realizes that all of one's thinking for years has been based
on irrational prejudice, and one attempts to dig oneself out of the resulting heap of epistemic defects. Again, there
are right and wrong ways to proceed, but the likelihood of a clear and elegant principle that solves the problem is
nearly nil. 

\section{Intellectual limitations}
Limitations in recognizing logical implications and contradictions are a case where rational norms are plausibly
species-relative. It is epistemically irrational for one to conclude that it's raining from the fact that it's neither 
raining nor snowing. But it is not irrational for one to take $22828 \times 2219 = 50645332$ (which is in fact
false) to follow from the axioms of arithmetic due to an arithmetical slip. Of course, someone with 
exceptional calculational skills may immediately that see the latter is mistaken, but the ordinary person's failure to see it is 
not an instance of irrationality. 

Ethics also contains intellectual limitation cases. Analogically to the multiplication
case, we would not consider a person to be less than virtuous because they are unable to see an extremely complex medical 
ethics case rightly. But if someone doesn't realize that it's wrong to ambush strangers in order to sell their organs,
there seems to be something morally wrong.

But the ethical cases may also be different. Consider an adult who strives to follow their conscience as best they can,
but nonetheless fails to see that it's wrong to kill strangers for their organs. This person presumably has
a severe intellectual and/or emotional disability. We might judge them to be a good person, or at least not a vicious one. 
However, the logical case seems different. The person who, due to intellectual disability, concludes that it's raining 
from the claim that it's neither raining nor snowing \textit{is} failing at rationality, though of course they are 
inculpable. One can defensibly define moral worth in terms of the seriousness of their attempt to do what seems right, but
it is difficult to define someone's degree of rationality in terms of the seriousness of their attempt to think as seems
right. For it is paradigmatic of irrational people that they take themselves to be acting quite rationally---if one
derives that it's raining from its neither raining nor snowing, this is precisely because the conclusion seems to follow.
It is, indeed, unclear whether it is even possible to conclude $p$ from $q$ without its seeming that $p$ follows from $q$.
But it is paradigmatic of immoral people that they lack integrity and violate their own conscience. 

The line to be drawn in the epistemic rationality cases (and in the ethical ones if they turn out to be similar), is a line that we 
are unlikely to be able to draw in a species-independent way. It seems plausible that for beings that as a species are 
more adept at logic than we are, a failure to see the truth of Fermat's Last Theorem as following from the axioms of
arithmetic is a failure of rationality, but it is not so for us. 

The difficulty of handling limitations is particularly glaring in the case of Bayesianism. Typically 
we can't do ``proper'' Bayesian update. We don't have the time, don't have the mathematical skills, or perhaps most importantly aren't 
able to quantify our priors in such a way as to make them amenable to precise mathematics. We need an epistemology that 
works in this all too human predicament, a \textit{human} epistemology, and we need an account of what grounds that 
epistemology. 

One may insist that under these imperfect conditions, we should simply say that we are stuck with irrationality.
But nonetheless there are cases where it is clear that something should be done under the unhappy circumstances.
Suppose that the completely right credence on my priors in some proposition $p$ is $0.9874$, 
but I can only calculate to two significant figures. Then I should take the credence to be $0.99$, not $0.98$ or 
$0.01$. But things will be less clear if I need to form credences in both $p$ and $q$, which a perfect Bayesian
with my priors and evidence would take to be $0.9874$ and $0.0332$, but due to limitations of
time or energy level, I can only get a total of four significant figures. Are the right credences for me $0.99$
and $0.03$, or $0.987$ and $0.0$, or $1$ and $0.033$? It depends, surely, on the relative epistemic importance of $p$ and
$q$. And once importance needs to be taken into account, it is very unlikely that we will have a precise and elegant
account with no free parameters. Instead, we have human messiness.

Recently, Jeffrey??ref has argued that ethical norms may be relative to a stage in life. Whether this is so, it is very
plausible that some epistemic norms are so. There may be logical facts failure to notice which constitutes a failure of 
an adult's rationality, but would not constitute a failure of a child's rationality. If this is right, then we have all
the more reason to think the norms of epistemic rationality to be species-relative, since surely what the stages in 
life are, and when they occur, is something that is species-relative.

\section{Social epistemology}
\subsection{Rational social animals}
We are rational social animals. Our rationality is exhibited in a \textit{social} way, and our sociality is
exhibited in a way proper to the kind of animal---the human animal---that we are. On the Aristotelian account,
all the norms tied to these observations are grounded in our human nature. In this section, we will consider a
number of epistemologically relevant aspects to our sociality and show how they point towards our robust view
of human nature.

\subsection{Innate beliefs and testimony}
Humans are said to have much less in the way of instinct than other animals. Furthermore, we do not seem to
have any innate beliefs. But we can imagine a species of intelligent animals which do more by instinct than we do,
and which have evolved to be born with some unshakeable and true beliefs, such as that purple winged things are to 
be avoided and electrically charged spiky fruit is good to eat. For these beings, there is nothing irrational about 
having such beliefs without any evidence. For us, there is. In a Bayesian mode, we might say that for these
beings very high priors in these empirical claims are appropriate, but not so for us. 

Experiments in machine learning suggest that for certain kinds of problems it is useful for a system to be pre-trained
and hence have priors that embody some substantive information. Pre-training presumably comes with a sacrifice of flexibility.
Thus whether pre-training is useful depends on the subject area and the kinds of situations the system will face. And
of course the content of any pre-training is highly contingent. It would be surprising if our evolutionary process did not 
involve any pre-training. Our discussion of skepticism in ??backref implies that it is normal for us to have certain substantive 
priors, and that not to have them would be abnormal. Some empirical evidence for this is given by infants' neural responses to
snake pictures.\footnote{??ref:https://www.nature.com/articles/s41598-020-63619-y} But what those priors should be is surely highly 
species-dependent: we would not expect alien babies evolved for an environment without any snakelike objects to naturally
react to snakes. And even if humans, surprisingly, turn out not to have any substantial information encoded in their priors (if that is even logically 
possible), that fact is surely species-dependent. In such a case, for us to have substantively informative priors would be
abnormal, but for intelligent cats, say, it might be quite normal. 

We can also imagine a species where memories are genetically inherited (this would be a species for which
Lamarckism is true). A member of this species could, then, be born with a 
large number of beliefs that they had no evidence for. For, like we often do, the members of this species could forget the
evidence that led to a conclusion. But while in our case, we once had the evidence, in this species it was some ancestor
of theirs that had the evidence. Typically (apart from substantive priors, say) it is irrational for humans to have beliefs without ever having had evidence for them, but not typically irrational for us to have beliefs whose evidence
we have forgotten---say, facts learned in childhood. For members of the species with genetically inherited beliefs, on the other hand, there typically need be no problem with beliefs that the individual never had evidence for, as long as an ancestor had the evidence and forgot it. These inherited beliefs would be akin to innate beliefs or substantive priors, but unlike innate beliefs that would be normatively the same for all members of a species, they would differ from individual to individual
depending on what the parents believed. 

There is partial post-conception gene transfer in humans in a phenomenon known as fetal microchimerism where a few
cells with the fetus's genotype are found in women's healthy tissue after pregnancy??ref:https://pmc.ncbi.nlm.nih.gov/articles/PMC4989712/ as well as in microchimerism in twins??ref:https://pmc.ncbi.nlm.nih.gov/articles/PMC3902707/ . 
While we do not at present know of any benefit to the organism from such mechanisms\footnote{On the contrary,
fetal microchimerism may have unfortunate autoimmune consequences.??ref to first paper}, we can imagine cases where
there are benefits, say by one organism giving cells with a new genotype to an organism whose genotype is either
defective (as we do artificially in somatic cell therapy, say for colorblindness??ref:colorblindness) or less well
adapted for some new situation. Thus we could imagine organisms which non-accidentally engage in some post-conception gene transfer. If we now combine
this with the thought experiment of organisms that have inherited beliefs, say by supposing the transfer of neural
cells, we will have organisms that have voluntarily or involuntarily genetically transfered beliefs. And such 
transfer, whether voluntary or involuntary, could lead to rational beliefs, if it were to fit with the 
species norms.

Now, let us return to our species. Perhaps we should not think the species where memories are inherited to be all that
different from us. For our parents bequeath much knowledge to us. It is true that this is mediated by soundwaves or gestures rather than by gametes, but does that make a significant philosophical difference? We should, thus, take seriously the idea
that just as for members of a species where memories are inherited it is fundamentally rational to believe these inherited
memories, and to question them without special reason is irrational, for us it may be fundamentally rational to believe
parental testimony, and to question it without special reason is irrational. And transfer through ordinary 
testimony in adulthood can be thought to be similar to hypothetical voluntary or involuntary genetic transfer of
memory at later stages in life. Such hypothetical transfer would be a non-Bayesian mode of update, and so by analogy we have some reason to think
that testimony does not reduce to Bayesian update. And just as the modes of inheritance and transfer
of belief would be species-relative, it is likely that the specific norms of when and how it is rational to accept inherited 
or testified-to beliefs are species-relative. 

\subsection{Self vs.\ others}
Suppose it turns out that I have a near-identical twin on a planet just like Earth, all of whose mental life has been exactly 
the same as mine, with one exception: the first bird I saw today was a bluejay, while the first bird he saw today was a robin. 
Neither of us is an avid birder, and so this has no significant effect on me. We both learn that next time we fall asleep, we 
will be transported to a third planet, and my twin's memories will be changed so that his robin memory will be replaced by my
bluejay memory, with corresponding tweaks for anything else affected by the memory. Furthermore, I am given a choice of epistemic
policies: I can ensure that tomorrow both I and my twin will firmly believe that the bluejay memory is correct, and hence will 
believe ourselves not to have had our memories changed, or I can ensure that we will both assign credence $1/2$ to the bluejay 
memory's being correct. 

Insofar as my epistemic concern is solely for myself, it makes sense to take the policy of sticking to the bluejay memory.
But doing so harms my twin epistemically. What about the total epistemic value? As before, let $T(r)$ and $F(r)$ be the accuracy 
scores or epistemic utilities for assigning credence $r$ to a proposition that is true and false respectively. Then on the policy 
of sticking to the memory, the total epistemic value is $T(1)+F(1)$: I will have credence one in a truth and my twin in a falsehood.
On the policy of assigning credence $1/2$, the total epistemic value is $T(1/2)+F(1/2)$. If the pair $T$ and $F$ is strictly proper,
then $(1/2)T(1/2)+(1/2)F(1/2) > (1/2)T(1)+(1/2)F(1)$, and so $T(1/2)+F(1/2)>T(1)+F(1)$. Thus total epistemic utility is higher
when the policy is of total uncertainty. However, the total epistemic utility \textit{for me} is higher when the policy is of
certainty: obviously, $T(1)>F(1)$. 

When evaluating epistemic policies, then, we have two different points of view: a self-concerned and an other-concerned 
point of view. And we are led to different policies by the different points of view.

The above is a strange case. But there are everyday cases to be considered. It might well turn out that as a community we 
get to the truth more effectively if investigators have a stronger commitment to their theories than the evidence warrants,
because then they will be more motivated to search for evidence and will have ``skin in the game''. However, such a commitment
can be expected to be epistemically harmful to the individuals. Thus the epistemic goods of the community may conflict with 
individual epistemic goods. 

For completely asocial rational agents, we would expect norms of epistemic rationality favoring individual epistemic goods. But for 
social rational agents, we would expect norms of epistemic rationality favoring social epistemic goods. But sociality comes in degrees.
We would expect that in more social kinds of rational animals, there would be a stronger favoring of social epistemic goods, and in
less social ones, a weaker such favoring. We are neither asocial nor maximally social (we are not a hive mind!). For us, we would expect
some kind of an intermediate between favoring individual epistemic goods and favoring social epistemic goods. But that intermediate 
normative position will raise Mersenne questions as to why it lies where it does. For it is not plausible that there is some 
metaphysically necessary rule that takes the degree of sociality of a rational agent and spits out the degree to which social epistemic 
goods should be favored. It is much more plausible to suppose that the degree and manner of preference for social epistemic goods is 
a part of the kind norms. 

Nor is it likely to be a single numerical degree of preference per species. Surely the appropriate degree of preference for social epistemic 
goods depends not just on the species, but also on one's role in society. So we need a ground not just for single human numerical degree of
preference for social epistemic goods, but a function from social role, and probably area of knowledge, to degrees of preference for
social epistemic goods.

\subsection{Paternalism}
If there is such a thing as epistemic value, the most obviously right thing to do with regard to its possession
by others is to maximize the expectation of it. Suppose now that you and I have differing prior opinions about the efficacy
of a drug: you have a credence of $3/4$ that it is efficacious, while I have a credence of $1/2$ that it is efficacious.
I perform some experiments and find evidence of the drug's efficacy that raises my credence from $1/2$ to $3/4$.
Suppose I now consider whether to reveal the evidence to you. I know that if I reveal $E$, your own calculations
(e.g., Bayesian update) will make your credence go from $3/4$ to $7/8$. But I think the best credence all things considered is $3/4$.
So I withhold the evidence, thereby keeping your credence at the best credence. 

The above argument seems pretty dubious. But we can make it rigorous. Suppose you have a strictly proper 
scoring rule $(T,F)$. Then
my expectation of your epistemic utility after revealing the evidence will be:
$$
	U_R = (3/4)T(7/8)+(1/4)F(7/8)
$$
since by my lights, there is a $3/4$ probability that the drug is efficacious, and your epistemic utility if I
conceal the evidence will be
$$
	U_C = (3/4)T(3/4)+(1/4)F(3/4).
$$
It follows immediately from the definition of a strictly proper scoring rule that $U_R < U_C$, and so if I am trying
to maximize your expected score, I should conceal the evidence.

This seems to be unacceptable paternalism. But suppose that I am nearly certain of a quack remedy's
efficacy due to my acceptance of a conspiracy theory to suppress evidence about it (think of various
shenanigans during the pandemic here), and you are a scientist who just found a slight bit of evidence 
in favor of the drug's efficacy. If we're having a conversation and I mention my conspiracy theory, surely
you do not have on balance epistemic reason to tell me about your evidence and thereby confirm my conspiracy
theory. There is thus \textit{some} acceptable minor paternalism---nobody thinks we should tell everyone everything
we know! And, of course, some paternalism, whether epistemic or moral, is acceptable with respect to one's children.

Where the line is to be drawn between acceptable and unacceptable epistemic paternalism seems to be very
much a Mersenne question. It is quite implausible to think that there is a clearly privileged place to draw
the line. cf.??ref:https://philpapers.org/archive/JACWEA.pdf

More generally, the above line of thought shows that our behavior with respect to other people's epistemic
value follows subtle and complex rules. We shouldn't simply always maximize their expected epistemic value, as that
doesn't show respect for their autonomy. But obviously some attention needs to be paid to the epistemic value of
others.
Whatever the right rule here is, it likely has multiple parameters and an appearance of contingency. 
Moreover, the observation that epistemic paternalism can be more acceptable with respect to one's children points
to a species-relativity in the parameters, since it is likely that how one should relate to children is species
relative (think of a species that has very little parent-child relationship after birth).

\subsection{Being known}
Suppose a scientist discovers a fact $p$. She then communicates it to twenty more people at a conference,
and eventually publishes it in a journal where a hundred people read it. And then a Youtuber picks it up and
a million people hear of it. The most obvious way to think of the social epistemic value here is that there
is a value $V$ that comes into the world when the scientist discovers $p$. After the conference, the value
$V$ turns into $21V$, and after the journal readers get to it we have a value of $121V$. Finally, the Youtuber
multiplies this value by about ten thousand, to yield $1000121V$. 

This seems the wrong story about the value of what one might call ``human knowledge''. It doesn't seem
right to say that the Youtuber contributed so much more to human knowledge than the scientist. 

An additive account of the value of human knowledge yields an epistemic variant of Parfit's repugnant conclusion
for utilitarianism??ref. On an additive account, letting a sufficiently large number of people know some trivial
fact, such as the length of Charles III's left big toe, will swamp the value a major scientific discovery that only a few experts can understand, even if a single person's learning of the scientific discovery swamps the value of a single
person learning the length of the royal toe.

In our science story, it seems that the big jump in value occurred when $p$ \textit{became known}, which happened 
either when the scientist discovered it or when she informed others of it. 

What exactly it means for a proposition to \textit{be known} is not immediately clear. 
First note that when we talk of a proposition being known, we are talking of its being \textit{humanly} known. 
It is correct to say that it is not known whether
there is intelligent life in other solar systems, even though if there is, someone knows it---namely the intelligent
beings in question. And if God exists, then someone knows everything.

The obvious account is that a proposition is known provided that some human knows it. But that seems neither necessary nor sufficient. If the abstract of a random mathematics paper from half a century ago says: ``We give necessary and sufficient conditions for $\phi$'', then I have good reason to conclude that the necessary and sufficient conditions for $\phi$ \textit{are known}, even
though there is a pretty good chance that nobody has read the paper for decades and nobody who has read it and
is still alive remembers what the conditions are. Thus, some things that no one knows are nonetheless known. 
On the other hand, if Alice is dying of thirst on a desert island, it is correct to say that no one knows this
even if Alice knows it.

Roughly, we might say that something is known provided that the knowledge is generally accessible to humans 
with appropriate raining. Alice's lonely death is not accessible, but the conditions for $\phi$ are. What exactly counts as ``accessible'' and ``appropriate training'' is rather unclear. Suppose the only copy of the journal with the 
conditions for $\phi$ is on top of Mount Everest. It is not clear that the conditions are known. Quite likely
we should say that the being known in this sense is something that comes in degrees. And then there will be a 
very complicated function from the subject matter and the degree of accessibility to the value of the particular
instance of being known. This complicated function is very likely to have many free parameters and hence will
raise Mersenne questions about its grounds. And yet this value of being humanly known is important, as it informs our resource allocation.


\section{$^*$Imprecision}
Suppose that a spinner is randomly spun, and it comes to a stop at some angle between $0^\circ$ and $360^\circ$, inclusive,
though of course $0^\circ$ and $360^\circ$ label the same outcome. To make the logic of the situation a little simpler, 
let's label that outcome ``$360^\circ$'', and call the spinner angle $X$. Thus $0^\circ < X \le 360^\circ$.
Independently of the spinner, a fair coin is tossed. If the coin is tails, the spinner is left
unchanged. If the coin is heads, the spinner is adjusted so that it now points to half of the angle it started with. Call the 
final angle $Y$. All of the above happens out of your sight.

Note that if the coin lands heads, then we are guaranteed to have the final spinner angle $Y \le 180^\circ$. Thus, if you were
to learn that $Y>180^\circ$, you would thereby have learned that the coin landed tails. It is a standard result in Bayesian
epistemology that if something is evidence for a hypothesis, then its negation is evidence against the hypothesis. Hence, if
you were to learn that $Y \le 180^\circ$, that would be evidence against the tails hypothesis, and hence in favor of the heads
one. More specifically, the probability of heads given that $Y\le 180^\circ$ will be $2/3$. For there are initially four equally
likely options given your evidence: 
\begin{enumerate}
\item[(i)] heads and $X\le 180^\circ$
\item[(ii)] heads and $X> 180^\circ$
\item[(iii)] tails and $X\le 180^\circ$
\item[(iv)] tails and $X> 180^\circ$.
\end{enumerate}
Learning that $Y\le 180^\circ$ rules out (iv), since on tails no adjustment is made and so $X=Y$. However, learning that
$Y\le 180^\circ$ does not rule out any of the ways of getting any of the options (i)--(iii). Thus, (i)--(iii) remain equally
likely. Since heads occurs on two of these three options, thus the probability of heads is $2/3$ after learning that $Y\le 180^\circ$.

Suppose, now, that we have a three-step process. At $t_0$, the spin, toss and any adjustment is done out of your sight, but you
know the process that was followed and will be followed. At $t_1$, you will be informed whether $Y \le 180^\circ$. At $t_2$, you will 
be informed of the exact value of $Y$.

Imagine that at $t_1$ you learn that indeed $Y \le 180^\circ$ and at $t_2$ you learn that $Y$ is exactly $22.44^\circ$. 
At $t_0$, your credence in heads was $1/2$. At $t_1$, it went to $2/3$. What about at $t_2$? Well, here is the problem.
At $t_2$, the total information gained since $t_0$ can be summed up as $Y=22.44^\circ$ (this logically entails the
information from the $t_1$ stage that $Y\le 180^\circ$). But the information that $Y=22.44^\circ$ is equivalent to:
\begin{enumerate} 
\item[(v)] Either tails and $X=22.44^\circ$, or heads and $X=44.88^\circ$.
\end{enumerate}
But note that (v) tells you nothing about whether you got heads or tails, in light of the fact that all the spinner angles are
equally likely, the coin is fair, and the spinner and the coin are independent. Both paired coin-and-spinner outcomes are equally
likely, and when we learn that exactly one of them happened, they remain equally likely.

In light of the fact that your total information at $t_2$ is that given in $t_0$ plus (v), at $t_2$ your
probability for heads seems to be $1/2$. 

But now observe that this argument would work no matter what angle you learned $Y$ to have at $t_2$. Thus at $t_1$, you would
already know for sure that given the additional information you will receive at $t_2$, your final credence in heads will be
$1/2$. 

Famously, Bas van Fraassen has posited the plausible reflection principle that if a rational agent knows for sure that their future credence
will be some value, then that should \textit{already} be their credence. This principle needs some qualifications to avoid
easy counterexamples. The agent needs to know for sure that they are will maintain rationality, and that they will 
not lose any information.??refs We can suppose that all of these qualifications are met in our case, and given these qualifications,
the reflection principle is very plausible. Yet our case appears to be a counterexample to the principle.

It is worth noting the pragmatic effect of this failure: you are subject to a Dutch Book, a sequence of bets with a guaranteed
loss. At $t_1$, you will be willing to accept a wager where you get $\$45$ on heads and pay $\$55$ on tails, since
$(2/3)\cdot \$45 - (1/3)\cdot \$55 = \$11.67$. But at $t_2$, you will be regretting this wager and expecting an outcome of
$(1/2)\cdot \$45 - (1/2)\cdot \$55 = \$(-5)$. Given your expectation at $t_2$ of a five dollar loss, you will be willing to pay the 
bookie four dollars if they offer to let you change your mind. As a result, you have a guaranteed loss of four dollars.  
Nor is this some kind of a rare situation: there is nothing that special about the case where $Y=22.44^\circ$: any angle
less than or equal to $180^circ$ has the same property. Thus half of the time that the whole game is played you will end up
Dutch-Bookable at $t_1$ and $t_2$.

Note that there is a technical problem in the above reasoning. I argued that updating on the information in (v) yields equal probability
of heads and tails. However, the information in (v) has probability \textit{zero}. To see this, note that for any small positive
number $\e$, the probability that $X$, when measured in degrees, is between $22.44-\e$ and $22.44+\e$ is $2\e / 360$. Thus the 
probability that $X$ is \textit{exactly} $22.44^\circ$ is no bigger than $2\e / 360$ for \textit{any} positive real number $\e$.
But the only way a non-negative number $p$ (and probabilities are non-negative) can be less than or equal to $2\e / 360$ for any positive
number $\e$ is if $p$ is zero.\footnote{Suppose that $p\ge 0$ and $p\le 2\e/360$ for 
every $\e>0$. For a \textit{reductio ad absurdum} suppose $p>0$. Then let $\e = p$. This is a positive number, so $p \le 2\e/360 = p/180$.
But we cannot have $p \le p/180$ for a positive $p$.} Standard Bayesian reasoning updates the probability of a hypothesis $H$ on evience $E$
to $P(H\mid E)=P(H\And E)/P(E)$. But if $P(E)=0$, then this formula yields $0/0$, which is undefined. 

The literature contains various ways out of the difficulty of updating our information on an event that seems to have zero probability.
We might say that it's not zero but an infinitesimal, or we might have a mathematical framework like Popper functions that allows
for such conditionalization, or we might work with probability densities. Whatever we do, a criterion of adequacy on the method will be that we 
preserve the symmetries in the situation by taking the two disjuncts in (v) to be probabilistically on par, and hence making the 
update yield equal probabilities for heads and tails.\footnote{There are technical problems with maintaining symmetry in general in contexts with
infinitesimal probabilities. But the amount of symmetry needed for the above argument---namely, equal probability of $22.44^\circ$ and
$11.22^\circ$---can indeed be maintained. ??ref} And that will lead to our paradox.

The solution I want to propose instead is this. Stop the search for general ways of updating on zero probability events. Except in special
cases (more on that soon???), such updates should not be done.  Instead, embrace imprecision. In practice, we never exactly measure the
position of a spinner. Suppose that instead of measuring the spinner position at $22.44^\circ$ at $t_2$, we measure it to be $22.44\pm 0.01$
degrees. Let $E$ be the evidence, then, that the spinner finally ended between $22.43$ and $22.45$. Then $P(E)=(1/2)(0.02/360)+(1/2)(0.04/360)$
(on tails, to get $E$ we need need the spinner to initially land between $22.43$ and $22.45$; on heads, we need it to 
initially land between $44.86$ and $44.90$). Then by Bayes' theorem:
$$
    P(\text{heads} \mid E) = \frac{P(E \mid \text{heads})}{P(E)} P(\text{heads}) = \frac{ (1/2)(0.04/360) }{ (1/2)(0.02/360)+(1/2)(0.04/360) } = 
	frac23.
$$    

Now, if we were to measure things will full precision, we would indeed have to update on zero-probability events, and we would sometimes
end up in a bind. But our rationality, I propose, is not made for full-precision measurements. Our rationality is made for imprecisionand is 
kind-relative. Beings who made full-precision measurements of real numbered quantities would 
have to reason differently from how we do. Maybe they would have some other method than conditionalization for dealing with these cases.
Maybe violating the reflection principle and being Dutch-Booked would be rationally acceptable for them. This all fits very well with 
our Aristotelian picture. We have our nature and the rationality that our nature requires of us.

\section{\textit{A priori} intuitions}
We have a large store of what one might call substantive \textit{a priori} intuitions, such as that nothing can cause itself,
that there is contingency, that nothing is a proper part of one of its proper parts, that a material object couldn't
have been always immaterial, that if it is not necessary that there exist green objects then neither is it necessary 
that there exist blue objects, that every thought has a thinker, that every number has a successor, that the Peano axioms 
of arithmetic are consistent, and so on. These intuitions vary in strength, though all the above examples are very plausible.

An Aristotelian account of epistemological normativity can hold that among the norms of our nature are requirements that we 
have a high credence, maybe in some cases even certainty, in such propositions, and add optimistically that we are so 
constituted that we tend to follow this requirement of our nature. We can then have a basic justification in such claims 
simply in virtue of our nature requiring us to assign them a high credence.

\section{Going beyond the applicability of human epistemology}
Suppose that you are certain you are one of infinitely many people have each rolled a fair die, none of whom have seen the 
result. It is then announced by a perfectly reliable angel that all but finitely many of the dice show six. 
What should be your credence that your die shows six? 

On the one hand, it seems very likely that your die shows six. After all, the vast majority of the dice show
six, and you have no reason to think yourself exceptional. 

On the other hand, prior to the announcement, your credence in six was $1/6$. And the announcement told you nothing 
about your die. For the following two statements are logically equivalent:
\ditem{die-announce-1}{All but finitely many of the dice show six.}
\ditem{die-announce-2}{All but finitely many of other people's dice show six.}
For your die's state makes no difference to whether there are finitely or infinitely many non-sixes.
But then given your certainty that the dice are fair, \dref{die-announce-2} gives no information about 
your die's result. And since \dref{die-announce-1} is logically equivalent, it too gives no information.
Having received no information, you should stick to $1/6$. 

This reasoning seems convincing. Yet if everyone sticks to $1/6$, then all but finitely many people are quite far 
from the truth. Furthermore, if everyone is playing a game where you are asked to guess if you have a six or not,
and you are rewarded for getting it right and penalized for getting it wrong, then if everyone sticks to $1/6$
for having a six, everyone will guess that they don't have six, and all but finitely many people will be penalized,
which is surely not the right result.

In ??ref, I argued that a good solution to epistemological paradoxes like this is to reject the metaphysical possibility
of an event depending causally on infinitely many events, in the way that the angel's accouncement depends on the infinite
number of die rolls. But there is another possible solution: we can deny that our epistemic norms extend to far-fetched
situations, just as in ??ref it was suggested that our ethical norms do not apply to far-fetched situations. Whether this is 
a completely satisfactory resolution to the paradox is not clear. For there is some plausibility in thinking that if 
predicaments like the above were metaphysically possible, there could be rational beings who could reason
in them. But it seems there couldn't be any. However, no matter what we say about the metaphysical possibility of such beings,
there is something right about the thought that \textit{we} are not made to reason about such things.

There are many other examples where epistemology seems to break down in radical cases. What should you think if you came to be
convinced that an evil demon is trying to get you to believe as many falsehoods as possible? Any thought you might have ends
up undercut. Should your credences in everything be at $1/2$, then? But that is incoherent: for then you have credence $1/2$
that the last die you rolled is $1$, and credence $1/2$ that it was $2$, and credence $1/2$ that it was $3$, and credence $1/2$
that you never tossed a die, and so on. Perhaps suspension of judgment is something other than assigning probability $1/2$. But 
if so, should you suspend judgment about everything, including about the norm of fitting your beliefs to you evidence? However,
if you suspend your judgment about that, then why bother with suspending your judgment about other things, given that you don't 
think you need to fit your beliefs to your evidence? Or what should you think if you come to be convinced that you are a computer 
simulated non-player character in a sophisticated video game? What kind of a world should you think you are in? 

Note now that there is such a thing as realizing that when you were using certain words, you had no concept behind them, and
the words were mere meaningless words. For instance, take a word that we as laypeople defer to experts on the meaning of, such
as ``gluon''. But now imagine that physicists have been pulling our collective legs about gluons all this time: it was just a 
made-up word without any meaning behind it. We can imagine discovering that. And words whose meaning we get by deference are not
the only words like that. The phenomenon of a person who doesn't know what they are talking about is not uncommon. My metaphysics
includes accidents. But I could imagine finding myself in a position where I come to be convinced that I never had a concept of
an accident---that ``accident'' is a meaningless word. Now imagine a radical hypothesis: I come to be convinced that I have no
concepts at all. What should I think now? 

There is something plausible about the idea that in certain extreme situations there is no right way for us to think,
but beings other than us could have moral norms that provide additional specifications---ones that may appear 
arbitrary to us, say---as what one should think in some of these extreme situations. 

\section{Epistemic supererogation}
In ??backref, we saw that a Natural Law ethic can easily accommodate the concept of supererogation by allowing for a case
where one flourishes more in willing to $\phi$ than in willing to $\psi$, but nonetheless willing to $\psi$ would not be a case of languishing. This was
a special case of a general phenomenon where one state can constitute greater flourishing than another, even though neither
is a case of languishing. 

Does this phenomenon appear in epistemology as well? ??refs  Intuitively, it does. Often, when Dr.~Watson fails to see the 
connections that Sherlock Holmes sees, we cannot say that Watson is epistemically defect---rather, Holmes is, at times, 
superlatively insightful in weighing the evidence. This distinction, often specific to a particular epistemic area, between
insufficient insight, acceptable insight and superb insight is familiar to us, and an Aristotelian metaepistemology simply
makes it a part of a general phenomenon. The possibility of epistemic supererogation thus provides some support for the 
present account.

If the correct epistemology is Bayesian, however, one might think there is no room for supererogation. Either one updates
correctly on the evidence or one updates incorrectly. There is, no doubt, a gradation among failures of correct update, 
but there is only way to update correctly.  Thus, it seems, on a Bayesian epistemology, there is no room for supererogation,
and an Aristotelian's ability to account for epistemic supererogation is no advantage.

This is not correct. There are at least three areas within a reasonable Bayesianism where there is room for supererogation.
First, if Bayesianism is to be at all a candidate for our epistemology, and assuming some version of an ought-implies-can doctrine,
Bayesianism has to accommodate imprecise probabilities, because we humans are only capable of having  
probabilities that are both precise and rational in very limited situations such as coin flips. Maybe I could make myself
have a credence of \textit{exactly} $0.9997$ that my copy of \textit{Being and Time} is in my office, but that number wouldn't 
be rational. But we are required to have some level of precision. I would be failing in rationality if I vaguely judged
that the probability of \textit{Being and Time} being in my office is ``at least a little more likely than not''. If I
simply judged it to be ``very likely'', I would not be irrational. But it could be that a person more excellent in precision
could have a finer grained probabilities than my ``very likely'', whether partly numerical (``somewhere around $0.9997$'' or
maybe ``between $0.9997$ and $0.9999$'') or just with more refined non-numerical levels. Such a person's judgment could well
count as supererogatorily rational. 

Second, we are simply incapable of updating fully on all of our evidence---there is too much of it. When
I consider the evidence that it's getting to be time when I will go to the gym, I don't update on the fact that there are
smudges on the living room window. Our world is deeply interrelated. There is likely some kind of a correlation. (E.g.,
smudges may indicate something about the weather, which may indicate something about whether I am going to exercise outdoors
or indoors.) But I ignore it. And that's part of being an acceptable human reasoner. Similarly, when Watson ignored the dog's
famous failure to bark??ref in his holistic evaluation of the evidence, he wasn't irrational. Holmes, on the other hand, in focusing
on this piece of evidence was more than merely rational: he was an excellent reasoner. 

Third, and more speculatively, if rational constraints on priors involve ranges of acceptable asisgnments, it could well be that 
sometimes assignments near the edges of a range are less rational than ones further from the edge. For instance, plausibly, we 
should have anti-skeptical priors that assign high credence to such things as the uniformity of nature or the reliability of 
our senses, and our nature requires us to keep these credences between the bounds of two epistemic vices: undue skepticism (setting 
the priors too low) and excessive credulity (too high). Then someone whose credences are just slightly above undue skepticism 
or slightly below excessive credulity would be rational, but perhaps only barely so. It would be better to have a more balanced
credence, away from the two boundaries of rationality.

Thus, even the Bayesian should be glad for the possibility of accounting for epistemic supererogation.

\section{Metaepistemology}
\subsection{Why be epistemically rational?}
One of the main questions in metaethics for the past century has been why one should bother being moral. The analogous
question for epistemic rationality has received comparatively little attention??ref:exceptions.
One reason for the low amount of attention could be that there is an obvious answer: It's good 
(both instrumentally and not) to have the truth, and following the norms of epistemic rationality 
helps one get there.

Indeed, there are proposed norms which can be justified in this way. If one measures the utility of a 
doxastic state by a strictly proper scoring rule, then by the agent's own lights Bayesian update is 
an algorithm that maximizes epistemic utility.??refs Similarly, it seems reasonable to think that the
epistemic rationality involved in choosing which evidence to pursue, i.e., which experiments to perform??backref, 
can be accounted for by such a maximization. And finally the norm that one's credences should be 
(probabilistically) consistent can be justified by score maximization, because of a famous theorem
that if one's credences are not consistent, there is a set of credences that are guaranteed to yield 
a better score no matter what the truth turns out to be. If Bayesian update, choice of experiments and consistency
are all there is to  epistemic rationality, then this kind of reasoning has some hope of answering the question 
of why one should bother being epistemically rational, assuming we take it for granted that the good is worth
having. 

But this hope may not be borne out. As argued in ??backref, there is good reason to think that prudential 
rationality does not always involve maximizing utility---risk needs to be taken into account. Once we have
seen that expected utility maximization is not the only reasonable position to take with respect to prudential
goods, why should we assume that it is so for epistemic goods?

But more seriously, Bayesian update, consistency and the norms governing choice of experiment do not exhaust the 
scope of epistemic norms. For a Bayesian should admit there are substantive objective norms constraining 
rational priors going beyond consistency. It is not rational, for instance, to have a very high
prior credence for the hypothesis that one is a brain in a vat hooked up to a simulation of precisely the 
physics of our universe (cf.\ ??backref). And then we need an account of why one should bother to have consistent 
priors that are rational rather than some others.

It won't do to say that the rational priors best conduces to epistemic utility. For they don't. It would 
have been irrational for human beings to have a high prior for such hypotheses as that water is made of hydrogen
and oxygen atoms, that 2020 would be a year of a global pandemic, or that the fine-structure constant is 
approximately $1/137$. And yet epistemic utility would have been increased by adopting such priors, because 
all these hypotheses are in fact true. The priors that uniquely best conduce to epistemic utility are a 
prior of $1$ for every actual truth and a prior of $0$ for every actual falsehood. But such priors would not 
be rational for human beings. 

An Aristotelian has a story. Although truth is partly constitutive of our cognitive flourishing, so is conformity 
to the norms of rationality. It is thus non-instrumentally good for us that we reason in accordance with our 
nature. A human being who thought \textit{a priori} that the fine-structure constant is approximately $1/137$
would be right, and would flourish to that extent, but would also have crazy priors, which is contrary to 
cognitive flourishing. Typically, we have more control over whether we follow the norms of epistemic rationality
than whether we get to the truth, and so from the point of view of pursuit of one's own flourishing it makes
sense to follow these norms in typical situations.

That said, one can imagine cases where tradeoffs are reasonable. Suppose a trustworthy but eccentric alien
offers to inform you of the correct theory of stellar formation in exchange for the very small sacrifice 
of brainwashing yourself into thinking that you have an even number of hairs on your head. Irrationally
coming to a belief about the parity of your hair count is contrary to our cognitive flourishing, but in terms 
of epistemic utility, knowing the correct theory of stellar formation seems worth it.

That said, there is another possible answer to the ``Why bother?'' question. It could be that to the extent
that our cognitive life is under our voluntary control, there are moral requirements that we follow epistemic
norms. After all, on the account defended here, morality extends throughout the sphere of the voluntary: 
the moral is defined by the proper functioning of the will. Thus it is possible that one have a deontological
view on which even if voluntarily brainwashing yourself to believe something you have insufficient evidence 
for is beneficial to your epistemic flourishing, nonetheless it is morally wrong to do so. 
Much of our cognitive life is not under our direct voluntary control. But we may nonetheless have a 
\textit{prima facie} moral duty  to do what we reasonably can to develop our cognition in such a way that even the 
involuntary parts cohere with the norms, just as we may have a \textit{prima facie} moral duty to promote the 
health of our bodies. It does, after all, seem to be a part of the proper function of our will to direct us to 
both physical and cognitive health.

And we should not take the moral account too far. We can have two people who act equally well morally, but one 
of them does cognitively better than the other, due to innate talent, opportunities for cognitive self-improvement,
and simple luck at getting to the truth. Epistemic value goes beyond moral excellence, if only because it includes 
the value of epistemic success, much as health goes beyond moral excellence, even though there are \textit{prima 
facie} duties to promote our own health.

\subsection{What is epistemic rationality?}
We have an Aristotelian answer as to what epistemic rationality is among humans: it is the proper functioning of 
our cognitive faculties. That answer nicely generalizes to yield an account of epistemic rationality for a variety 
of kinds of beings, as long as we can make sense of what cognitive faculties are. 

In the case of morality, a similar functional approach defined morality in terms of the proper function of the will,
and the will as a system directing behavior in the light of practical good-directed reasons. We might try here try for 
a similar account but this time in terms of reasons directed at epistemic goods, such as correct and informative 
(whether in the sense of yielding understanding or some other sense) representation of the world.
This approach is more problematic, however, than on the moral side, especially if we are open to a wide range of 
possible epistemologies for different kinds of rational beings.

First, we could imagine beings that directly contemplate the truth, like Aristotle's gods or Kant's intellectual
intuiters. It is difficult to identify reasons in such a case, and yet the contemplation appears to be a rational
activity. And while humans do not normally directly intuit reality, it is possible that we do so in special cases,
such as Platonic contemplation of the Forms or the Catholic of human flourishing as bound up with a direct beatific
vision of God. 

Second, reliabilist epistemology can involve processes that reliably directly generate beliefs in us rather than 
doing by the intermediary of reasons. And even if reliabilism is not a correct description of human epistemology,
it seems imaginable that there be rational beings of whom it is normatively true.

Third, objective Bayesianism has three places for epistemic normativity: updating on evidence, choosing which 
experiments to perform, and having rational ur??-priors. The update and experiment-selection are indeed based on 
reasons. However, the priors are not typically attained on the basis of reasons, but appear to be simply innately 
had by an agent (though in cases where one changes one's priors reasons may be involved). Again, even if objective 
Bayesianism is not the right normative theory of humans, it is plausible that it could be the normative theory for 
a different kind of rational being. Moreover, there are other phenomena that could be governed by rational 
normativity that are similar to that of priors, such as innate beliefs, or beliefs passed on genetically in 
imaginable alien species. 

Without adverting to reasons, we might simply define an epistemically rational system as one directed at 
epistemic goods, such as correct and informative representation of reality, and epistemic rationality as 
the normativity of the proper functioning of such a system. However, if we did that, then epistemic rationality
would go very far down in the animal kingdom: even worms have systems directed at correct representation of 
reality. To rule worms as not rational, we might want to add an element of self-reflection, where the agent 
is such that they ought to explicitly embrace the epistemic goods as such, and ensure that their epistemic 
functioning is correctly aimed at this goods. This last condition would contemplation, reliable
belief formation and prior-generation back into the fold of rationality: we might say that it is precisely 
when the agent reflectively monitors these processes in their directedness at epistemic goods that the 
agent is epistemically rational in engaging in them. 

It is worth noting that on this reflective account, the high forms of cognition, such as the 
Aristotelian gods' direct vision, might well be rational in a somewhat different way from the rationality 
of lower forms of cognition. For the high forms of cognition 
are often seen as essentially correct, neither in need of correction nor repair, and hence the 
self-monitoring could be limited to realizing that necessarily everything is working correctly. 
This is akin to the way that morality would work differently for a perfect being that does not need 
to engage in self-correction, but that knows that it always functions morally correctly. 

\subsection{Grounds of epistemic norms}
The primary question of metaethics is whether ethically normative claims have truth value, and if so, what grounds their truth value.
The Natural Law answer that I have been defending is that ethically normative claims have truth value grounded in facts about the 
flourishing of the will, which facts in turn are grounded in our form (??add this!). Analogically, we would expect epistemologically 
normative facts to have truth value grounded in facts about the flourishing of the intellect, which facts in turn would be grounded
in our form.

Natural Law metaethics as it stands is compatible with a wide variety of normative ethical theories, though some combinations are
less appealing philosophically. Thus, while one could suppose that the proper function of the will is to maximize utility, 
normative utilitarianism fits more elegantly with metaethical utilitarianism according to which facts about the right
and wrong just are facts about utility maximization. 

Similarly, Natural Law metaepistemology fits with a variety of normative epistemological theories. Reflection on ethical cases
shows that flourishing can have internal and external components (e.g., one's friends' well-being is partly constitutive of one's 
well-being), and so Natural Law metaepistemology is compatible with internalist and externalist norms. It is compatible with 
full-blown Bayesianism, though just as normative utilitarianism fits more neatly with metaethical utilitarianism, so too those
versions of Bayesianism on which the norms all have simple formal statements---say, subjective Bayesianism or Carnapian objective
Bayesianism---fit more simply with an epistemology on which the norms are grounded in formal constraints. On the other hand, an
objective Bayesianism with non-formal constraints on priors fits very well with a Natural Law metaepistemology.

However, over the last century or so there has been a disciplinary difference between ethics and epistemology. While the ethicists
have had a primary focus on the question of how one should act, the epistemologists' primary focus has not been the question of 
how we should think, but what knowledge is. One might take this question of the nature of knowledge to be akin to many ethicists' 
interest in the nature of virtue. In both cases, there is a sense in which the question does not carry normativity on its sleeve,
in that there are possible answers to the question that do not involve any normative components. Thus, Descartes identified knowledge
with the clear and distinct, and a naive Aristotelian (though of course not Aristotle himself) might identify virtue with the 
mathematical center between the humanly possible extremes. Of course, in both cases it is highly plausible that the item is worth
having, but that does not make the item essentially normative---water is also worth having, but is not essentially normative.
However, likewise, in both cases more plausible theories of the item have a significant normative component. Thus, the real 
Aristotle would identify virtue with a \textit{reasonable} mean, while many have tried for accounts of knowledge where one of the
components is \textit{justification}. 

Natural Law metaepistemology more easily yields an account of the grounding of more obviously normative concepts such as 
justification. For instance, a justified inference could be an inference performing which constitutes intellectual flourishing 
(or the intellect \textit{qua} engine of inference). Again, this is compatible with many theories as to what kinds of inferences
are in fact justified. But it is more difficult to see exactly how to ground knowledge. Nonetheless, the Natural Law metaepistemologist
can simply say that facts about knowledge are grounded in facts about intellectual flourishing with specifying how the grounding
goes.  Or they might punt and say that the concept of knowledge is not essentially normative, and hence explaining how knowledge is
grounded is beyond the scope of the normative parts of metaepistemology---and perhaps it is only to the normative parts that the
Natural Lawyer has a distinctive contribution.

I think there is a plausible story about something in the vicinity of knowledge, however. We can think of two aspects of the
intellect, namely process and accomplishment, and both aspects have their distinctive flourishing. Justification will be a matter of
process flourishing. But knowledge seems more like accomplishment. Thus, we might try to identify an accomplishment of the intellect as 
knowledge provided that this output constitutes intellectual flourishing. However, this is not exactly right. For flourishing
intellectually in an accomplishment will include not just the aspects of knowledge acknowledged in the analytic tradition, like true 
belief and justification, but also \textit{understanding}, structural connections between different things found in the intellect that
enlightens us about the things that are important. Aquinas famously distinguishes the vice of \textit{curiositas} from the virtue of
studiousness, and the distinction lies in curiositas' trivial pursuit of mere bits of knowledge while the virtue pursues a holistic
structured understanding of the world's explanatory conenctions. Thus flourishing with respect to intellectual accomplisment is more
like what the medievals called \textit{scientia} than what we call knowledge.

So a Natural Law metaepistemology elegantly gives us a theory about the grounding of \textit{scientia} rather than of knowledge. 
Is this a disadvantage? Or is it a hint that in fact many of the blind alleys of post-Gettier epistemology have been due to the
fact that the more natural concept in the vicinity is the intellectual accomplishment of \textit{scientia} rather than mere 
knowledge?  There is room for significant further exploration here, as well as room to search for a purely normative characterization
of knowledge that might end up being grounded in human nature. How such a search might go will depend on controversial questions about
the role of knowledge in our rational life. 

While my own view suggests we should minimize that role in favor of understanding and credence, 
someone attracted to knowledge having a significant role might have room to give a normative-functional account of knowledge that
has a significant role for human nature. Thus, if one thinks that the knowledge is the norm of assertion, then one might say that
knowledge is that state which makes assertion non-defective, and the standards of defectiveness of assertion are given by our nature.
Or one might try to characterize knowledge as whatever it is that needs to be added to beliefs about explanatory relations in order to yield
understanding, with understanding now having a normative characterization as the object of a human's intellect.
Given an account of knowledge that goes back to the flourishing of the intellect in this kind of an Aristotelian way, we would have
an explanation for why there are so many epicycles in post-Gettier accounts of knowledge. For what we have seen of other areas of the
normative life suggests that we should not expect a great deal of simplicity in the norms governing us. 

That said, it is not the task of metaepistemology to provide a detailed account of knowledge---that is the task of epistemology---but simply
to provide an account of the types of facts that ground knowledge. We might roughly break up these facts into three categories:
(i)~mental facts constituting one's belief and the inferences that led to it; (ii)~the facts (in typical cases extra-mental) making the 
belief true and entering into the warrant or justification; and, finally, (iii)~facts because of which the first two sets of
facts count justifying the belief and yielding what additional condition is needed to get out of the Gettier condition. It is only
the third class of facts that seems to fall under metaepistemology, and it is plausible that facts from that category are normative 
facts about the flourishing of the intellect. 

\subsection{Bayesianism versus natural law metaepistemology}
Bayesian update says that upon learning a new piece of evidence $E$, you should replace all your credences with the 
ones you get by conditionalizing on $E$. Typically, a Bayesian update gets you closer to truth about some 
propositions and further from truth about others. For instance, if you pull a coin from your pocket and look at one
side of it, and see heads, this confirms that the coin has at least one heads side, which is true. But it also 
confirms the hypothesis that the coin is a rare defective double-heads one, which hypothesis is typically going to 
be false. More generally, none of the evidence we ever learn logically excludes the existence of fairies, but I 
assume there are no fairies. Suppose you learn some piece of evidence $E$. Then you trivially get closer to the truth
about $E$, but you also confirm the conjunctive proposition $F$ that $E$ obtains and there are fairies, by confirming the 
first conjunct (unless the prior for $F$ was zero or one).\footnote{We have $P(E\mid F)=1$. However, $P(E\mid \Not F)<1$,
since if we had $P(E\mid \Not F)=1$, then we would have $P(E)=1$, and hence you wouldn't be \textit{learning} $E$, as your
prior in $E$ would have been $1$. Thus, $P(E\mid F)>P(E\mid\Not F)$ and $E$ raises the credence of $F$ by Bayes' theorem
given Bayesian update.} 

Whether you are epistemically better off after a Bayesian update depends on a variety of factors, including the relative 
importance of the propositions that you get closer to the truth on and those that you get further from the truth on. 
If for some reason the above false proposition $F$ is extremely epistemically important (maybe aliens have announced they will 
destroy the earth if and only if $F$ is true), getting spurious confirmation of $F$ could well be an overall epistemic harm. 
Thus, Bayesian update doesn't always get the best results. And typically there is, abstractly speaking, 
a rule that will get you better results than Bayesian update. Suppose your concern is with propositions $p_1,...,p_n$, and 
imagine that a stranger comes up to you and tells you to always employ the following update strategy:
\dref{update-1}{if you learn that $p_1$ is false, do Bayesian update}
\dref{update-2}{if you learn that $p_1$ is true, change your credences to specific numbers $t_1,...,t_n$, respectively.}
Suppose that the stranger happened, by chance or not, to pick the numbers $t_1,...,t_n$ in such a way that $t_i=0$ if 
$p_i$ is false and $t_i=1$ if $p_i$ is true. As long as \dref{update-2} does not happen to coincide with Bayesian 
update (typically it won't), and as long as there is any chance of learning that $p_1$ is true, the stranger's strategy 
will beat Bayesian update. 

Now, chances are, you won't be able to design a better update strategy than a Bayesian one \textit{for yourself}.
It is a famous theorem??ref that your expected epistemic utility, defined using a proper scoring rule and calculated by your 
own lights, will be maximized by Bayesian update over the space of all update strategies. But the crucial thing to 
note in this result is the ``by your own lights''. Someone else who has information that is not available to you can 
sometimes---as in the stranger example---design a better rule for you. 

Imagine now that you are designing an intelligent robot. You have access to more types of information than the robot ever 
will, and you use your superior knowledge to design a robot with an update rule that beats Bayesian update (with respect to 
some or maybe even all ways of prioritizing the importance of propositions). The robot, however, is rather smart, 
and observes that it is finding itself ``naturally'' or ``instinctively'' reasoning in ways that do not match Bayesian update. This makes the 
robot worry: ``Should I keep on reasoning the way I `naturally' do, or should I reprogram myself for Bayesian update?''
From your point of view as the robot's designer, you may well hope---and hope for the sake of the robot's doxastic
flourishing---that the robot will stick to the rule you programmed in, as it is in fact superior with respect to getting
to the truth (given the right prioritization). In fact, it seems plausible that the robot would be epistemically defective 
if it switched to Bayesianism, and started engaging in less reliable reasoning.

We are not robots. Whether we are designed by a person more intelligent than ourselves is highly controversial (I will 
argue in the affirmative in Chapter~\ref{ch:XI}). However, evolution might be metaphorically said to have ``access'' to more 
information than an individual human does: the selective pressures that made us what we are were responsive to a vast amount
of data that we individually do not have, and perhaps that our ancestors did not even have in their minds. Some think 
that with the advent of the large amount of data in modern science, we can do 
better than our instinctive ways of reasoning, say by adopting Bayesianism as our update rule. For the argument I want 
to press, I am willing to concede the point.??ref:fast-and-slow But it is not clear that the same would have been true at earlier points in our species' intellectual development. And it \textit{is} clear that the same is not true for all possible species 
of intelligent animals: there \textit{could} be intelligent animals whose instinctive reasoning would beat Bayesian
reasoning. 

The mere fact that these intelligent animals' instinctive methods of reasoning would beat Bayesian update does not mean that they would be rational to opt for the instinctive methods. But reflection on these possibilities should make one incline to 
the hypothesis that Bayesianism does not have a hegemony on rationality. It seems possible to have beings for which some
other mode of reasoning than Bayesianism is right. Thus, even if Bayesianism is the right epistemology for humans,
it is not the right meta-epistemology: to reason rightly is not the same thing as to update in a Bayesian way. 


%% appendix has been revised
\section*{Appendix: $^*$Our pathological scoring rule}
Recall our pathological scoring rule given by:
$$
	T_0(x) = \begin{cases} 1000 &\text{if }x\ge 0.999\\
						-1000000 &\text{if }x\le 0.001\\
						0 &\text{otherwise}.
						\end{cases}
$$
and
$$
	F_0(1-x).
$$

In ??backref, I claimed that this rule was proper and that it can be written as a limit of
symmetric, strictly proper, finite and continuous scoring rules. It is easy to see that if it
can be written as such a limit, then it \textit{is} proper, so we only need to show it can be
written as such a limit.

First note that any symmetric continuous proper scoring rule $(t,f)$ can be written as the limit of symmetric continuous
strictly proper scoring rules by letting $t_n(x)=t(x)-(1-x)^2/n$ and $f_n(x)=f(x)-x^2/n$, since the Brier scoring rule defined
by the functions $-(1-x)^2$ and $-x^2$ is strictly proper, and the sum of a proper and a strictly proper scoring rules is
strictly proper. 

Thus, all we need to show is that is that we can approximate $(T_0,F_0)$ with symmetric, finite, continuous and proper scoring rules.
Furthermore, we can drop the symmetry requirement. For write $f^*(x)=f(1-x)$. Then $(t,f)$ is a proper scoring rule if and
only if $(f^*,t^*)$ is a proper scoring rule. Now if $T_0(x)=\lim_{n\to\infty} t_n(x)$ for all $x$ and $F_0(x)=\lim_{n\to\infty} f_n(x)$, then
$$
    \frac{T_0(x)+F_0^*(x)}2 = \lim_{n\to\infty} \frac{t_n(x)+f_n^*(x)}2
$$    
    and
$$    
    \frac{F_0(x)+T_0^*(x)}2 = \lim_{n\to\infty} \frac{f_n(x)+t_n^*(x)}2. 
$$    
But $T_0=F^*_0$ and $F_0=T_0^*$, so the left-hand sides are just $T_0(x)$ and $F_0(x)$, respectively. Moreover, $((t_n+f_n^*)/2,(f_n+t_n^*)/2)$  
will be a continuous symmetric finite proper scoring rule if $(t_n,f_n)$ is a continuous finite proper scoring rule.

Fix $\e>0$. Let $\phi_\e$ be a continuous non-negative finite function that is zero except on the set $U_\e=(0.001,0.001+\e]\cup [0.999-\e,0.999)$, 
and is such that $\int_{0.999-\e}^{0.999} (1-u) \phi_\e(u)\, du = 1000$ and $\int_{0.001}^{0.001+\e} u \phi_\e(u) \, du = 1000000$. 
Define 
$$
    T_n(x) = \int_{1/2}^x \, (1-u)\phi_{1/n}(u) \, du
$$
and 
$$
    F_n(x) = \int_{1/2}^x \, u \phi_{1/n}(u) \, du.
$$
By ??SchervishThm4.2, $(T_n,F_n)$ is proper. It is clearly continuous and finite. It is, further, easy to calculate that 
$T_n(x)$ and $F_n(x)$ equal $T(x)$ and $F(x)$ except perhaps on $U_{1/n}$.??check For any $x$ in $[0,1]$, there is an $N$
such that $x$ is not in $U_{1/n}$ for any $n\ge N$. It follows that $T_0(x)=T_n(x)$ and $F_0(x)=F_n(x)$ if $n\ge N$, and we have
the limiting condition we wanted.

\chaptertail 


